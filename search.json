[
  {
    "objectID": "ML_w03_The Naïve Bayesian Classifier.html",
    "href": "ML_w03_The Naïve Bayesian Classifier.html",
    "title": "Week03: The Naïve Bayesian Classifier",
    "section": "",
    "text": "This expose is based on Brett Lantz, 2019. Machine Learning with R Chapter 4: Classification Using the Naïve Bayes.\nThe naïve Bayesian classifier is defined for binary target and metric and/or categorical feature variables.\nIt relies on the Bayesian theorem:\n\\[\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\\]\n\\[= \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B)}\\]\n\\[= \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B|A) \\cdot \\Pr(A) + \\Pr(B|\\bar{A}) \\cdot \\Pr(\\bar{A})}\\]\nwith \\(\\Pr(A|B)\\) being the posteriori probability,\n\\(\\Pr(B|A)\\) being the likelihood,\n\\(\\Pr(A)\\) being the prior probability, and\n\\(\\Pr(B) = \\Pr(B|A) \\cdot \\Pr(A) + \\Pr(B|\\bar{A}) \\cdot \\Pr(\\bar{A})\\) being the marginal probability of \\(B\\).\nFor instance, from an observed frequency table the posteriori probability can be calculated:\n\n\n\n\nFigure 4.5 - Frequency and likelihood tables are the basis for computing the posterior probability of spam\n\n\nThe posteriori probability of receiving a spam email given having the word Viagra is\n\\[\\Pr(spam|Viagra) = \\frac{\\Pr(Viagra|spam) \\cdot \\Pr(spam)}{\\Pr(Viagra)}\\]\n\\[= \\frac{\\frac{4}{20} \\cdot \\frac{20}{100}}{\\frac{5}{100}} = \\frac{4}{5} = 0.80\\]\n\nThus, the posteriori probability is substantially larger than the likelihood.\nThe complement of word \\(B\\) not being in the email is denoted by \\(\\bar{B}\\) with \\(\\Pr(\\bar{B}) = 1 - \\Pr(B)\\).\nFor a bag of words \\(\\{W_1, W_2, \\dots, W_n\\}\\) we get, based on the joint probability\n\\[\\Pr(spam|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam)}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n)}\\]\nEvaluation of the denominator is not necessary because the probabilities can be calculated simply by using the numerator for either \\(spam\\) and \\(\\overline{spam}\\):\n\\[\\Pr(spam|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam)}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam) + \\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}\\]\n\\[\\Pr(\\overline{spam}|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam) + \\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}\\]\nEvaluating the joint probabilities becomes infeasible in a \\(n + 1\\) dimensional table, because [a] of storage demands, [b] requirement of large samples to populate the \\(+1\\) dimensional table, and [c] sparsity of non-zero cell counts.\nTherefore, the multidimensional table is broken up into \\(n\\) bivariate tables between the target variables and each feature.\nThus any dependencies among the features are ignored and it is naïvely assumed the features are independent.\nFor example, we observe the relative frequency tables of several words in relationship to an email being either spam or ham:\n\n\n\n\nFigure 4.6 - An expanded table adds likelihoods for additional terms in spam and ham messages\n\n\n\nThese \\(n\\) bivariate tables are learned from the training sample.\nDue to the independence assumption the joint probability becomes a product of the marginal probabilities: \\(\\Pr(A \\cap B) = \\Pr(A) \\cdot \\Pr(B)\\).\n\n\n\nTherefore, the joint posteriori probability of becomes\n\\[\\Pr(spam|W_1, W_2, \\cdots, W_n) = \\frac{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam)}{const}\\]\nor\n\\[= \\frac{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam)}{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam) + \\Pr(\\overline{spam}) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|\\overline{spam})}\\]\nAn issue arises if any bivariate likelihood \\(\\Pr(W_i|spam)\\) is empirically zero because then the whole product \\(\\prod_{i=1}^{n} \\Pr(W_i|spam) = 0\\) in the numerator reduces to zero.\nThus, the probabilities become unreasonably \\(\\Pr(spam|W_1, W_2, \\cdots, W_n) = 0\\) and \\(\\Pr(\\overline{spam}|W_1, W_2, \\cdots, W_n) = 1\\).\nTo circumvent this problem the Laplace adjustment is performed by adding a constant \\(\\lambda\\) to each cell in the bivariate cross-tabulation while holding the prior probabilities \\(\\Pr(spam)\\) and \\(\\Pr(ham)\\) fixed.\nUsually, a constant \\(\\lambda = 1\\) is selected, but this is a hyper-parameter and could be determined empirically.\nIn contrast to logistic regression, the naïve classifier can handle multinominal distributed target and feature variables with more than 2 classes.\nNaïve Bayesian classifier can handle a large number of features, because the underlying covariance structure among the features is ignored.\nFor metric variables the conditional probabilities \\(\\Pr(X_i = m|Y = k)\\) are replaced by univariate normally distributed densities \\(f(X_i = x|Y = k) = f(X_i = x|\\hat{\\mu}_k, \\hat{\\sigma}_k^2)\\) where \\(\\hat{\\mu}_k\\) is the estimated mean of \\(X_i\\) in class \\(k\\) and analog for the estimated variance \\(\\hat{\\sigma}_k^2\\).\nAlternatively, also conditional kernel densities can be used to model \\(f(X_i = x|Y = k)\\).\n\n\n\nTwo class example of mixed continuous and discrete features:\no \\[\\Pr(Y = 1|X = x^*) = 0.944 = \\frac{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5}{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5 + 0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}\\]\no \\[\\Pr(Y = 2|X = x^*) = 0.056 = \\frac{0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5 + 0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}\\]\n\n\n\n\nFigure - Density estimates for class k=1 and k=2\n\n\n\nAdvantages and disadvantages of the naïve Bayesian classifier:\nStrengths:\n\nSimple, fast, and very effective\nDoes well with noisy and missing data\nRequires relatively few examples for training, but also works well with very large numbers of examples\nEasy to obtain the estimated probability for a prediction\n\nWeaknesses:\n\nRelies on an often-faulty assumption of equally important and independent features\nNot ideal for datasets with many numeric features\nEstimated probabilities are less reliable than the predicted classes\n\n\n\n\n\nTable - Advantages and disadvantages of the naïve Bayesian classifier",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-The Naïve Bayesian Classifier"
    ]
  },
  {
    "objectID": "ML_w03_The Naïve Bayesian Classifier.html#the-naïve-bayesian-classifier",
    "href": "ML_w03_The Naïve Bayesian Classifier.html#the-naïve-bayesian-classifier",
    "title": "Week03: The Naïve Bayesian Classifier",
    "section": "",
    "text": "This expose is based on Brett Lantz, 2019. Machine Learning with R Chapter 4: Classification Using the Naïve Bayes.\nThe naïve Bayesian classifier is defined for binary target and metric and/or categorical feature variables.\nIt relies on the Bayesian theorem:\n\\[\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\\]\n\\[= \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B)}\\]\n\\[= \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B|A) \\cdot \\Pr(A) + \\Pr(B|\\bar{A}) \\cdot \\Pr(\\bar{A})}\\]\nwith \\(\\Pr(A|B)\\) being the posteriori probability,\n\\(\\Pr(B|A)\\) being the likelihood,\n\\(\\Pr(A)\\) being the prior probability, and\n\\(\\Pr(B) = \\Pr(B|A) \\cdot \\Pr(A) + \\Pr(B|\\bar{A}) \\cdot \\Pr(\\bar{A})\\) being the marginal probability of \\(B\\).\nFor instance, from an observed frequency table the posteriori probability can be calculated:\n\n\n\n\nFigure 4.5 - Frequency and likelihood tables are the basis for computing the posterior probability of spam\n\n\nThe posteriori probability of receiving a spam email given having the word Viagra is\n\\[\\Pr(spam|Viagra) = \\frac{\\Pr(Viagra|spam) \\cdot \\Pr(spam)}{\\Pr(Viagra)}\\]\n\\[= \\frac{\\frac{4}{20} \\cdot \\frac{20}{100}}{\\frac{5}{100}} = \\frac{4}{5} = 0.80\\]\n\nThus, the posteriori probability is substantially larger than the likelihood.\nThe complement of word \\(B\\) not being in the email is denoted by \\(\\bar{B}\\) with \\(\\Pr(\\bar{B}) = 1 - \\Pr(B)\\).\nFor a bag of words \\(\\{W_1, W_2, \\dots, W_n\\}\\) we get, based on the joint probability\n\\[\\Pr(spam|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam)}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n)}\\]\nEvaluation of the denominator is not necessary because the probabilities can be calculated simply by using the numerator for either \\(spam\\) and \\(\\overline{spam}\\):\n\\[\\Pr(spam|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam)}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam) + \\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}\\]\n\\[\\Pr(\\overline{spam}|W_1 \\cap W_2 \\cap \\cdots \\cap W_n) = \\frac{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}{\\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|spam) \\cdot \\Pr(spam) + \\Pr(W_1 \\cap W_2 \\cap \\cdots \\cap W_n|\\overline{spam}) \\cdot \\Pr(\\overline{spam})}\\]\nEvaluating the joint probabilities becomes infeasible in a \\(n + 1\\) dimensional table, because [a] of storage demands, [b] requirement of large samples to populate the \\(+1\\) dimensional table, and [c] sparsity of non-zero cell counts.\nTherefore, the multidimensional table is broken up into \\(n\\) bivariate tables between the target variables and each feature.\nThus any dependencies among the features are ignored and it is naïvely assumed the features are independent.\nFor example, we observe the relative frequency tables of several words in relationship to an email being either spam or ham:\n\n\n\n\nFigure 4.6 - An expanded table adds likelihoods for additional terms in spam and ham messages\n\n\n\nThese \\(n\\) bivariate tables are learned from the training sample.\nDue to the independence assumption the joint probability becomes a product of the marginal probabilities: \\(\\Pr(A \\cap B) = \\Pr(A) \\cdot \\Pr(B)\\).\n\n\n\nTherefore, the joint posteriori probability of becomes\n\\[\\Pr(spam|W_1, W_2, \\cdots, W_n) = \\frac{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam)}{const}\\]\nor\n\\[= \\frac{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam)}{\\Pr(spam) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|spam) + \\Pr(\\overline{spam}) \\cdot \\prod_{i=1}^{n} \\Pr(W_i|\\overline{spam})}\\]\nAn issue arises if any bivariate likelihood \\(\\Pr(W_i|spam)\\) is empirically zero because then the whole product \\(\\prod_{i=1}^{n} \\Pr(W_i|spam) = 0\\) in the numerator reduces to zero.\nThus, the probabilities become unreasonably \\(\\Pr(spam|W_1, W_2, \\cdots, W_n) = 0\\) and \\(\\Pr(\\overline{spam}|W_1, W_2, \\cdots, W_n) = 1\\).\nTo circumvent this problem the Laplace adjustment is performed by adding a constant \\(\\lambda\\) to each cell in the bivariate cross-tabulation while holding the prior probabilities \\(\\Pr(spam)\\) and \\(\\Pr(ham)\\) fixed.\nUsually, a constant \\(\\lambda = 1\\) is selected, but this is a hyper-parameter and could be determined empirically.\nIn contrast to logistic regression, the naïve classifier can handle multinominal distributed target and feature variables with more than 2 classes.\nNaïve Bayesian classifier can handle a large number of features, because the underlying covariance structure among the features is ignored.\nFor metric variables the conditional probabilities \\(\\Pr(X_i = m|Y = k)\\) are replaced by univariate normally distributed densities \\(f(X_i = x|Y = k) = f(X_i = x|\\hat{\\mu}_k, \\hat{\\sigma}_k^2)\\) where \\(\\hat{\\mu}_k\\) is the estimated mean of \\(X_i\\) in class \\(k\\) and analog for the estimated variance \\(\\hat{\\sigma}_k^2\\).\nAlternatively, also conditional kernel densities can be used to model \\(f(X_i = x|Y = k)\\).\n\n\n\nTwo class example of mixed continuous and discrete features:\no \\[\\Pr(Y = 1|X = x^*) = 0.944 = \\frac{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5}{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5 + 0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}\\]\no \\[\\Pr(Y = 2|X = x^*) = 0.056 = \\frac{0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}{0.368 \\cdot 0.484 \\cdot 0.226 \\cdot 0.5 + 0.030 \\cdot 0.130 \\cdot 0.616 \\cdot 0.5}\\]\n\n\n\n\nFigure - Density estimates for class k=1 and k=2\n\n\n\nAdvantages and disadvantages of the naïve Bayesian classifier:\nStrengths:\n\nSimple, fast, and very effective\nDoes well with noisy and missing data\nRequires relatively few examples for training, but also works well with very large numbers of examples\nEasy to obtain the estimated probability for a prediction\n\nWeaknesses:\n\nRelies on an often-faulty assumption of equally important and independent features\nNot ideal for datasets with many numeric features\nEstimated probabilities are less reliable than the predicted classes\n\n\n\n\n\nTable - Advantages and disadvantages of the naïve Bayesian classifier",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-The Naïve Bayesian Classifier"
    ]
  },
  {
    "objectID": "ML_w03_kNNClassification.html",
    "href": "ML_w03_kNNClassification.html",
    "title": "Week03: Classification - Bayesian k-Nearest Neighbor Class Assignment",
    "section": "",
    "text": "In the classification setting the outcome variable is discrete, that is,\n\\[y_i = k \\text{ if object } i \\text{ belongs to the } k^{th} \\text{ class}\\]\nThe error rate, which we aim to minimize, becomes \\(\\frac{1}{n}\\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\\), where the indicator function\n\\[I(\\quad) = \\begin{cases}\n1 & \\text{if } y_i \\neq \\hat{y}_i \\\\\n0 & \\text{if } y_i = \\hat{y}_i\n\\end{cases}.\\]\nA Bayesian classifier assigns an observation \\(i\\) to the class \\(k\\) if its conditional probability \\(\\Pr(Y = k|X = \\mathbf{x}_0)\\) is larger than that for any other class.\nFor just two classes \\(K = 2\\) the probability must be \\(\\Pr(Y = k|X = \\mathbf{x}_0) &gt; 0.5\\).\nThe Bayesian class membership probabilities in \\(k\\)-nearest neighborhood (kNN) is calculated as:\no Determine those \\(k\\) training points which are the closest to the prediction point \\(\\mathbf{x}_0\\).\nCloseness is measured in terms of the Euclidian distance between two objects \\(i\\) and \\(j\\) with their feature vectors \\(\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{iP})^T\\) and \\(\\mathbf{x}_j = (x_{j1}, x_{j2}, \\dots, x_{jP})^T\\) by\n\\[d_{ij} = \\sqrt{\\sum_{p=1}^{P} (x_{ip} - x_{jp})^2}.\\]\n\n\n\n\nFigure 2.13 - A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.\n\n\no Important: Distances change with the scale (i.e., range) of the underlying variables. This is a problem encountered in all distance-based machine learning methods.\no Subsequently estimate the Bayesian group probabilities by \\(\\Pr(Y = k|X = \\mathbf{x}_0) = \\frac{1}{|\\mathcal{N}_0|}\\sum_{i \\in \\mathcal{N}_0} I(y_i = k)\\), where \\(\\mathcal{N}_0\\) is the neighborhood around the prediction point \\(\\mathbf{x}_0\\) with \\(|\\mathcal{N}_0|\\) the number of training points \\(K\\) around the prediction location.\no Assign the observation to class \\(k\\) with the highest probability.\n\n\n\nFigure 2.14 - The KNN approach, using K = 3, is illustrated in a simple situation with six blue observations and six orange observations. Left: a test observation at which a predicted class label is desired is shown as a black cross. The three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue. Right: The KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class.\n\n\n\nThe user decides the hyper-parameter \\(|\\mathcal{N}_0|\\) to the desired number of nearest neighbors\nThis hyper-parameter \\(|\\mathcal{N}_0|\\) determines flexibility of kNN algorithm:\n\n\n\n\nFigure 2.16 - A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.\n\n\n\nThe hyperparameter \\(|\\mathcal{N}_0|\\) also influences the test and training error rates:\n\n\n\n\nFigure 2.17 - The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.\n\n\n\nThe function e1071::gknn( ) performs kNN model calibrations.\n\\(k\\)-nearest neighbors can also be used to handle metric dependent variables \\(y\\). In that case, the predicted value \\(y_0\\) is the average value of the observed values \\(y_i\\) in its neighborhood \\(\\mathcal{N}_0\\).\nIn case of an overall class tie, the predicted class is chosen at random.\nAdvantages of the kNN model:\no Simple and effective estimation, which gives by default the probability of the predicted group membership.\no Makes no assumptions about the underlying distribution of the features.\no Virtually no training time required.\nDisadvantages of the kNN model:\no As the number of features increases, the decision space becomes more sparsely populated (curse of dimensionality) which increases the overall uncertainty.\no How are ties (two classes have equal maximum probabilities) broken?\no Features that are irrelevant for the classification task are not eliminated (Machine Learning concept of regularization) thus there is the potential of overfitting the data.\no The performance of the algorithm depends on how comparable the features are scaled to evaluate their distances.\no For each test data point \\(\\mathbf{x}_0\\) the distances need to be calculated to identify the \\(k\\) nearest neighborhood \\(\\mathcal{N}_0\\).\no Euclidian distances for nominal scaled features and missing feature information require additional processing.\no The algorithm’s prediction performance depends on the the hyper-parameter \\(k\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-KNN Classification"
    ]
  },
  {
    "objectID": "ML_w03_kNNClassification.html#classification-bayesian-k-nearest-neighbor-class-assignment",
    "href": "ML_w03_kNNClassification.html#classification-bayesian-k-nearest-neighbor-class-assignment",
    "title": "Week03: Classification - Bayesian k-Nearest Neighbor Class Assignment",
    "section": "",
    "text": "In the classification setting the outcome variable is discrete, that is,\n\\[y_i = k \\text{ if object } i \\text{ belongs to the } k^{th} \\text{ class}\\]\nThe error rate, which we aim to minimize, becomes \\(\\frac{1}{n}\\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\\), where the indicator function\n\\[I(\\quad) = \\begin{cases}\n1 & \\text{if } y_i \\neq \\hat{y}_i \\\\\n0 & \\text{if } y_i = \\hat{y}_i\n\\end{cases}.\\]\nA Bayesian classifier assigns an observation \\(i\\) to the class \\(k\\) if its conditional probability \\(\\Pr(Y = k|X = \\mathbf{x}_0)\\) is larger than that for any other class.\nFor just two classes \\(K = 2\\) the probability must be \\(\\Pr(Y = k|X = \\mathbf{x}_0) &gt; 0.5\\).\nThe Bayesian class membership probabilities in \\(k\\)-nearest neighborhood (kNN) is calculated as:\no Determine those \\(k\\) training points which are the closest to the prediction point \\(\\mathbf{x}_0\\).\nCloseness is measured in terms of the Euclidian distance between two objects \\(i\\) and \\(j\\) with their feature vectors \\(\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{iP})^T\\) and \\(\\mathbf{x}_j = (x_{j1}, x_{j2}, \\dots, x_{jP})^T\\) by\n\\[d_{ij} = \\sqrt{\\sum_{p=1}^{P} (x_{ip} - x_{jp})^2}.\\]\n\n\n\n\nFigure 2.13 - A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.\n\n\no Important: Distances change with the scale (i.e., range) of the underlying variables. This is a problem encountered in all distance-based machine learning methods.\no Subsequently estimate the Bayesian group probabilities by \\(\\Pr(Y = k|X = \\mathbf{x}_0) = \\frac{1}{|\\mathcal{N}_0|}\\sum_{i \\in \\mathcal{N}_0} I(y_i = k)\\), where \\(\\mathcal{N}_0\\) is the neighborhood around the prediction point \\(\\mathbf{x}_0\\) with \\(|\\mathcal{N}_0|\\) the number of training points \\(K\\) around the prediction location.\no Assign the observation to class \\(k\\) with the highest probability.\n\n\n\nFigure 2.14 - The KNN approach, using K = 3, is illustrated in a simple situation with six blue observations and six orange observations. Left: a test observation at which a predicted class label is desired is shown as a black cross. The three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue. Right: The KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class.\n\n\n\nThe user decides the hyper-parameter \\(|\\mathcal{N}_0|\\) to the desired number of nearest neighbors\nThis hyper-parameter \\(|\\mathcal{N}_0|\\) determines flexibility of kNN algorithm:\n\n\n\n\nFigure 2.16 - A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.\n\n\n\nThe hyperparameter \\(|\\mathcal{N}_0|\\) also influences the test and training error rates:\n\n\n\n\nFigure 2.17 - The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.\n\n\n\nThe function e1071::gknn( ) performs kNN model calibrations.\n\\(k\\)-nearest neighbors can also be used to handle metric dependent variables \\(y\\). In that case, the predicted value \\(y_0\\) is the average value of the observed values \\(y_i\\) in its neighborhood \\(\\mathcal{N}_0\\).\nIn case of an overall class tie, the predicted class is chosen at random.\nAdvantages of the kNN model:\no Simple and effective estimation, which gives by default the probability of the predicted group membership.\no Makes no assumptions about the underlying distribution of the features.\no Virtually no training time required.\nDisadvantages of the kNN model:\no As the number of features increases, the decision space becomes more sparsely populated (curse of dimensionality) which increases the overall uncertainty.\no How are ties (two classes have equal maximum probabilities) broken?\no Features that are irrelevant for the classification task are not eliminated (Machine Learning concept of regularization) thus there is the potential of overfitting the data.\no The performance of the algorithm depends on how comparable the features are scaled to evaluate their distances.\no For each test data point \\(\\mathbf{x}_0\\) the distances need to be calculated to identify the \\(k\\) nearest neighborhood \\(\\mathcal{N}_0\\).\no Euclidian distances for nominal scaled features and missing feature information require additional processing.\no The algorithm’s prediction performance depends on the the hyper-parameter \\(k\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-KNN Classification"
    ]
  },
  {
    "objectID": "ML_w03_kNNClassification.html#unsupervised-ml-multivariate-normal-distribution-mixture-grouping",
    "href": "ML_w03_kNNClassification.html#unsupervised-ml-multivariate-normal-distribution-mixture-grouping",
    "title": "Week03: Classification - Bayesian k-Nearest Neighbor Class Assignment",
    "section": "2 Unsupervised ML: Multivariate Normal-Distribution Mixture Grouping",
    "text": "2 Unsupervised ML: Multivariate Normal-Distribution Mixture Grouping\nSee Boehmke & Greenwell Chapter 22: Model-based Clustering pp 429-441\n\nIn unsupervised learning, only the object features \\(\\mathbf{x}_i\\) but not the outcome \\(y_i\\) are known to the investigator.\nA function \\(f(\\quad)\\) is sought that allows us to guess what \\(y\\) could be based on the inherent structure within the observed feature matrix \\(\\mathbf{X}\\).\nAn example of unsupervised learning is to group feature adjacent objects together into clusters. With respect to their features \\(\\mathbf{x}_i\\) the clusters are supposed to be internally as homogenous as possible, but the identified clusters should be as heterogeneous (distinct) from each other.\nFuzziness in the cluster delimitations determines the success of the classification rule.\nExample: Identifying a multivariate mixture distribution\no The distribution of points with multiple centers and ellipsoidal standard distances can be modeled by a mixture of multi-normal distributions.\nFor example in the 2-dimensional setting the mixture distribution the joint density becomes:\n\\[f\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right] \\sim \\sum_{q=1}^{Q} \\pi_q \\cdot \\mathcal{N}\\left(\\left[\\begin{matrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{matrix}\\right], \\left[\\begin{matrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{matrix}\\right]\\right) \\text{ with } \\pi_q &gt; 0 \\text{ and } \\sum_{q=1}^{Q} \\pi_q = 1\\]\no Unknowns are:\n[a] the number of clusters \\(Q\\),\n[b] the proportion of points per cluster \\(\\pi_q\\),\n[c] the \\(Q\\) cluster centers \\(\\left[\\begin{matrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{matrix}\\right]\\) and\n[d] their \\(Q\\) ellipsoidal covariance matrices \\(\\left[\\begin{matrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{matrix}\\right]\\).\n\n\n\n\nFigure 2.8 - A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging.\n\n\no The probability \\(\\Pr\\left(q \\left|\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]\\right.\\right)\\) of observation \\(\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]\\) belonging to cluster \\(q\\) is evaluated using the Bayesian theorem\n\\[\\Pr\\left(q \\left|\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]\\right.\\right) \\sim \\frac{f\\left(\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]|q\\right) \\cdot \\Pr(q)}{\\sum_{q=1}^{Q} f\\left(\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]|q\\right) \\cdot \\Pr(q)}\\]\nwith the likelihood of \\(\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]\\) being in the \\(q^{th}\\) class \\(f\\left(\\left[\\begin{matrix} x_i \\\\ y_i \\end{matrix}\\right]|q\\right) \\sim \\mathcal{N}\\left(\\left[\\begin{matrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{matrix}\\right], \\left[\\begin{matrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{matrix}\\right]\\right)\\)\nand the prior probability \\(\\Pr(q)\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-KNN Classification"
    ]
  },
  {
    "objectID": "ML_w02_PseudoRandomNumbers.html",
    "href": "ML_w02_PseudoRandomNumbers.html",
    "title": "Week02: Pseudo Random Number Generation",
    "section": "",
    "text": "A sequence of “random” numbers is used in almost all machine learning and statistical procedures, e.g. bootstrapping, or splitting a dataset into a training sample and a test sample.\nThese “random” numbers are usually generated by a computer algorithm that provides a sequence of a given length of pseudo random numbers that follow a uniform distribution.\nOnce the sequence is completed the same set of pseudo random numbers is repeated.\nThis sequence of pseudo random numbers should not have a discernable pattern, e.g., the numbers appear to be independent with no serial autocorrelation.\n\n\n\n\nIllustration\n\n\n\nEach sequence of pseudo random numbers depends on a starting value.\n\nFor a given starting value the sequence will always give the same order of pseudo random numbers.\nFor different starting values the sequences will be different and uncorrelated with sequences based on different starting values.\nThe internal computer clock can provide a starting value. No reported computer clock time will be exactly identical to another’s computer reported clock time even if two users think that they hit the return key at the same time.\nTo obtain identical results from one experiment to another experiment one needs to start each experiment with exactly the same starting value. This starting value is called the seed.\nProblems may arise in parallel computing when each thread is based on the same starting value.\n\nA simple algorithm proceeds as follows:\n\nChoose a starting value or seed \\(X_0\\) at iteration zero.\nGenerate a new number \\(Y = a \\cdot X_0 + b\\), where \\(a\\) and \\(b\\) are some externally given constants.\nDivide \\(Y\\) by another externally given constant \\(c\\), and let the remainder be the first pseudo random number \\(X_1\\).\nReturn to step 2 above, this time using \\(X_1\\) to generate a new \\(Y\\).\n\nThe script BasicRandomNumberGenerator.R implements this simple pseudo random number algorithm.\n and all other software environments use highly sophisticated algorithms to generate random numbers. See the document RNGVersions.pdf for further discussion.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Pseudo Random Numbers"
    ]
  },
  {
    "objectID": "ML_w02_PseudoRandomNumbers.html#pseudo-random-number-generation",
    "href": "ML_w02_PseudoRandomNumbers.html#pseudo-random-number-generation",
    "title": "Week02: Pseudo Random Number Generation",
    "section": "",
    "text": "A sequence of “random” numbers is used in almost all machine learning and statistical procedures, e.g. bootstrapping, or splitting a dataset into a training sample and a test sample.\nThese “random” numbers are usually generated by a computer algorithm that provides a sequence of a given length of pseudo random numbers that follow a uniform distribution.\nOnce the sequence is completed the same set of pseudo random numbers is repeated.\nThis sequence of pseudo random numbers should not have a discernable pattern, e.g., the numbers appear to be independent with no serial autocorrelation.\n\n\n\n\nIllustration\n\n\n\nEach sequence of pseudo random numbers depends on a starting value.\n\nFor a given starting value the sequence will always give the same order of pseudo random numbers.\nFor different starting values the sequences will be different and uncorrelated with sequences based on different starting values.\nThe internal computer clock can provide a starting value. No reported computer clock time will be exactly identical to another’s computer reported clock time even if two users think that they hit the return key at the same time.\nTo obtain identical results from one experiment to another experiment one needs to start each experiment with exactly the same starting value. This starting value is called the seed.\nProblems may arise in parallel computing when each thread is based on the same starting value.\n\nA simple algorithm proceeds as follows:\n\nChoose a starting value or seed \\(X_0\\) at iteration zero.\nGenerate a new number \\(Y = a \\cdot X_0 + b\\), where \\(a\\) and \\(b\\) are some externally given constants.\nDivide \\(Y\\) by another externally given constant \\(c\\), and let the remainder be the first pseudo random number \\(X_1\\).\nReturn to step 2 above, this time using \\(X_1\\) to generate a new \\(Y\\).\n\nThe script BasicRandomNumberGenerator.R implements this simple pseudo random number algorithm.\n and all other software environments use highly sophisticated algorithms to generate random numbers. See the document RNGVersions.pdf for further discussion.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Pseudo Random Numbers"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html",
    "href": "ML_w01_MLIntroduction.html",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "",
    "text": "Course and textbook objective: Democratizing machine learning with commonly accessible soft- and hardware to regain control over our destiny rather than letting robots or artificial intelligence (AI) agents rule us.\nThe job of robots and AI agents should support us by making our life easier.\nHowever, we are running the risk of becoming too dependent on, monitored or manipulated by algorithms.\n\nExamples:\n\nFinding our way around a new city. Just 2 decades ago we would study a road map in advance of our trip and memorize key turning points. On our way we would make extensive use of guiding landmarks and road signs. Furthermore, we have a broad overview of the area that we traverse, rather than a point-to-point conception of space. This would train our hippocampus, which houses the brain’s function of spatial orientation. This training is lost with our blind reliance of GPS navigation.\nSelf-driving vehicles. The objective is to reduce accidents and minimize congestion relative to human abilities. We are not there (yet).\nUse AI to answer singular questions and assignments, rather than learning and memorizing the information. If we don’t learn the material, then our brain will never be given to connect bits of information together to generate knowledge and synthesize the knowledge into innovations.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#lecture01-an-introduction-to-machine-learning",
    "href": "ML_w01_MLIntroduction.html#lecture01-an-introduction-to-machine-learning",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "",
    "text": "Course and textbook objective: Democratizing machine learning with commonly accessible soft- and hardware to regain control over our destiny rather than letting robots or artificial intelligence (AI) agents rule us.\nThe job of robots and AI agents should support us by making our life easier.\nHowever, we are running the risk of becoming too dependent on, monitored or manipulated by algorithms.\n\nExamples:\n\nFinding our way around a new city. Just 2 decades ago we would study a road map in advance of our trip and memorize key turning points. On our way we would make extensive use of guiding landmarks and road signs. Furthermore, we have a broad overview of the area that we traverse, rather than a point-to-point conception of space. This would train our hippocampus, which houses the brain’s function of spatial orientation. This training is lost with our blind reliance of GPS navigation.\nSelf-driving vehicles. The objective is to reduce accidents and minimize congestion relative to human abilities. We are not there (yet).\nUse AI to answer singular questions and assignments, rather than learning and memorizing the information. If we don’t learn the material, then our brain will never be given to connect bits of information together to generate knowledge and synthesize the knowledge into innovations.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#useful-websites",
    "href": "ML_w01_MLIntroduction.html#useful-websites",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "2 Useful Websites",
    "text": "2 Useful Websites\n\n&lt;www.statlearning.com&gt;\nhttps://cran.r-project.org/web/views/MachineLearning.html\n&lt;www.kaggle.com&gt;",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#r-installation",
    "href": "ML_w01_MLIntroduction.html#r-installation",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "3 R-Installation",
    "text": "3 R-Installation\n\nIt is important that we all work with the same version of R to guarantee that the assigned labs are working properly.\nPlease see RStudioTutorial2024.pptx to get a common foundation.\nHere is an example on how to use GitHub’s Copilot in RStudio to code tasks. Note we frequently observe than AI is generating a substantial amount of unnecessary code and cannot address specialized tasks that have not previously been trained.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#a-critical-reflection-on-large-language-models",
    "href": "ML_w01_MLIntroduction.html#a-critical-reflection-on-large-language-models",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "4 A Critical Reflection on Large Language Models",
    "text": "4 A Critical Reflection on Large Language Models\n\nThe latest advances in artificial intelligence are the Large Language Models with the capabilities to generate data. This however is just based on randomization and merger of previously trained\nSee the GPT-4.o video.\nKey question: how will this influence our future?",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#quiz-procedure-and-entry-questionnaire",
    "href": "ML_w01_MLIntroduction.html#quiz-procedure-and-entry-questionnaire",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "5 Quiz Procedure and Entry Questionnaire",
    "text": "5 Quiz Procedure and Entry Questionnaire\n\nPlease return the entry questionnaire by the end of this meeting. It will me tailor this course to your specific needs.\nPlease use our course’s ELEARNING site to practice the ungraded Quiz00.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "ML_w01_MLIntroduction.html#historical-background",
    "href": "ML_w01_MLIntroduction.html#historical-background",
    "title": "Week01: An Introduction to Machine Learning, Matrices and R Installation",
    "section": "6 Historical Background",
    "text": "6 Historical Background\n\nAI objective: Perform predictions of target outcomes \\(y\\) or future actions by using specific AI rules (i.e., functions \\(f(x)\\)) based on observable feature information \\(x\\).\n\n\n\n\nFigure 1.1 - Artificial intelligence, machine learning, and deep learning\n\n\n\n6.1 Artificial Intelligence\n\nIn the 1950 computer pioneers start asking the question “Can computers think?”.\nUltimately its definition was reduced to automating intellectual tasks normally performed by humans.\nThe tasks were hardcoded as “rules” and lead to symbolic AI and expert systems in the 1980.\nAI can solve well-defined logical problems, but it cannot handle complex and fuzzy problems.\n\n\n\n\nFigure 1.2 - Machine learning: a new programming paradigm\n\n\n\n\n\n6.2 Machine Learning (ML)\n\nThe objective of machine learning is to develop computer algorithms that transform data into intelligent actions.\nML started in the 1990s asking the question “can a computer discover hidden rules by processing data?”\nIn supervised ML humans input data and answers, subsequently the computer finds rules connecting data with answers (target variable).\n\nThe target variable can be metric (value prediction) or categorical (class prediction)\n\nThis is achieved by training the “machine” with the available features and their associated outcome.\nOnce rules are established with some confidence, they can be used to perform predictions of answers based on new input features.\nIn unsupervised ML an algorithm searches for a structure (i.e., a signal or pattern) in the data to establish homogeneous groups of observations.\nThe identified groups are expected to be as distinct (heterogeneous) as possible from each other.\nThis allows, for instance, [a] to generalize the characteristics of individual observations by their associated group attributes, or [b] find uniquely outlying observations that don’t match up with any of the identified groups.\nML learning algorithms become progressively less structured, allowing us to model more and more complex patterns in the data. This loss of structure hinders interpretation of the models and diminishes the ability to generalize the models to independent data sources.\nIn general, ML is tightly related to statistics but is applied on large and complex data sets for which standard parametric statistical models become too rigid.\nProgress in ML is mainly driven by statisticians and computer scientists as well as advancing technology and automated data collection.\nThe different backgrounds of statisticians and computer science can lead to inconsistencies and rediscovery of procedures and problems, which already have been address in the other discipline.\nML is applied on big data which are characterized by [a] volume, [b] velocity, [c] variety (heterogeneity) and [d] veracity (that is, certainty or the lack thereof).\nBig unbiased data are usually needed to learn patterns in highly complex functional relationships among the features.\nCriticism of ML:\n\nIt is data driven and ideas are “proven” empirically rather than being rooted in a logical organized deductive theoretical system.\nThere is always the risk to overfit the functional relationship so it just becomes training sample specific rather than being able to generalized to new external data.\n\n\n\n\n\n6.3 Types of Models\n\n6.3.1 Descriptive Models\n\nAims at describing the data and finding underlying patterns in observed data.\nThe employed method of describing the data already acts as filter by letting specific characteristics of the data pass through whereas other characteristics of the data are ignored. We need to understand the properties of these filters.\nExamples: Loess smoother in a scatterplot.\n\n\n\n6.3.2 Inferential Model\n\nEvaluates theories, hypotheses and assumptions based on a set of given sample data and statistical models.\nThat is, inferential models start with a conjecture about the underlying data generating process.\nDeeply rooted in probability theory to capture the inherent randomness in the sample data.\nAllows data analyses in the Bayesian domain in which the conjectures have a prior distribution, the observed sample data follows a likelihood model, and the resulting model parameters have a posterior distribution.\n\n\n\n6.3.3 Predictive Models\n\nThe objective of predictive models is to provide the most accurate prediction of an outcome based on independent features.\nThe simplest form of predictive methods is the multiple linear regression:\n\nThe underlying data generating processes is assumed to have the form \\[Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\cdots + \\beta_K \\cdot X_K + \\varepsilon\\]\nThe estimated prediction for the \\(i^{th}\\) observation in the \\(K + 1\\) estimated parameters \\(\\beta_k\\) becomes \\[\\hat{y}_i = b_0 + b_1 \\cdot x_{i1} + \\cdots + b_K \\cdot x_{iK}\\]\n\nThe predictive model is trained on sample data in the hope that the model generalizes well to independent objects described by their features. Therefore, the model is evaluated based on test data.\nCaveat: if there is no signal in the data not even the most sophisticated model will lead to meaningful predictions.\nA predictive model can always be designed in a way that its prediction on training data is perfect but leads to a disastrous prediction for independent test data.\nThus, a perfect balance must be found between the model bias and the error variance of the predictive model structure. This leads for predictive models to:\n\nEvaluating different statistics describing the model fit.\nProperly transforming the data to become adequate for the underlying model structure by feature engineering (see EPPS6323).\nThis may require tuning hyper-parameters, perhaps by resampling techniques and cross-validation.\nReducing the dimensionality (i.e., complexity) of the model to eliminate irrelevant features using regularization techniques.\nThe machine learning approaches of tree-based models and support vector machines are historically key techniques, but their performance is now surpassed by neural networks. Nevertheless, the simplicity and lower data requirements of simpler models is still a main advantage.\n\nComplex models are mainly black boxes, and their interpretation becomes virtually impossible.\nTraining predictive models is usually an iterative process.\n\n\n\n\n6.3.4 Example of a Predictive Model: Deep Learning\n\nDeep Learning (DL) is a specialized subfield of predictive modelling based neural networks. Small computational neural networks emerged in the early 1960ies. They were able to handle only a small set of hidden layers and thus provided a coarse approximation of the underlying data structure.\nDeep learning is based on very simple data processing units. A sheer multitude of the units are pooled together into layers which are stringed together in a non-linear manner.\nThe objective is to find an increasingly more accurate functional representations of the outcome \\(y\\).\nWith the increase in affordable computing power, development of numerical robust algorithms and an increasing availability of large data, DL with many hidden layers became prominent in the early 2010.\n“Deep” does not refer to a deeper understanding of the task.\nDL is based on the mathematical framework of neural networks but does not have any relationship to neurobiology (i.e., neurobiology studies how our brains work).\nOne can think of neural networks as a series of non-linear filters (the layers) that distill the input information which is measured by the features \\(X\\) into a “purified version” that is closely related to the prescribed output \\(y\\).\nBuilding blocks of understanding neural networks are “logistic regression” and gradient descent optimizers.\nA sequence of layers weight feature input vectors \\(\\mathbf{x}_i\\) that will generate an output label \\(y_i\\): \\[y_i = f_{NN}(\\mathbf{x}_i) = f_3(f_2(f_1(\\mathbf{x}_i)))\\]\nDL involves more than two hidden layers. In \\(y_i = f_{NN}(\\mathbf{x}_i) = f_3(f_2(f_1(\\mathbf{x}_i)))\\) the function \\(f_3\\) is the final output layer.\n\n\n\n\nFigure 1.9 - The loss score is used as a feedback signal to adjust the weights\n\n\n\nThe objective of specifying the \\(l\\)-th layer functions \\(f_l()\\) is to minimize the prediction error, which is measured by a loss score functions.\n\n\n\n\n\n6.4 Criticism\n\nBlack box approach: The focus of machine learning is to perform accurate predictions rather than to provide meaningful explanations and insights into the underlying structure within data (e.g., tests of regression models in econometrics)\nMachine learning cannot ask relevant questions. Thus, an analyst is needed to train the machine with relevant data.\nMachine learning algorithms are always biased towards an inherent structure of the input training data (recall earlier filter argument).\nMachine learning needs to find a balance of distinguishing noise from relevant structures to avoid overfitting the model to the just the training data.\n\n\n\n\n6.5 Steps of Machine Learning Process\n\nCollecting data\nExploring and preparing data. Feature and target engineering.\nTraining an appropriate model to the data (or set of models and average their predictions)\nEvaluation of the model’s performance\nImproving model performance",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk01-Machine Learning Introduction"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html",
    "href": "AG_w02_Chap02BivarReg.html",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "",
    "text": "Def. causality: The change in a cause induces in a predictable manner a change in an effect.\nThe role of endogenous (effect) and exogenous (cause) variable depends on the context. E.g.: The relationship between income and education can either be:\n\n[a] the parent’s income determines the child’s education\n[b] A person’s earlier education level determines a person’s later income\n\nTime ordering: The cause \\(X\\) precedes the effect \\(Y\\). This is the closest we get in empirical research to causality.\n\nNote: in the income-education example the exogenous variable always precedes the endogenous variable.\n\nCo-variation: The variables \\(X\\) and \\(Y\\) jointly covary in a random but systematic manner (not causality).\n\nNo statement can be made about the influence that one variable has on the variation of another variable. Thus, there is no cause and effect relationship.\nBoth variables seem to just correlate in their variation for what-ever reasons.\n\nSpurious relationships: The empirical covariation between \\(X\\) and \\(Y\\) can be induced by their joint relationship to another variable \\(Z\\) (or a set of variables).\n\nThe variable \\(Z\\) is called confounder.\nBivariate regression analysis cannot control for confounding effects because only the two variables \\(X\\) and \\(Y\\) define the regression model. We will see in Chapter 3 that multiple regression models can control for confounding effects.\n\nTheory based research (or confirmatory research) makes predictive statements about the [a] strength, [b] direction, [c] shape (linear or nonlinear) and [d] conditional distributions \\(p(Y|X)\\) of \\(Y\\).\nRegression traces the conditional distributions of \\(Y\\) given a particular value of \\(X\\) by means of the conditional expectation.\nFOX Fig. 6.1 assumes positive linearity and equal conditional distributions. We get the conditional expectations \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1}) = \\beta_0 + \\beta_1 \\cdot x_{i1}\\)\n\n\n\n\nFOX Figure 6.1\n\n\n\n\n\n\nFor the \\(i\\)-th observation the population model with an underlying data generating process becomes the regression model \\(y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\varepsilon_i\\).\nNone of the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) nor the error term \\(\\varepsilon_i\\) are directly observable using an empirical sample.\nThe parameter \\(\\beta_0\\) is called the intercept and the parameter \\(\\beta_1\\) is the slope.\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are not indexed by the individual observations \\(i\\). Thus, they are constant across all observations.\nIn the population the error term \\(\\varepsilon_i\\), also called disturbance, is directly associated to the \\(i\\)-th observation.\nThe predicted value of the estimated model is \\(\\hat{y}_i = b_0 + b_1 \\cdot x_{i1}\\) with the estimated residual \\(e_i = y_i - \\hat{y}_i\\).\n\nSome people also write \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the estimated parameters and \\(\\hat{\\varepsilon}_i\\) for the residuals.\nHamilton uses the symbol \\(K\\) for the number of estimated parameters including the currently invisible intercept. Thus, for bivariate regression analysis the number of estimated parameters is \\(K = 2\\).\n\n\n\n\n\n\n\nA sample of just two data-points determines exactly a line, which is characterized by two parameters. Consequently, we will lose two degrees of freedom when performing bivariate regression analysis.\nThe linear regression model separates\n\nthe conditional expectations (conditional on the observation \\(x_{i1}\\)) \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1}) = \\beta_0 + \\beta_1 \\cdot x_{i1}\\) (systematic component)\nfrom the unobserved disturbances \\(\\varepsilon_i\\) (random component) that are unique for each observation.\n\nThe disturbances vary around the systematic component. The set of \\(n\\) disturbances has only \\(n - K\\) degrees of freedom.\nGaining further understanding of any potentially inherent pattern in the residuals (unexplained part) enhances our knowledge.\n\\(\\Rightarrow\\) For this reason, extensive residual analysis is part of regression analysis (see Chapter 4).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#regression-and-causality",
    "href": "AG_w02_Chap02BivarReg.html#regression-and-causality",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "",
    "text": "Def. causality: The change in a cause induces in a predictable manner a change in an effect.\nThe role of endogenous (effect) and exogenous (cause) variable depends on the context. E.g.: The relationship between income and education can either be:\n\n[a] the parent’s income determines the child’s education\n[b] A person’s earlier education level determines a person’s later income\n\nTime ordering: The cause \\(X\\) precedes the effect \\(Y\\). This is the closest we get in empirical research to causality.\n\nNote: in the income-education example the exogenous variable always precedes the endogenous variable.\n\nCo-variation: The variables \\(X\\) and \\(Y\\) jointly covary in a random but systematic manner (not causality).\n\nNo statement can be made about the influence that one variable has on the variation of another variable. Thus, there is no cause and effect relationship.\nBoth variables seem to just correlate in their variation for what-ever reasons.\n\nSpurious relationships: The empirical covariation between \\(X\\) and \\(Y\\) can be induced by their joint relationship to another variable \\(Z\\) (or a set of variables).\n\nThe variable \\(Z\\) is called confounder.\nBivariate regression analysis cannot control for confounding effects because only the two variables \\(X\\) and \\(Y\\) define the regression model. We will see in Chapter 3 that multiple regression models can control for confounding effects.\n\nTheory based research (or confirmatory research) makes predictive statements about the [a] strength, [b] direction, [c] shape (linear or nonlinear) and [d] conditional distributions \\(p(Y|X)\\) of \\(Y\\).\nRegression traces the conditional distributions of \\(Y\\) given a particular value of \\(X\\) by means of the conditional expectation.\nFOX Fig. 6.1 assumes positive linearity and equal conditional distributions. We get the conditional expectations \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1}) = \\beta_0 + \\beta_1 \\cdot x_{i1}\\)\n\n\n\n\nFOX Figure 6.1\n\n\n\n\n\n\nFor the \\(i\\)-th observation the population model with an underlying data generating process becomes the regression model \\(y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\varepsilon_i\\).\nNone of the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) nor the error term \\(\\varepsilon_i\\) are directly observable using an empirical sample.\nThe parameter \\(\\beta_0\\) is called the intercept and the parameter \\(\\beta_1\\) is the slope.\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are not indexed by the individual observations \\(i\\). Thus, they are constant across all observations.\nIn the population the error term \\(\\varepsilon_i\\), also called disturbance, is directly associated to the \\(i\\)-th observation.\nThe predicted value of the estimated model is \\(\\hat{y}_i = b_0 + b_1 \\cdot x_{i1}\\) with the estimated residual \\(e_i = y_i - \\hat{y}_i\\).\n\nSome people also write \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the estimated parameters and \\(\\hat{\\varepsilon}_i\\) for the residuals.\nHamilton uses the symbol \\(K\\) for the number of estimated parameters including the currently invisible intercept. Thus, for bivariate regression analysis the number of estimated parameters is \\(K = 2\\).\n\n\n\n\n\n\n\nA sample of just two data-points determines exactly a line, which is characterized by two parameters. Consequently, we will lose two degrees of freedom when performing bivariate regression analysis.\nThe linear regression model separates\n\nthe conditional expectations (conditional on the observation \\(x_{i1}\\)) \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1}) = \\beta_0 + \\beta_1 \\cdot x_{i1}\\) (systematic component)\nfrom the unobserved disturbances \\(\\varepsilon_i\\) (random component) that are unique for each observation.\n\nThe disturbances vary around the systematic component. The set of \\(n\\) disturbances has only \\(n - K\\) degrees of freedom.\nGaining further understanding of any potentially inherent pattern in the residuals (unexplained part) enhances our knowledge.\n\\(\\Rightarrow\\) For this reason, extensive residual analysis is part of regression analysis (see Chapter 4).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#key-assumptions-of-regression-analysis",
    "href": "AG_w02_Chap02BivarReg.html#key-assumptions-of-regression-analysis",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "2 Key Assumptions of Regression Analysis",
    "text": "2 Key Assumptions of Regression Analysis\n\nKey assumptions:\n\nThe relationship between the independent variable and the dependent variable is linear (or can be transformed to linearity).\nThe independent variable \\(X\\) is fixed (that is, it is deterministic variables and not influenced by random effects).\nNote: should it for some reason be influenced by randomness, then we need at least assume that it is uncorrelated with the population disturbances. There is a strong risk of obtaining biased estimates for the regression coefficients. See instrumental variable estimation later.\nDisturbances at any level of \\(x_i\\) have identical distributions, with zero mean and constant variance.\nDisturbances are in general assumed to be independent (uncorrelated) among each other.\nThe independence and identical distribution assumption is abbreviated by statement that the disturbances are i.i.d. (independently identically distributed)\nThe additional i.i.d. normality assumption of the disturbances allows statistical significance testing of the estimated regression model in small samples.\n\nNotes:\n\nArguing purely statistically, only the disturbances are required to be normal i.i.d. Neither \\(Y\\) nor \\(X\\) need to follow a normal distribution.\nHowever: A joint normal distribution of the variables \\(X\\) and \\(Y\\) is highly desirable because it guarantees linearity of their relationship and a balanced distribution of all data points in the scatterplot; thus, minimizing any impact of potential outliers and counteracting potential heteroscedasticity.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#ordinary-least-squares-estimation-and-variance-decomposition",
    "href": "AG_w02_Chap02BivarReg.html#ordinary-least-squares-estimation-and-variance-decomposition",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "3 Ordinary Least Squares Estimation and Variance Decomposition",
    "text": "3 Ordinary Least Squares Estimation and Variance Decomposition\n\nWhat is the inter-pretation of the intercept and the slope parameters?\nThe method of ordinary least squares fits a straight line through the scatterplot point cloud that minimizes the sum of the squared regression residuals.\nAs long as the model has an intercept the regression line always goes through means of \\(X\\) and \\(Y\\), i.e., the point \\((\\bar{x}, \\bar{y})\\) will be on the regression line.\n\n\n\n\nFigure 13-11, 13-12, 13-13 - Decomposition and variance\n\n\n\nDefinition of variance terms: Total Sum of Squares (TSS), Residual Sum of Squares (RSS) and Explained Sum of Squares (ESS)\n\n\\[TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\text{ with } df = n - 1\\]\n\\[RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\text{ with } df = n - K\\]\n\\[ESS = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 \\text{ with } df = K - 1\\]\n\nThe validity of decomposition \\(TSS = ESS + RSS\\) relies on the fact that the product \\(\\sum(y_i - \\hat{y}_i) \\cdot (\\hat{y}_i - \\bar{y}) = 0\\), which can be easily shown later using matrix algebra.\nKey consequence: \\(Cov(\\hat{y}, e) = Cov(b_0 + b_1 \\cdot X, e) = b_1 \\cdot \\underbrace{Cov(X, e)}_{=0} = 0\\)\nTherefore, the incorporated independent variables cannot explain any remaining variation in the regression residuals.\nThat is, the independent variable has exhausted all relevant information in the regression model.\n\n\n\n3.1 Estimation of Slope and Intercept\n\nIn ordinary least squares regression, we want to minimize the sum of squared residuals RSS with the residuals being defined by \\(e_i = y_i - \\underbrace{(b_0 + b_1 \\cdot x_{i1})}_{\\hat{y}_i}\\) (see HAM p 33), that is,\n\n\\[\\min_{b_0, b_1} RSS = \\min_{b_0, b_1} \\sum (y_i - \\underbrace{(b_0 + b_1 \\cdot x_{i})}_{=\\hat{y}_i})^2\\]\n\nMinimizing this function is done by setting its first derivatives equal to zero and solving for the unknown parameters \\(b_0\\) and \\(b_1\\) leads to:\n\n\\[\\frac{\\partial RSS}{\\partial b_0} = -\\sum y_i + nb_0 + b_1 \\sum x_{i1} \\equiv 0\\]\n\\[\\frac{\\partial RSS}{\\partial b_1} = -\\sum x_i \\cdot y_i + b_0 \\sum x_{i1} + b_1 \\sum x_{i1}^2 \\equiv 0\\]\n\nThe first equation shows that the regression line must go through the means \\((\\bar{x}, \\bar{y})\\) of the independent and the dependent variables because,\n\n\\[-\\sum y_i + nb_0 + b_1 \\sum x_{i1} \\equiv 0\\] \\[\\Rightarrow \\sum y_i = nb_0 + b_1 \\sum x_{i1} \\quad | \\div n\\] \\[\\Rightarrow \\bar{y} = b_0 + b_1 \\cdot \\bar{x}\\]\nTherefore, \\(b_0 = \\bar{y} - b_1 \\cdot \\bar{x}\\) and the sum of the regression residuals is zero, i.e., \\(\\sum e_i = 0\\).\n\nInserting the means expression for \\(b_0\\) into \\(\\frac{\\partial RSS}{\\partial b_1} = 0\\) and solving for \\(b_1\\) gives\n\n\\[b_1 = \\frac{n \\cdot \\sum y_i \\cdot x_{i1} - \\sum x_{i1} \\cdot \\sum y_i}{n \\cdot \\sum x_{i1}^2 - (\\sum x_{i1})^2}\\]\n\nEquivalent expressions for the slope parameter \\(b_1\\) in bivariate regression are\n\n\\[b_1 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y}) \\cdot (x_i - \\bar{x})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\\[= \\frac{s_{YX}}{s_X^2} = r \\cdot \\frac{s_Y}{s_X}\\]\nor \\(b_1 = \\frac{Cov[Y,X]}{Var[X]}\\) (see Hamiliton p 294).\nQuestion: What would the slope be if we regress \\(X\\) on \\(Y\\). Answer: \\(b_{X|Y} = r \\cdot \\frac{s_X}{s_Y}\\) or equivalently \\(\\frac{Cov[Y,X]}{Var[Y]}\\).\n\n\n\n3.2 \\(R^2\\) and Adjusted \\(R_{adj}^2\\) (REVIEW, SKIPPED)\n\nThe goodness of fit measure (proportion of explained variance relative to the total variance) is defined by\n\n\\[R^2 \\equiv \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\\]\n\nThe adjusted goodness of fit takes the degrees of freedom into account because. Note, the more variables are enter into the regression equation, the better the fit of the model will be or at least stay the same (recall the perfect fit of the regression through two points)\n\n\\[R_{adj}^2 \\equiv 1 - \\frac{RSS/(n - K)}{TSS/(n - 1)}\\]\nWhen \\(n\\) is large relative to \\(K\\) then the difference between the adjusted and the ordinary \\(R^2\\) is negligible.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#statistical-inference-on-the-unknown-regression-parameters",
    "href": "AG_w02_Chap02BivarReg.html#statistical-inference-on-the-unknown-regression-parameters",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "4 Statistical Inference on the Unknown Regression Parameters",
    "text": "4 Statistical Inference on the Unknown Regression Parameters\n\nQuestion: Why are estimated regression parameters \\(b_0\\) and \\(b_1\\) random variables that have their own distribution?\nAnswer: Repeated samples from a population will yield different sets of observations.\nThus, the estimated regression parameters will differ from sample to sample and therefore will have a distribution.\n\n\n\n\nFigure 12-9 - Sampling for a regression model\n\n\n\nAssuming i.i.d. disturbances \\(\\varepsilon_i\\), then the estimated regression parameters \\(b_0\\) and \\(b_1\\) will be asymptotically normal distributed with \\(b_0 \\sim \\mathcal{N}(\\beta_0, \\sigma_{b_0}^2)\\) and \\(b_1 \\sim \\mathcal{N}(\\beta_1, \\sigma_{b_1}^2)\\) due to the central limit theorem. Thus, if other assumptions are satisfied, the estimated parameters are unbiased with \\(E(b_0) = \\beta_0\\) and \\(E(b_1) = \\beta_1\\).\nThe square roots of the estimated parameter variances \\(\\hat{\\sigma}_{b_0}^2\\) and \\(\\hat{\\sigma}_{b_1}^2\\) are called standard errors of the estimated regression coefficients.\nSeveral equations make use of the residual standard deviation \\(s_e = \\sqrt{\\frac{RSS}{n - K}}\\). It is also called the Root Mean Square Error.\nOne can show that the standard errors of the regression parameters are\n\n\\[\\sqrt{Var(b_1)} = SE_{b_1} = \\frac{s_e}{\\sqrt{TSS_X}} \\text{ and } \\sqrt{Var(b_0)} = SE_{b_0} = s_e \\cdot \\sqrt{\\frac{1}{n} + \\frac{\\bar{X}^2}{TSS_X}} \\text{ with } TSS_X = \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\]\n\nBoth standard errors depend on the spread \\(TSS_X\\) of the independent variable \\(X\\). Notice that with increasing spread \\(TSS_X\\) the a regression vary less and thus becomes more precise (i.e., they have smaller standard errors).\n\n\n\n\nFigure 14-5 - \\(S_b\\) as a function of the variability in X\n\n\n\nLow uncertainty and unbiasedness of any estimates are desirable properties.\nSince we work with estimates of standard errors, the distribution of the test statistic \\(t = \\frac{b_1 - \\beta_1}{SE_{b_1}}\\) follows a t-distribution with \\(df = n - K\\) rather than a standard normal distribution. This t-statistic resembles a z-transformed variable around a hypothetical population value \\(\\beta_1\\).\nIf the exogenous variable \\(X\\) does not explain any variation in the endogenous variable \\(Y\\) then the slope estimate \\(b_1\\) does not differ significantly from zero.\n\nTherefore, the null hypothesis in this case becomes \\(H_0: \\beta_1 = 0\\) and the alternative hypothesis \\(H_1: \\beta_1 \\neq 0\\)\nConsequently, the test statistic reduces to \\(t_{obs} = (b_1 - 0) / SE_{b_1} = b_1 / SE_{b_1}\\)\n\nThere are scenarios where a different base-line level, such as \\(H_0: \\beta_1 = 1\\) against \\(H_1: \\beta_1 \\neq 1\\), becomes relevant. See the later discussion of elasticity.\nAn alternative test statistic is based on the F-statistics. More about this in the multiple regression chapter.\nIf a theory suggests that the regression parameter is expected to be positive (or negative, respectively), then a one-sided test becomes appropriate, that is\n\n\\(H_0: \\beta_1 \\leq 0\\) against \\(H_1: \\beta_1 &gt; 0\\) for a positivity assumption, or\n\\(H_0: \\beta_1 \\geq 0\\) against \\(H_1: \\beta_1 &lt; 0\\) for a negativity assumption.\nIn those cases, as long as the sign of estimated regression coefficient points towards the direction of the alternative hypothesis, compared to the reported prob-value by the software (it is based a two-sided test scenario) the true prob-value is only half as large because just a one-tailed probability is used.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#confidence-intervals",
    "href": "AG_w02_Chap02BivarReg.html#confidence-intervals",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "5 Confidence Intervals",
    "text": "5 Confidence Intervals\n\nA \\(1 - \\alpha\\) confidence interval, e.g., 95% interval, around the hypothetical population parameter \\(\\beta_1\\) is given by\n\n\\[\\Pr(t_{\\alpha/2, df}^{-} &lt; \\frac{b_1 - \\beta_1}{SE_{b_1}} &lt; t_{1-\\alpha/2, df}^{+}) = 1 - \\alpha\\]\n\\[\\Rightarrow \\Pr(\\underbrace{b_1 - SE_{b_1} \\cdot t_{1-\\alpha/2, df}}_{+} &lt; \\beta_1 &lt; \\underbrace{b_1 - SE_{b_1} \\cdot t_{\\alpha/2, df}}_{-}) = 1 - \\alpha\\]\nIf we assume \\(H_0: \\beta_1 = 0\\) is true then the confidence interval covers the value zero, that is,\n\\[0 \\in [b_1 - SE_{b_1} \\cdot t_{1-\\alpha/2, df}, b_1 + SE_{b_1} \\cdot t_{1-\\alpha/2, df}]\\]\nThus the zero hypothesis cannot be rejected at the significance level \\(\\alpha\\), because the estimate regression parameter \\(b_1\\) is not statistically different from zero.\n\nThere are two confidence intervals associated with the predictions of the dependent variable \\(\\hat{Y}_i\\): [a] one for the estimated regression line and [b] another for an individual point prediction \\(\\hat{Y}_i\\)\n\nFor the regression line (book calls it mean expected value) we need to account simultaneously for the sampling variation in the intercept \\(b_0\\) and slope \\(b_1\\).\nIn this case the variation of the predicted line \\(\\hat{Y}_{i,line}\\) around the true population line at a given value \\(X_i\\) has the standard error of\n\\[SE_{\\hat{Y}_{i,line}} = s_e \\sqrt{\\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{TSS_X}}\\]\n[b] For an individual point prediction of \\(Y_{i,point}\\) (book calls it an individual case’s Y value) we need to account not only for the sampling variation of the estimated regression parameters but also for the variation of the individual observation at a given value \\(X_i\\), that is, the potential disturbance \\(\\varepsilon_i\\).\nTherefore, the standard error increases by one unit of the residual standard deviation:\n\\[SE_{\\hat{Y}_{i,point}} = s_e \\sqrt{1 + \\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{TSS_X}}\\]\n\n\n\n\n\nFigure 2.7 - Confidence and prediction intervals around regression line\n\n\n\nThe term \\((X_i - \\bar{X})^2\\) in both equations signifies that the prediction standard errors will increase the further we move away from the mean \\(\\bar{X}\\).\nFurthermore, the larger the sample size \\(n\\) gets the narrower the confidence intervals will become, because \\(TSS_X\\) in the denominator is increasing while \\(1/n\\) is shrinking.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#regression-through-the-origin",
    "href": "AG_w02_Chap02BivarReg.html#regression-through-the-origin",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "6 Regression through the Origin",
    "text": "6 Regression through the Origin\n\nRegression through the origin should only be considered if there are strong theoretical reasons to assume that the intercept \\(\\beta_0 = 0\\).\nA test outcome, that indicates that the intercept is zero, does not justify dropping the intercept.\nIn R we can suppress the intercept with the statement model.lm &lt;- lm(y~-1+x), where the negative term -1 denotes to drop the intercept\nMost of the standard test statistics of regression analysis become invalid if we suppress the intercept. For instance:\n\nWithout an intercept the sum of the regression residuals will not necessarily remain zero, that is, \\(0 \\neq \\sum_{i=1}^{n} e_i\\)\nThe goodness of fit measure \\(R^2\\) is no longer defined because it is based on variations around the mean.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#problems-associated-with-bivariate-regression-analysis",
    "href": "AG_w02_Chap02BivarReg.html#problems-associated-with-bivariate-regression-analysis",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "7 Problems Associated with Bivariate Regression Analysis",
    "text": "7 Problems Associated with Bivariate Regression Analysis\n\nOmitted additional relevant variable. The estimate regression parameters become biased (on average, over many samples from the population, not identical to the population parameter).\nNon-linear relationship. Mis-specified model. Perhaps the residuals indicate autocorrelation.\nNon-constant disturbance variance (heteroscedasticity). The variance of the regression residuals changes systematically with either [a] an independent variable or [b] some other external factors currently not considered in the regression model.\nAutocorrelation. The disturbances are no longer independent.\nNon-normal disturbances. Test statistics, that are based on the normal assumptions, become unreliable in small samples.\nInfluential cases. Regression analysis is not resistant to outliers.\nLarge, squared residuals \\(\\hat{e}_i^2 = (y_i - \\hat{y}_i)^2\\) have a strong impact on the ordinary least squares estimation (a squared large distance becomes even larger).\n\n\n\n\nFigure 2.10 - “All clear” e-versus-\\(\\hat{Y}\\) plot (artificial data)\n\n\n\nSome of these problems can be identified by an inspection of the regression residuals.\nRecall: the residuals \\(e_i\\) are uncorrelated with the predicted values \\(\\hat{Y}_i = b_0 + b_1 \\cdot X_i\\). The same holds for the exogenous variable \\(X_i\\).\nTherefore, a scatterplot of the residuals against either the predicted value or an independent variable, which already is included in the model, should not show a systematic pattern.\nSome violations are indicated in the plots on the right.\n\n\n\n\nFigure 2.11 - Examples of trouble seen in e-versus-\\(\\hat{Y}\\) plots (artificial data)",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#transformations-of-the-endogenous-and-exogenous-variables-following-hamiltons-naïve-approach",
    "href": "AG_w02_Chap02BivarReg.html#transformations-of-the-endogenous-and-exogenous-variables-following-hamiltons-naïve-approach",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "8 Transformations of the Endogenous and Exogenous Variables (Following Hamilton’s Naïve Approach)",
    "text": "8 Transformations of the Endogenous and Exogenous Variables (Following Hamilton’s Naïve Approach)\n\nA scatterplot of \\(Y \\sim X\\) or the analysis of the regression residuals may indicate that a transformation of the dependent and/or the independent variables are required.\n\nto fix influential cases by pulling them into the population\nto fix a curvilinear relationship and making the relationship linear\nto bring the distribution of the regression residuals closer to a symmetric distribution and/or desirable the normal distribution.\nto stabilize the variability of the regression residuals by a transformation of the dependent variable.\n\nThis requires experimentation. Both variables could be simultaneously transformed, that is, \\(Y_i^* = f(Y_i)\\) and \\(X_i^* = g(X_i)\\) and the regression residuals must be evaluated until a set of transformations is found, for which the underlying regression assumptions are not violated.\nThe functions for the dependent and independent variable \\(f(\\cdot)\\) and \\(g(\\cdot)\\), respectively, can be different, e.g., have different \\(\\lambda\\)-parameters.\nIn the transformed model predictions can be made \\(\\hat{Y}_i^* = b_0 + b_1 \\cdot X_i^*\\)\nThe inverse function is applied on the predicted values \\(\\hat{Y}_i = f^{-1}(\\hat{Y}_i^*)\\) and the curvilinear relationship between \\(\\hat{Y}_i\\) and \\(X_i\\) can be graphed in the original measurement units.\nFor the Box-Cox transformation \\(Z^* = \\frac{Z^\\lambda - 1}{\\lambda}\\) its inverse transformation is \\(Z = (Z^* \\cdot \\lambda + 1)^{1/\\lambda}\\).\n\n\n\n\nFigures 2.4, 2.15, 2.17 - Scatterplot with regression line, Transformed water use versus transformed household income, Curvilinear relation of water use to income",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#statistically-rigorous-transformation-approach",
    "href": "AG_w02_Chap02BivarReg.html#statistically-rigorous-transformation-approach",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "9 Statistically Rigorous Transformation Approach",
    "text": "9 Statistically Rigorous Transformation Approach\n\nThe naïve approach exhibits two problems:\n\nThe objective is not that the dependent variable is symmetrically or normally distributed, but rather that the regression residuals are symmetrically or normal distributed.\nThe predicted values after the inverse transformation back into the original measurement units become conditional medians rather than conditional expectations \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1})\\)\n\nTo overcome these problems the best approach is\n\nTransform all independent variable to become approximately symmetry. This enhances the linearity of the model \\(\\Rightarrow X^* = g(X)\\).\nFind a transformation \\(f(\\cdot)\\) for the dependent variable \\(Y\\) so that the regression residuals become approximately normal distributed \\(\\Rightarrow e^* = f(Y) - b_0^* + b_1^* \\cdot X^*\\) with \\(e \\sim N(0, \\sigma^2)\\).\nPredict \\(\\widehat{f(Y)} = b_0^* + b_1^* \\cdot X^*\\) in the transformed system. Note: all model assumptions should be satisfied in the transformed system.\nMap \\(\\widehat{f(Y)}\\) back into the original measurement units either by the inverse transformation\n\n\n\\(f^{-1}(\\widehat{f(Y)})\\) for the predicted median \\(\\hat{Y}_{median}\\), or\n\n\n\\(f_{corrected}^{-1}(\\widehat{f(Y)})\\) for the predicted expected value \\(\\mu_{y_i|x_{i1}} \\equiv E(y_i | x_{i1})\\)\n\n\nThe script BOXCOXBIVARIATEREGRESSION.R outlines the procedure.\n\nThe same logic can be applied in multiple regression models with more than one independent variable.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w02_Chap02BivarReg.html#elasticity",
    "href": "AG_w02_Chap02BivarReg.html#elasticity",
    "title": "Week02: Hamilton Chapter 2 - Bivariate Regression Analysis",
    "section": "10 Elasticity",
    "text": "10 Elasticity\n\nTo estimate a non-linear exponential model\n\n\\[y = \\exp(b_0) \\cdot x^{b_1} \\cdot \\exp(\\varepsilon)\\]\nby linear regression the model can be transformed into the log-log form. This gives the transformed model of the form in the variables \\(Y\\) and \\(X\\):\n\\[\\ln(y) = b_0 + b_1 \\cdot \\ln(x) + \\varepsilon\\]\n\nIn this model the estimated regression coefficient \\(b_1\\) is interpreted as a relative rate of change (i.e., percentage change) at a given value \\(y_0\\) and \\(x_0\\)\n\n\\[b_1 = \\frac{\\%\\Delta y}{\\%\\Delta x} = \\frac{\\frac{\\Delta y}{y_0}}{\\frac{\\Delta x}{x_0}} = \\frac{\\frac{y_0 - y}{y_0}}{\\frac{x_0 - x}{x_0}}\\]\nWhich may be evaluated at any feasible value \\(x_0\\) and in particular for \\(\\bar{x}\\). The estimate \\(b_1\\) in the log-log model is called in economics the “elasticity”.\n\nTo learn more about the economic concept of elasticity start with http://en.wikipedia.org/wiki/Elasticity_(economics)\nThe value \\(b_1 = 1\\) is the neutral value where any relative change of \\(x\\) is equal to a relative change in \\(y\\).\nTherefore, the meaningful null hypothesis becomes \\(H_0: \\beta_1 = 1\\) against \\(H_1: \\beta_1 \\neq 1\\).\nInterpretation:\n\nFor \\(b_1 &gt; 1\\) one observes increasing rates of return, that is, \\(y\\) changes relatively faster than \\(x\\).\nWhereas for \\(0 &lt; b_1 &lt; 1\\) one observes decreasing rates of return.\n\nSee for example graphs of the exponential model \\(crime = \\exp(b_0) \\cdot enroll^{b_1}\\).\n\n\n\n\nFigure 4.5 - Graph of crime = enroll^β₁ for β₁&lt;1, β₁=1, and β₁&gt;1\n\n\n\nThe interpretation of models with mixtures of log-transformed variables is given in the table below:\n\n\n\n\nSummary of Functional Forms Involving Logarithms\n\n\nExample 2.11, in the log-log model, \\(\\beta_1\\) is the elasticity of \\(y\\) with respect to \\(x\\). Table 2.3 warrants careful study, as we will refer to it often in the remainder of the text.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk02-Hamilton Ch02: Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html",
    "href": "AG_w01_HamChap01.html",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Data analysis fundamental: Treat your data with respect to learn something about the underlying data generating process.\n\nData tell a story about the phenomena under investigation.\nAlways handle data and analysis results with a critical attitude and use common sense. Link the results back to your original observations.\nAlways ask yourself: Do the data or the generated analysis results make sense?\n\nDescribing the variability and distribution of a variable is the required first step of any data analysis.\nThe shape of an univariate distribution can have substantially impact on the outcome of statistical procedures.\nE.g.: Outliers or heavy tails may detrimentally influence the outcome of model calibrations and parameter estimations.\nMissing to account for the distribution of variables can force a researcher to redo their data analysis later.\nMost methods assume symmetricly or preferably normally distributed variables.\nTransformations to symmetry are discussed in Chapter 1.\nNote: statisticians use many more transformations under particular circumstances.\nE.g., we will encounter later the logit-transformation.\n\n\n\n\n\nDef. Central Limit Theorem: Let \\(X_1, X_2, \\ldots, X_n\\) be a random independent sample of size \\(n\\) drawn from an arbitrarily distributed population with expectation \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThen for large enough sample sizes \\(n\\), the sampling distribution of the arithmetic mean \\(\\bar{X}\\) is [a] asymptotically (i.e., as the sample size \\(n \\to \\infty\\)) normal distributed [b] with\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\\]\nProof for independent sample observations \\(X_i\\):\n\\[\nVar\\left(\\frac{1}{n} \\cdot \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\cdot \\sum_{i=1}^{n} Var(X_i) = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}\n\\]\nImplications:\n\nThus the standard error \\(s_{\\bar{X}} = \\sigma/\\sqrt{n}\\) of the mean \\(\\bar{X}\\) shrinks by \\(1/\\sqrt{n}\\) with increasing sample size.\nAnother implication of the central limit theorem is that the sum of a set of small random errors or shocks will lead to normal distributed total error.\nIn contrast, the product of a set of small random errors will lead to a log-normal distributed total error.\n\nExample: Central limit theorem with the R-script CENTRALLIMIT.R:\n\n\n\n\nCentral Limit Theorem demonstration showing convergence to normality for three different population distributions (uniform, bimodal, exponential) with sample sizes n=4, 16, and 256\n\n\n\n\n\n\n\nDistributions can be distinguished with regards the balance of their left and right tails:\n\nSymmetric distributions. Tails are balanced into either direction from a central value.\nNegatively skewed distributions (long tail into the negative direction)\nPositively skewed distributions (long tail into the positive direction). These distributions frequently emerge for variables with a binding lower origin (like zero income).\nExtreme skewness may hint at outliers that do not match the rest of the observed data.\n\nThe number of meaningful clusters of observations is described by the term modality:\n\nUni-modality refers to just one peak.\nBi-modality refers to two outstanding peaks\nMultimodality refers to more than two outstanding peaks.\n\nMultimodality may hint at a heterogeneous underlying data generating process in which the underlying process for observations in the first mode is different for observation in the second mode.\n\n\n\n\nFigure 3.9: Shapes of frequency distributions: (a) Normal, (b) Bimodal, (c) Negatively skewed, (d) Positively skewed\n\n\n\n\n\n\n\nTechnically, quantiles and percentiles are generated from a sorted list of the original data points \\(x_{[1]} \\leq x_{[2]} \\leq x_{[3]} \\leq \\cdots \\leq x_{[n-1]} \\leq x_{[n]}\\) where each observations has an assigned rank \\(i \\in \\{1,2,\\ldots,n\\}\\), with \\(i = 1\\) for the smallest observation and \\(i = n\\) for the largest observation.\nFor a give data value \\(\\mathbf{x}_{[i]}\\) the percentile approximates the proportion of sample observations less or equal to \\(\\mathbf{x}_{[i]}\\), that is, their cumulative distribution:\n\n\\[\n\\mathbf{p}_{[i]} = \\frac{i - \\frac{1}{2}}{n} \\approx \\mathbf{Pr}(X \\leq \\mathbf{x}_{[i]}) = \\int_0^{\\mathbf{x}_{[i]}} f(\\mathbf{x}) \\cdot d\\mathbf{x}.\n\\]\nNote that the \\(\\boldsymbol{\\alpha} = \\mathbf{0.5}\\) of the percentile equation \\(p(x_{[i]}) = \\frac{i - \\alpha}{n + (1 - \\alpha) - \\alpha}\\) has been chosen here.\n\nA quantile is the potentially fictious data value of a distribution, which is associated with a particular percentile value.\nImportant quantiles are:\n\n0.25 quantile also called \\(Q_1\\) quartile (25 % of the observations are smaller or equal to this quantile value)\n0.50 quantile also called the median (50 % of the observations are smaller or larger than the given quantile value)\n0.75 quantile also called \\(Q_3\\) quartile (75 % of the observations are smaller or equal to this quantile value and 25 % of the observations are larger than this value)\nA measure of spread is the inter-quartile range: \\(IQR = Q_3 - Q_1\\)\n\n\n\n\n\n\n\nConstruction of the box-plot\n\nDraw a box from \\(Q_1\\) to \\(Q_3\\). Mark the median \\(\\mathbf{Q_2}\\) in the center of the box with a line.\nDefinition of adjacent values \\(x_{low}^{adj} = \\min(x_{[i]} \\in (Q_1, Q_1 - 1.5 \\cdot IQR)\\) plus \\(x_{[i]}\\) in dataset\\()\\) and \\(x_{high}^{adj} = \\max(x_{[i]} \\in (Q_3, Q_3 + 1.5 \\cdot IQR)\\) plus \\(x_{[i]}\\) in dataset\\()\\).\nThe term \\(x \\in (a,b)\\) means, all \\(x\\)-values in the interval between \\(a\\) and \\(b\\).\nDraw the “fences” so they just include the smallest and largest data values \\(x_{low}^{adj}\\) and \\(x_{high}^{adj}\\), respectively.\nOutliers are in the interval \\([1.5 \\cdot IQR, 3.0 \\cdot IQR]\\) starting from \\(Q_1\\) below or \\(Q_3\\) above, respectively.\nSevere outliers are beyond that range \\((&gt; 3.0 \\cdot IQR)\\)\n\n\n\n\n\nBox-plot example showing monthly distributions\n\n\n\nUse of box-plots:\n\nEasy visual description of the distribution of a variable and potential outliers\nComparison of distributions for several variables side-by-side.\n\n\n\n\n\n\n\nCalculate the theoretical quantiles of a normally distributed random variable \\(Y_{[i]}\\) (assuming the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) were estimated from the sample data) based on the given sample percentiles \\(p_{[i]}\\) of the observed variable \\(x_{[i]}\\).\nQuantile-Normal Plot: Plot the theoretical normal distribution quantiles \\(Y_{[i]}\\) on the abscissa (X-axis) against their matching empirical distribution of \\(x_{[i]}\\) on the ordinate (Y-axis).\n\nInterpretation: - Diagonal with slope 1 =&gt; equal distributions. - Not a straight-line =&gt; different shapes.\n\n\n\nFigure 1.9: Quantile-normal plot of household water use (positively skewed)\n\n\n\n\n\nFigure 1.10: Quantile-normal plots reflect distribution shape - showing examples of Heavy Tails/High and Low Outliers, Light Tails/No Outliers, Positive Skew/High Outliers, Negative Skew/Low Outliers, Granularity, and Two Peaks/Central Gap\n\n\n\n\n\n\n\nImplications of the zero-sum property \\(\\sum_{i=1}^{n} (Y_i - \\bar{Y}) = 0\\): Assuming the mean is known, then \\(n - 1\\) observation can vary freely, whereas we can predict the last observation with certainty.\n\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n} (Y_i - \\bar{Y}) &= 0 \\\\\n\\Rightarrow \\quad \\sum_{i=1}^{n} Y_i &= n \\cdot \\bar{Y} \\\\\n\\Rightarrow \\quad Y_n &= n \\cdot \\bar{Y} - \\sum_{i=1}^{n-1} Y_i\n\\end{aligned}\n\\]\nThat implies that we loose one degree of freedom.\n\nImplication of the least squares property \\(\\min_{\\theta} \\sum_{i=1}^{n} (Y_i - \\theta)^2 \\Rightarrow \\theta = \\bar{Y}\\).\nLarge deviations have a strong impact on the estimated mean, variance etc. because the large deviations are squared\n\\(\\Rightarrow\\) Thus, large deviations pull the mean into their direction.\n\\(\\Rightarrow\\) Standard deviations are drastically inflated.\nLacking any other information, the arithmetic mean will become best predictor for the variable under investigation.\nThe deviations from the mean are the unexplained part or the residuals of the observations, i.e., \\(y_i = \\bar{y} + \\varepsilon_i\\).\nDefinition of total sum of squares: \\(TSS = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\\) or \\(TSS = \\sum_{i=1}^{n} Y_i^2 - n \\cdot \\bar{Y}^2\\).\nWhy is the population variance estimated with \\((n-1)\\) in the denominator, that is, by \\(s^2 = TSS/(n-1)\\):\nExplanation 1: If we calculate the mean from the sample then there are only \\(n - 1\\) “degrees of freedom” left because of the zero sum property of the mean.\nExplanation 2: The mean is calculated by minimizing the TSS.\nThus the sample mean always fits the observed sample data better than any unobserved but true population expectation \\(\\mu\\).\nFor the true expectation \\(\\mu\\), the TSS would be slightly larger. That is why the sample TSS needs to be inflated by dividing it by a slight smaller value than \\(n\\), that is, \\(n - 1\\).\nStandard deviation measures the variation in original units rather than in squared units.\n\n\n\n\n\n\nWhy does the distribution of the water consumption in the Concord dataset deviate from the normal distribution?\nReason: Fixed lower bound (negative water consumption impossible).\nSkewness and bounded/truncated distributions: For skewed distributions the notion of the center of the distribution (mean) becomes ambiguous and the median may be a better representation of the central tendency in the data.\nThe skewness is defined by \\(skew(X) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^3}{n \\cdot s_X^3}\\)\nThe normal distribution has a skewness of 0.\n\n\n\n\n\n\nThis lecture focuses on the more general Box-Cox transformation (note 11 on page 28 in Hamilton) rather than the slightly simpler power transformation, which is discussed in Hamilton. For both transformations, the general interpretation of the parameter \\(\\lambda\\) does not change.\nCauses for extreme observations: [a] skewed distributions, [b] measurement or recoding errors, [c] extreme but feasible events (perhaps not belonging to the population under investigation).\nThe power-transformation presented in book and the Box-Cox transformation only work for variables whose observations are all larger than zero.\nThe Box-Cox transformation is a generalization of the power transformation: \\(Y = \\frac{X^{\\lambda} - 1}{\\lambda}\\) and for \\(\\lambda = 0\\) we get \\(y = \\ln(x)\\).\n\\(\\lambda &gt; 1\\) reduce negative skewness, whereas \\(\\lambda &lt; 1\\) reduce positive skewness.\nRemember: Positive skewness is very common for variables with a natural bound of zero.\nIf the power \\(\\lambda &lt; 0\\) then all values are multiplied by a negative number to preserve the natural order of observations because, for example, if \\(x_1 &lt; x_2\\) then \\(x_1^{-1} &gt; x_2^{-1} \\Longleftrightarrow \\frac{1}{x_1} &gt; \\frac{1}{x_2}\\). However, \\(-x_1^{-1} &lt; -x_2^{-1}\\)\nThis explains the value \\(\\lambda\\) in the denominator of the Box-Cox transformation\n\n\n\n\nFigure 4.1: The family of power transformations \\(X^p\\) of \\(X\\). The curve labeled \\(p\\) is the transformation \\(X^{(p)}\\), that is, \\((X^p - 1)/p\\); \\(X^{(0)}\\) is \\(\\log_e X\\)\n\n\n\nNote: R’s function car::powerTransform() is performing several statistical tests whether a variable either needs to be transformed at all (i.e., neutral \\(\\lambda = 1\\)) or whether a log-transformation (i.e., neutral \\(\\lambda = 0\\)) is sufficient by using the likelihood ratio test (LR) principle:\n\nThe first LR tests the null hypotheses \\(H_0: \\lambda_{optimal} = 0\\). If we cannot reject the null hypothesis then we should tentatively work with a log-transformation to achieve normality/symmetry.\nThe second LR tests the null hypotheses \\(H_0: \\lambda_{optimal} = 1\\). If we cannot reject the null hypothesis then we should tentatively work with an untransformed variable because it is approximately symmetric.\nThe Wald confidence interval provides the 95% probability range within which the true population transformation parameter \\(\\lambda\\) lies.\n\n\n\n\n\n\n\nAfter inspection of the variable’s distribution one can overcome the problem of zero or negative data values by\n\nadding a constant such as \\(\\min(X) + \\varepsilon\\), where \\(\\varepsilon\\) is a small positive number to the variable to make its range solidly positive.\nHowever, if \\(\\varepsilon\\) is too small, which leads to positive but close to zero values, outliers may be introduced.\nOn the other hand, choosing \\(\\varepsilon\\) too large, may make the transformation to normality ineffective.\nThe function car::symbox() calculates the offset or start value: “a start will be automatically generated if there are zero or negative values in x, and a warning will be printed; the auto-generated start is the absolute value of the minimum x plus 1 percent of the range of x.\n\nSee the ?car::bcPower() and Fox & Weissberg pp 161-162 for the bcnPower transformation family.\nA more informed way avoiding some of the problems by just adding a constant is to first transform the data by:\n\n\\[\nz(X, \\gamma) = \\frac{(X + \\sqrt{X^2 + \\gamma^2})}{2} \\quad \\text{with}\n\\]\n\nThe transformation \\(z(X, \\gamma)\\) is monotonic (i.e., if \\(x_1 &lt; x_2\\) then \\(z(x_1, \\gamma) &lt; z(x_2, \\gamma)\\))\nFor large positive \\(X\\) relative to \\(\\gamma\\) \\((X \\gg \\gamma)\\) the transformation is approximately linear with \\(z(X, \\gamma) \\approx X\\).\nIf \\(\\gamma = 0\\) then \\(z(X, \\gamma) = X\\) for \\(X &gt; 0\\) and \\(z(X, \\gamma) = 0\\) for \\(X \\leq 0\\).\nSubsequently, once the \\(\\gamma\\)-parameter is determined a standard Box-Cox transformation is applied to \\(z(X, \\gamma)\\).\n\n\n\n\n\n\nThe LOESS smoother highlights any non-linearities in the relationship between two variables.\nMany of R’s scatterplot functions not only show a linear regression fit through the data cloud but also show a locally smoothed loess-curve:\n\nIn essence, a sliding window moves over the value range of the variable X.\nIn each window a local regression line is estimated.\nThese local window regression lines are “splined” together into the smooth loess curve over the whole value range of X\n\n\n\n\n\nFigure 9.17: MA-plot with curve obtained with loess\n\n\n\n\n\nFigure 9.16: Illustration of how loess estimates a curve. Showing 12 steps of the process",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-central-limit-theorem-review-skipped",
    "href": "AG_w01_HamChap01.html#review-central-limit-theorem-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Def. Central Limit Theorem: Let \\(X_1, X_2, \\ldots, X_n\\) be a random independent sample of size \\(n\\) drawn from an arbitrarily distributed population with expectation \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThen for large enough sample sizes \\(n\\), the sampling distribution of the arithmetic mean \\(\\bar{X}\\) is [a] asymptotically (i.e., as the sample size \\(n \\to \\infty\\)) normal distributed [b] with\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\\]\nProof for independent sample observations \\(X_i\\):\n\\[\nVar\\left(\\frac{1}{n} \\cdot \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\cdot \\sum_{i=1}^{n} Var(X_i) = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}\n\\]\nImplications:\n\nThus the standard error \\(s_{\\bar{X}} = \\sigma/\\sqrt{n}\\) of the mean \\(\\bar{X}\\) shrinks by \\(1/\\sqrt{n}\\) with increasing sample size.\nAnother implication of the central limit theorem is that the sum of a set of small random errors or shocks will lead to normal distributed total error.\nIn contrast, the product of a set of small random errors will lead to a log-normal distributed total error.\n\nExample: Central limit theorem with the R-script CENTRALLIMIT.R:\n\n\n\n\nCentral Limit Theorem demonstration showing convergence to normality for three different population distributions (uniform, bimodal, exponential) with sample sizes n=4, 16, and 256",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-the-shape-of-distributions-review-skipped",
    "href": "AG_w01_HamChap01.html#review-the-shape-of-distributions-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Distributions can be distinguished with regards the balance of their left and right tails:\n\nSymmetric distributions. Tails are balanced into either direction from a central value.\nNegatively skewed distributions (long tail into the negative direction)\nPositively skewed distributions (long tail into the positive direction). These distributions frequently emerge for variables with a binding lower origin (like zero income).\nExtreme skewness may hint at outliers that do not match the rest of the observed data.\n\nThe number of meaningful clusters of observations is described by the term modality:\n\nUni-modality refers to just one peak.\nBi-modality refers to two outstanding peaks\nMultimodality refers to more than two outstanding peaks.\n\nMultimodality may hint at a heterogeneous underlying data generating process in which the underlying process for observations in the first mode is different for observation in the second mode.\n\n\n\n\nFigure 3.9: Shapes of frequency distributions: (a) Normal, (b) Bimodal, (c) Negatively skewed, (d) Positively skewed",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-quantiles-and-percentiles-review-skipped",
    "href": "AG_w01_HamChap01.html#review-quantiles-and-percentiles-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Technically, quantiles and percentiles are generated from a sorted list of the original data points \\(x_{[1]} \\leq x_{[2]} \\leq x_{[3]} \\leq \\cdots \\leq x_{[n-1]} \\leq x_{[n]}\\) where each observations has an assigned rank \\(i \\in \\{1,2,\\ldots,n\\}\\), with \\(i = 1\\) for the smallest observation and \\(i = n\\) for the largest observation.\nFor a give data value \\(\\mathbf{x}_{[i]}\\) the percentile approximates the proportion of sample observations less or equal to \\(\\mathbf{x}_{[i]}\\), that is, their cumulative distribution:\n\n\\[\n\\mathbf{p}_{[i]} = \\frac{i - \\frac{1}{2}}{n} \\approx \\mathbf{Pr}(X \\leq \\mathbf{x}_{[i]}) = \\int_0^{\\mathbf{x}_{[i]}} f(\\mathbf{x}) \\cdot d\\mathbf{x}.\n\\]\nNote that the \\(\\boldsymbol{\\alpha} = \\mathbf{0.5}\\) of the percentile equation \\(p(x_{[i]}) = \\frac{i - \\alpha}{n + (1 - \\alpha) - \\alpha}\\) has been chosen here.\n\nA quantile is the potentially fictious data value of a distribution, which is associated with a particular percentile value.\nImportant quantiles are:\n\n0.25 quantile also called \\(Q_1\\) quartile (25 % of the observations are smaller or equal to this quantile value)\n0.50 quantile also called the median (50 % of the observations are smaller or larger than the given quantile value)\n0.75 quantile also called \\(Q_3\\) quartile (75 % of the observations are smaller or equal to this quantile value and 25 % of the observations are larger than this value)\nA measure of spread is the inter-quartile range: \\(IQR = Q_3 - Q_1\\)",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-box-plots-review-skipped",
    "href": "AG_w01_HamChap01.html#review-box-plots-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Construction of the box-plot\n\nDraw a box from \\(Q_1\\) to \\(Q_3\\). Mark the median \\(\\mathbf{Q_2}\\) in the center of the box with a line.\nDefinition of adjacent values \\(x_{low}^{adj} = \\min(x_{[i]} \\in (Q_1, Q_1 - 1.5 \\cdot IQR)\\) plus \\(x_{[i]}\\) in dataset\\()\\) and \\(x_{high}^{adj} = \\max(x_{[i]} \\in (Q_3, Q_3 + 1.5 \\cdot IQR)\\) plus \\(x_{[i]}\\) in dataset\\()\\).\nThe term \\(x \\in (a,b)\\) means, all \\(x\\)-values in the interval between \\(a\\) and \\(b\\).\nDraw the “fences” so they just include the smallest and largest data values \\(x_{low}^{adj}\\) and \\(x_{high}^{adj}\\), respectively.\nOutliers are in the interval \\([1.5 \\cdot IQR, 3.0 \\cdot IQR]\\) starting from \\(Q_1\\) below or \\(Q_3\\) above, respectively.\nSevere outliers are beyond that range \\((&gt; 3.0 \\cdot IQR)\\)\n\n\n\n\n\nBox-plot example showing monthly distributions\n\n\n\nUse of box-plots:\n\nEasy visual description of the distribution of a variable and potential outliers\nComparison of distributions for several variables side-by-side.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#quantile-normal-plot-discussed",
    "href": "AG_w01_HamChap01.html#quantile-normal-plot-discussed",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Calculate the theoretical quantiles of a normally distributed random variable \\(Y_{[i]}\\) (assuming the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) were estimated from the sample data) based on the given sample percentiles \\(p_{[i]}\\) of the observed variable \\(x_{[i]}\\).\nQuantile-Normal Plot: Plot the theoretical normal distribution quantiles \\(Y_{[i]}\\) on the abscissa (X-axis) against their matching empirical distribution of \\(x_{[i]}\\) on the ordinate (Y-axis).\n\nInterpretation: - Diagonal with slope 1 =&gt; equal distributions. - Not a straight-line =&gt; different shapes.\n\n\n\nFigure 1.9: Quantile-normal plot of household water use (positively skewed)\n\n\n\n\n\nFigure 1.10: Quantile-normal plots reflect distribution shape - showing examples of Heavy Tails/High and Low Outliers, Light Tails/No Outliers, Positive Skew/High Outliers, Negative Skew/Low Outliers, Granularity, and Two Peaks/Central Gap",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-properties-of-arithmetic-mean-review-skipped",
    "href": "AG_w01_HamChap01.html#review-properties-of-arithmetic-mean-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Implications of the zero-sum property \\(\\sum_{i=1}^{n} (Y_i - \\bar{Y}) = 0\\): Assuming the mean is known, then \\(n - 1\\) observation can vary freely, whereas we can predict the last observation with certainty.\n\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n} (Y_i - \\bar{Y}) &= 0 \\\\\n\\Rightarrow \\quad \\sum_{i=1}^{n} Y_i &= n \\cdot \\bar{Y} \\\\\n\\Rightarrow \\quad Y_n &= n \\cdot \\bar{Y} - \\sum_{i=1}^{n-1} Y_i\n\\end{aligned}\n\\]\nThat implies that we loose one degree of freedom.\n\nImplication of the least squares property \\(\\min_{\\theta} \\sum_{i=1}^{n} (Y_i - \\theta)^2 \\Rightarrow \\theta = \\bar{Y}\\).\nLarge deviations have a strong impact on the estimated mean, variance etc. because the large deviations are squared\n\\(\\Rightarrow\\) Thus, large deviations pull the mean into their direction.\n\\(\\Rightarrow\\) Standard deviations are drastically inflated.\nLacking any other information, the arithmetic mean will become best predictor for the variable under investigation.\nThe deviations from the mean are the unexplained part or the residuals of the observations, i.e., \\(y_i = \\bar{y} + \\varepsilon_i\\).\nDefinition of total sum of squares: \\(TSS = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\\) or \\(TSS = \\sum_{i=1}^{n} Y_i^2 - n \\cdot \\bar{Y}^2\\).\nWhy is the population variance estimated with \\((n-1)\\) in the denominator, that is, by \\(s^2 = TSS/(n-1)\\):\nExplanation 1: If we calculate the mean from the sample then there are only \\(n - 1\\) “degrees of freedom” left because of the zero sum property of the mean.\nExplanation 2: The mean is calculated by minimizing the TSS.\nThus the sample mean always fits the observed sample data better than any unobserved but true population expectation \\(\\mu\\).\nFor the true expectation \\(\\mu\\), the TSS would be slightly larger. That is why the sample TSS needs to be inflated by dividing it by a slight smaller value than \\(n\\), that is, \\(n - 1\\).\nStandard deviation measures the variation in original units rather than in squared units.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#review-skewness-review-skipped",
    "href": "AG_w01_HamChap01.html#review-skewness-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "Why does the distribution of the water consumption in the Concord dataset deviate from the normal distribution?\nReason: Fixed lower bound (negative water consumption impossible).\nSkewness and bounded/truncated distributions: For skewed distributions the notion of the center of the distribution (mean) becomes ambiguous and the median may be a better representation of the central tendency in the data.\nThe skewness is defined by \\(skew(X) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^3}{n \\cdot s_X^3}\\)\nThe normal distribution has a skewness of 0.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#box-cox-transformation-discussed",
    "href": "AG_w01_HamChap01.html#box-cox-transformation-discussed",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "This lecture focuses on the more general Box-Cox transformation (note 11 on page 28 in Hamilton) rather than the slightly simpler power transformation, which is discussed in Hamilton. For both transformations, the general interpretation of the parameter \\(\\lambda\\) does not change.\nCauses for extreme observations: [a] skewed distributions, [b] measurement or recoding errors, [c] extreme but feasible events (perhaps not belonging to the population under investigation).\nThe power-transformation presented in book and the Box-Cox transformation only work for variables whose observations are all larger than zero.\nThe Box-Cox transformation is a generalization of the power transformation: \\(Y = \\frac{X^{\\lambda} - 1}{\\lambda}\\) and for \\(\\lambda = 0\\) we get \\(y = \\ln(x)\\).\n\\(\\lambda &gt; 1\\) reduce negative skewness, whereas \\(\\lambda &lt; 1\\) reduce positive skewness.\nRemember: Positive skewness is very common for variables with a natural bound of zero.\nIf the power \\(\\lambda &lt; 0\\) then all values are multiplied by a negative number to preserve the natural order of observations because, for example, if \\(x_1 &lt; x_2\\) then \\(x_1^{-1} &gt; x_2^{-1} \\Longleftrightarrow \\frac{1}{x_1} &gt; \\frac{1}{x_2}\\). However, \\(-x_1^{-1} &lt; -x_2^{-1}\\)\nThis explains the value \\(\\lambda\\) in the denominator of the Box-Cox transformation\n\n\n\n\nFigure 4.1: The family of power transformations \\(X^p\\) of \\(X\\). The curve labeled \\(p\\) is the transformation \\(X^{(p)}\\), that is, \\((X^p - 1)/p\\); \\(X^{(0)}\\) is \\(\\log_e X\\)\n\n\n\nNote: R’s function car::powerTransform() is performing several statistical tests whether a variable either needs to be transformed at all (i.e., neutral \\(\\lambda = 1\\)) or whether a log-transformation (i.e., neutral \\(\\lambda = 0\\)) is sufficient by using the likelihood ratio test (LR) principle:\n\nThe first LR tests the null hypotheses \\(H_0: \\lambda_{optimal} = 0\\). If we cannot reject the null hypothesis then we should tentatively work with a log-transformation to achieve normality/symmetry.\nThe second LR tests the null hypotheses \\(H_0: \\lambda_{optimal} = 1\\). If we cannot reject the null hypothesis then we should tentatively work with an untransformed variable because it is approximately symmetric.\nThe Wald confidence interval provides the 95% probability range within which the true population transformation parameter \\(\\lambda\\) lies.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#handling-transformations-with-negative-data-values-skipped",
    "href": "AG_w01_HamChap01.html#handling-transformations-with-negative-data-values-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "After inspection of the variable’s distribution one can overcome the problem of zero or negative data values by\n\nadding a constant such as \\(\\min(X) + \\varepsilon\\), where \\(\\varepsilon\\) is a small positive number to the variable to make its range solidly positive.\nHowever, if \\(\\varepsilon\\) is too small, which leads to positive but close to zero values, outliers may be introduced.\nOn the other hand, choosing \\(\\varepsilon\\) too large, may make the transformation to normality ineffective.\nThe function car::symbox() calculates the offset or start value: “a start will be automatically generated if there are zero or negative values in x, and a warning will be printed; the auto-generated start is the absolute value of the minimum x plus 1 percent of the range of x.\n\nSee the ?car::bcPower() and Fox & Weissberg pp 161-162 for the bcnPower transformation family.\nA more informed way avoiding some of the problems by just adding a constant is to first transform the data by:\n\n\\[\nz(X, \\gamma) = \\frac{(X + \\sqrt{X^2 + \\gamma^2})}{2} \\quad \\text{with}\n\\]\n\nThe transformation \\(z(X, \\gamma)\\) is monotonic (i.e., if \\(x_1 &lt; x_2\\) then \\(z(x_1, \\gamma) &lt; z(x_2, \\gamma)\\))\nFor large positive \\(X\\) relative to \\(\\gamma\\) \\((X \\gg \\gamma)\\) the transformation is approximately linear with \\(z(X, \\gamma) \\approx X\\).\nIf \\(\\gamma = 0\\) then \\(z(X, \\gamma) = X\\) for \\(X &gt; 0\\) and \\(z(X, \\gamma) = 0\\) for \\(X \\leq 0\\).\nSubsequently, once the \\(\\gamma\\)-parameter is determined a standard Box-Cox transformation is applied to \\(z(X, \\gamma)\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "AG_w01_HamChap01.html#loess-smoother-of-yx-relationships-review-skipped",
    "href": "AG_w01_HamChap01.html#loess-smoother-of-yx-relationships-review-skipped",
    "title": "Week01: Hamilton Chapter 1 - Univariate Variable Distributions",
    "section": "",
    "text": "The LOESS smoother highlights any non-linearities in the relationship between two variables.\nMany of R’s scatterplot functions not only show a linear regression fit through the data cloud but also show a locally smoothed loess-curve:\n\nIn essence, a sliding window moves over the value range of the variable X.\nIn each window a local regression line is estimated.\nThese local window regression lines are “splined” together into the smooth loess curve over the whole value range of X\n\n\n\n\n\nFigure 9.17: MA-plot with curve obtained with loess\n\n\n\n\n\nFigure 9.16: Illustration of how loess estimates a curve. Showing 12 steps of the process",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Hamilton Ch01: Univariate Variable Distributions"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "AG_w00_HamAppTheory.html",
    "href": "AG_w00_HamAppTheory.html",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "",
    "text": "Greek letters are frequently used to denote either specific population properties, to name specific statistical test or as mathematical operator.\n\n\n\n\n\n\n\n\nGreek Letter\nPhonetic\nUsage\n\n\n\n\n\\(\\alpha\\)\nalpha\nerror of first type\n\n\n\\(\\beta\\)\nbeta\nerror of second type / regression parameters\n\n\n\\(\\varepsilon\\)\nepsilon\nregression population error term\n\n\n\\(\\mu\\)\nmu\nexpected population mean\n\n\n\\(\\pi\\)\npi\npopulation probability in binomial distribution\n\n\n\\(\\rho\\)\nrho\npopulation correlation coefficient\n\n\n\\(\\sigma\\)\nsigma\npopulation standard deviation\n\n\n\\(\\chi\\)\nchi\n\\(\\chi^2\\)-test, \\(\\chi^2\\)-distribution with \\(df\\) degrees of freedom\n\n\n\\(\\theta\\)\ntheta\ngeneric parameter of a distribution\n\n\n\\(\\lambda\\)\nlambda\nparameter of the Poisson and exponential distributions\n\n\n\\(\\Pi\\)\ncapital pi\nmultiplication symbol\n\n\n\\(\\Sigma\\)\ncapital sigma\nsummation symbol",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#greek-letters-skipped",
    "href": "AG_w00_HamAppTheory.html#greek-letters-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "",
    "text": "Greek letters are frequently used to denote either specific population properties, to name specific statistical test or as mathematical operator.\n\n\n\n\n\n\n\n\nGreek Letter\nPhonetic\nUsage\n\n\n\n\n\\(\\alpha\\)\nalpha\nerror of first type\n\n\n\\(\\beta\\)\nbeta\nerror of second type / regression parameters\n\n\n\\(\\varepsilon\\)\nepsilon\nregression population error term\n\n\n\\(\\mu\\)\nmu\nexpected population mean\n\n\n\\(\\pi\\)\npi\npopulation probability in binomial distribution\n\n\n\\(\\rho\\)\nrho\npopulation correlation coefficient\n\n\n\\(\\sigma\\)\nsigma\npopulation standard deviation\n\n\n\\(\\chi\\)\nchi\n\\(\\chi^2\\)-test, \\(\\chi^2\\)-distribution with \\(df\\) degrees of freedom\n\n\n\\(\\theta\\)\ntheta\ngeneric parameter of a distribution\n\n\n\\(\\lambda\\)\nlambda\nparameter of the Poisson and exponential distributions\n\n\n\\(\\Pi\\)\ncapital pi\nmultiplication symbol\n\n\n\\(\\Sigma\\)\ncapital sigma\nsummation symbol",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#standard-symbols-and-definition-skipped",
    "href": "AG_w00_HamAppTheory.html#standard-symbols-and-definition-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "2 STANDARD SYMBOLS AND DEFINITION (SKIPPED)",
    "text": "2 STANDARD SYMBOLS AND DEFINITION (SKIPPED)\n\n\n\n\n\n\n\nOperation\nMeaning\n\n\n\n\n\\(\\frac{\\mathit{Numerator}}{\\mathit{Denominator}}\\)\nratio between the numerator and the denominator\n\n\n\\(\\times\\) or \\(\\cdot\\), and \\(\\div\\) or \\(/,\\ +,\\ -\\)\nmultiplication and division take precedence over addition and subtraction\n\n\n\\(X &lt; Y\\)\n\\(X\\) is less than \\(Y\\)\n\n\n\\(X \\leq Y\\)\n\\(X\\) is less or equal than \\(Y\\)\n\n\n\\(X \\pm Y\\)\n\\(X\\) plus minus \\(Y\\), i.e., the two values \\(X + Y\\) and \\(X - Y\\)\n\n\n\\(\\|X\\|\\)\n\\(X = \\begin{cases} X & \\text{for } X \\geq 0 \\\\ -X & \\text{for } X &lt; 0 \\end{cases}\\)\n\n\n\\(\\frac{1}{X} = X^{-1}\\)\nReciprocal of \\(X\\)\n\n\n\\(X^n\\)\n\\(X\\) to the power of \\(n\\)\n\n\n\\(\\sqrt{X} = X^{\\frac{1}{2}}\\)\nsquare root of \\(X\\)\n\n\n\\(i \\in \\{1, 2, \\ldots, n\\}\\)\n\\(i\\) is an element in the set \\(\\{1, 2, \\ldots, n\\}\\). It takes the values \\(1, 2, \\ldots\\) to \\(n\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#notation-for-random-variables-skipped",
    "href": "AG_w00_HamAppTheory.html#notation-for-random-variables-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "3 NOTATION FOR RANDOM VARIABLES (SKIPPED)",
    "text": "3 NOTATION FOR RANDOM VARIABLES (SKIPPED)\nA random variable is denoted by a capital letter \\(X\\) while a lower-case letter \\(x\\) is used to denote its observed value.\nA random variable can comprise of more than values \\(X_i\\) relates to a specific observation. The index \\(i\\) ranges from \\(1, 2, \\ldots, n\\). The number of observations in a variable is \\(n\\). Therefore, \\(X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{pmatrix}\\).\nFor example, if \\(X\\) has \\(n = 4\\) observation then it can take 4 random observations, i.e.,\n\\[x = \\begin{pmatrix} x_1 = 3 \\\\ x_2 = 5 \\\\ x_3 = 5 \\\\ x_4 = 4 \\end{pmatrix}\\]\n\n3.1 RANKED DATA (SKIPPED)\n\nStatisticians frequently work with an ascending sorted sequence of observations which is denoted by square brackets \\(X_{[ranked]} = \\begin{pmatrix} X_{[1]} \\\\ X_{[2]} \\\\ \\vdots \\\\ X_{[n]} \\end{pmatrix}\\). For example, \\(x_{[ranked]} = \\begin{pmatrix} x_{[1]} = 3 \\\\ x_{[2]} = 4 \\\\ x_{[3]} = 5 \\\\ x_{[4]} = 5 \\end{pmatrix}\\).\n\nShould two observations have the same rank, such as \\(x_i = 5\\) and \\(x_j = 5\\), then the ranks \\([r]\\) and \\([r + 1]\\) will be assigned arbitrarily. See the example below:\n\nOrdering vectors in R:\n\n&gt; x &lt;- c(4,5,2,3,4)\n&gt; Idx &lt;- order(x)\n&gt; xSort &lt;- x[Idx]\n&gt;\n&gt; cbind(x, Idx, xSort)\n     x Idx xSort\n[1,] 4   3     2\n[2,] 5   4     3\n[3,] 2   1     4\n[4,] 3   5     4\n[5,] 4   2     5",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#basic-summation-sum-rules-skipped",
    "href": "AG_w00_HamAppTheory.html#basic-summation-sum-rules-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "4 BASIC SUMMATION \\(\\sum\\)-RULES: (SKIPPED)",
    "text": "4 BASIC SUMMATION \\(\\sum\\)-RULES: (SKIPPED)\n\n\\[\\sum_{i=1}^{n} x_i = x_1 + x_2 + \\cdots + x_n\\]\nThe lower index \\(i = 1\\) express the starting value of the summation sequence and the upper index \\(n\\) the value where the summation index \\(i\\) stops.\nMore specifically \\[\\sum_{i=2}^{5} x_i = x_2 + x_3 + x_4 + x_5\\]\nFor a sum over a constant \\(c\\) we get \\[\\sum_{i=1}^{n} c = n \\cdot c\\]\nfor a mixture of a constant and a variable \\[\\sum_{i=1}^{n} c \\cdot x_i = c \\cdot \\sum_{i=1}^{n} x_i\\]\nfor an additive mixture of variables \\[\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i\\]\nInequalities (so do not confuse either side of the expression; they lead to different results):\n\n\\[\\sum_{i=1}^{n} x_i \\cdot y_i \\neq \\sum_{i=1}^{n} x_i \\cdot \\sum_{i=1}^{n} y_i\\]\n\\[\\left(\\sum_{i=1}^{n} x_i\\right)^2 \\neq \\sum_{i=1}^{n} x_i^2\\]\n\nSpecial rule for ranks: \\[\\sum_{i=1}^{n} i = \\frac{n}{2} \\cdot (n+1)\\]\nDoubly index variables \\(x_{ij}\\) in a cross-tabulation (or matrix) with \\(I\\) rows and \\(J\\) columns:\n\nLet:\n\\[\\begin{pmatrix}\nx_{11} & x_{12} & \\cdots & x_{1j} & \\cdots & x_{1J} \\\\\nx_{21} & x_{22} & \\cdots & x_{2j} & \\cdots & x_{2J} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nx_{i1} & x_{i2} & \\cdots & x_{ij} & \\cdots & x_{iJ} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nx_{I1} & x_{I2} & \\cdots & x_{Ij} & \\cdots & x_{IJ}\n\\end{pmatrix}\\]\nThen the \\(i\\)-th row sum is \\(x_{i+} = \\sum_{j=1}^{J} x_{ij}\\) and the \\(j\\)-th column sum is \\(x_{+j} = \\sum_{i=1}^{I} x_{ij}\\) and\nthe total sum becomes \\[x_{++} = \\sum_{i=1}^{I} x_{i+} = \\sum_{j=1}^{J} x_{+j} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} x_{ij} \\text{ or } \\sum_{j=1}^{J} \\sum_{i=1}^{I} x_{ij}\\]\n\nThe R functions:\n\nsum() calculated the sum over the elements of a vector\nrowSums() calculates along the rows of a matrix a vector of row sums.\ncolSums() calculates along the columns of a matrix a vector of column sums.\n\nExample: The variance estimator can either be calculated by \\[s^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i - \\overline{x})^2\\] or by \\[s^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} x_i^2 - \\frac{n}{n-1} \\cdot \\overline{x}^2\\]. To derive this equivalence of both expressions, remember the definition of the mean \\(\\overline{x} = 1/n \\cdot \\sum_{i=1}^{n} x_i\\):\n\n\\[\\begin{aligned}\ns^2 &= \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i - \\overline{x})^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i^2 - 2 x_i \\cdot \\overline{x} + \\overline{x}^2) \\\\\n&= \\frac{1}{n-1} \\cdot \\left(\\sum_{i=1}^{n} x_i^2 - \\sum_{i=1}^{n} 2 \\cdot x_i \\cdot \\overline{x} + \\sum_{i=1}^{n} \\overline{x}^2\\right) = \\frac{1}{n-1} \\cdot \\left(\\sum_{i=1}^{n} x_i^2 - 2 \\cdot \\overline{x} \\cdot \\underbrace{\\sum_{i=1}^{n} x_i}_{=n \\cdot \\overline{x}} + n \\cdot \\overline{x}^2\\right) \\\\\n&= \\frac{1}{n-1} \\cdot \\left(\\sum_{i=1}^{n} x_i^2 \\underbrace{- 2 \\cdot n \\cdot \\overline{x}^2 + n \\cdot \\overline{x}^2}_{=-n \\cdot \\overline{x}^2}\\right) = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} x_i^2 - \\frac{n}{n-1} \\cdot \\overline{x}^2\n\\end{aligned}\\]",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#finding-the-minimum-of-a-quadratic-function-discussed",
    "href": "AG_w00_HamAppTheory.html#finding-the-minimum-of-a-quadratic-function-discussed",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "5 FINDING THE MINIMUM OF A QUADRATIC FUNCTION (DISCUSSED)",
    "text": "5 FINDING THE MINIMUM OF A QUADRATIC FUNCTION (DISCUSSED)\n\nIn statistic, we encounter frequently the need to find an optimal value of a function. If we want to minimize square deviations around an unknown value, the optimal value would be the minimum.\nThe minimum is found at that point where the slope of the function is zero. The slope of a function is measured by the first derivative.\nBasic rules of derivatives:\n\n\\[\\frac{\\partial}{\\partial x} f(a) = 0 \\quad \\text{The function } f(a) \\text{ is constant with regards to } x\\]\n\\[\\frac{\\partial}{\\partial x} a \\cdot x^n = a \\cdot n \\cdot x^{n-1}\\]\nExample: \\(\\frac{\\partial}{\\partial x} 3 \\cdot x^2 = 6 \\cdot x\\)\n\\[\\frac{\\partial}{\\partial x} [f(x) + g(x)] = \\frac{\\partial}{\\partial x} f(x) + \\frac{\\partial}{\\partial x} g(x)\\]\nExample: \\(\\frac{\\partial}{\\partial x} (3 \\cdot x^2 + 5 \\cdot x^{-1}) = 6 \\cdot x - 1 \\cdot 5 \\cdot x^{-2}\\)\n\nWhich value of \\(\\theta\\) (theta) minimizes the quadratic expression \\(\\min_{\\theta} \\sum_{i=1}^{n} (x_i - \\theta)^2\\)?\n\n\\[f(\\theta) = \\sum_{i=1}^{n} (x_i - \\theta)^2 = \\sum_{i=1}^{n} x_i^2 - 2 \\cdot \\theta \\cdot \\sum_{i=1}^{n} x_i + n \\cdot \\theta^2\\]\nTake the first derivative with regard to \\(\\theta\\), which is the slope of \\(f(\\theta)\\) at \\(\\theta\\):\n\\[\\frac{\\partial}{\\partial \\theta} \\left( \\underbrace{\\sum_{i=1}^{n} x_i^2}_{\\text{dropped, does not depend on } \\theta} - 2 \\cdot \\theta \\cdot \\sum_{i=1}^{n} x_i + n \\cdot \\theta^2 \\right) = -2 \\cdot \\sum_{i=1}^{n} x_i + 2 \\cdot n \\cdot \\theta\\]\nAt its maximum or minimum the first derivative (that is, the slope) is zero for a given \\(\\theta\\).\nWe thus get:\n\\[-2 \\cdot \\sum_{i=1}^{n} x_i + 2 \\cdot n \\cdot \\theta = 0 \\Leftrightarrow \\theta = \\frac{\\sum_{i=1}^{n} x_i}{n}\\]\n\\(\\Rightarrow\\) This is the well-know arithmetic mean!!!\n\nExample: The data values are \\(x_i \\in \\{2, 5, 4, 6, 8\\}\\). Thus the function to be minimized with respect to \\(\\theta\\) is\n\n\\[f(\\theta) = (2 - \\theta)^2 + (5 - \\theta)^2 + (4 - \\theta)^2 + (6 - \\theta)^2 + (8 - \\theta)^2\\]\nThe solution is found at \\(\\theta = 5 \\Leftrightarrow \\overline{x}\\).\n\n\n\nPage 9",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#the-exponential-and-logarithmic-functions-skipped",
    "href": "AG_w00_HamAppTheory.html#the-exponential-and-logarithmic-functions-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "6 THE EXPONENTIAL AND LOGARITHMIC FUNCTIONS (SKIPPED)",
    "text": "6 THE EXPONENTIAL AND LOGARITHMIC FUNCTIONS (SKIPPED)\n\nBoth functions are inversely related: \\(x = \\exp(\\log(x))\\) and \\(x = \\log(\\exp(x))\\).\n\nNotes:\n\nThe \\(\\log\\)-function is usually the natural logarithm to the basis of the Euler constant \\(e = 2.718\\)\nThe support of the logarithmic function is limited from below by zero, that is, \\(x \\in ]0, \\infty]\\) with \\(\\log(0) = -\\infty\\).\nBoth functions distort constant distance units of the variable \\(x\\).\n\nE.g., \\(\\Delta x_1 = 4 - 2 = 2\\) and \\(\\Delta x_2 = 8 - 6 = 2\\)\nbut \\(\\log(4) - \\log(2) = 1.086\\) and \\(\\log(8) - \\log(6) = 0.288\\), respectively.\nThis means, logarithmic distances at the upper end of the scale become compressed.\n\nBasic rules:\n\nLogarithmic function: \\(\\log(x \\cdot y) = \\log(x) + \\log(y)\\), \\(\\log(x/y) = \\log(x) - \\log(y)\\) and \\(\\log(x^y) = y \\cdot \\log(x)\\)\nExponential function: \\(\\exp(x + y) = \\exp(x) \\cdot \\exp(y)\\), \\(\\exp(x - y) = \\exp(x) / \\exp(y)\\) and \\([\\exp(x)]^y = \\exp(x \\cdot y)\\)\n\n\n\n\n\nPage 10",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#appendix-1-population-and-sampling-distributions-ham-pp-289-293-skipped",
    "href": "AG_w00_HamAppTheory.html#appendix-1-population-and-sampling-distributions-ham-pp-289-293-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "7 APPENDIX 1: POPULATION AND SAMPLING DISTRIBUTIONS (HAM PP 289-293) (SKIPPED)",
    "text": "7 APPENDIX 1: POPULATION AND SAMPLING DISTRIBUTIONS (HAM PP 289-293) (SKIPPED)\n\nIn theoretical statistics we make statements about the population based on the distribution \\(f(y)\\) of a continuous random variable \\(Y\\) or \\(\\Pr(Y = y)\\) for a discrete random variable \\(Y\\), which takes the specific value \\(y\\), respectively.\nIn applied statistics we are dealing with sampled data from the population and aim at estimating properties of the underlying population from which the random sample has been drawn\nThe sample is a narrow keyhole allowing us to look at parts of the unknown population.\nConventions:\n\nParameters characterizing the population are usually denoted by Greek characters, e.g., the expectation \\(\\mu_X\\) of the random variable \\(X\\). Their estimates are either expressed by Latin characters, e.g., the mean \\(\\overline{X}\\), or by a hat symbol that denotes an estimate, e.g., \\(\\hat{\\mu}_X\\).\nA random variable from the population is usually denoted by a capital letter, e.g., \\(X\\), whereas its observed realization in the sample is denoted by small letters, e.g., \\(x_1, x_2, \\ldots, x_n\\)\n\nPopulation expectation (i.e., central tendency, center of gravity)\n\nThe mean in the population distribution is called expectation and denoted by \\(\\mu_X = E[X]\\)\nFor discrete variables the expectation function is defined by\n\n\n\\[E[Y] = \\sum_{i=1}^{I} y_i \\cdot \\Pr(Y = y_i)\\]\nwhere \\(I\\) is the total number of representations, which can be an infinite number as for the Poisson distribution \\(y_i \\in \\{0, 1, 2, \\ldots, \\infty\\}\\) or a finite set as in the sum of two throws of a dices \\(y_i \\in \\{2, 3, \\ldots, 12\\}\\)\n\nFor continuous random variables the expectation function is defined in terms of the density function \\(f(x)\\)\n\n\\[E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\\]\nFor infeasible values of \\(x\\) the density will become \\(f(x) = 0\\) because these values are improbable.\n\nRemember: the density function \\(f(x)\\) at \\(x\\) cannot be interpreted as probability. We only can express the probability for a range of value:\n\n\\[X \\in [a, b] \\Rightarrow \\Pr(a \\leq X \\leq b) = \\int_a^b f(x) \\cdot dx\\]\n\nSome rules for the expectation:\n\n\\[E[a] = a \\quad \\text{for a deterministic (constant) value } a\\]\n\\[E[a \\cdot X] = a \\cdot E[X]\\]\n\\[E[X \\pm Y] = E[X] \\pm E[Y]\\]\n\\[E[a + b \\cdot X] = a + b \\cdot E[X]\\]\n\\[E[a \\cdot X + b \\cdot Y] = E[a \\cdot X] + E[b \\cdot Y] = a \\cdot E[X] + b \\cdot E[Y]\\]\n\nAn unbiased sample estimator of the expectation \\(E[Y]\\) is the mean \\[\\overline{Y}_n = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} y_i\\]\nVariance\n\nThe variance is a measure of squared spread around the center, i.e., expectation, of a random variable\n\n\n\\[\\begin{aligned}\n\\text{Var}[X] &= \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\cdot f(x) \\, dx \\\\\n&= E[(X - E[X])^2] \\\\\n&= E[X^2] - (E[X])^2\n\\end{aligned}\\]\n\nThe unbiased sample variance estimator \\(s_X^2\\) for the population variance \\(\\sigma^2\\) is:\n\n\\[s_X^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i - \\overline{X}_n)^2\\]\n\nBasic properties:\n\n\\[\\text{Var}[a] = 0 \\quad \\text{because } a \\text{ is a constant (i.e., not random)}\\]\n\\[\\text{Var}[b \\cdot X] = b^2 \\cdot \\text{Var}[X]\\]\n\\[\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + 2 \\cdot \\text{Cov}[X, Y]\\]\n\\[\\text{Var}[X - Y] = \\text{Var}[X] + \\text{Var}[Y] - 2 \\cdot \\text{Cov}[X, Y]\\]\n\\[\\text{Var}[a + b \\cdot X] = \\text{Var}[a] + \\text{Var}[b \\cdot X] = b^2 \\cdot \\text{Var}[X]\\]\n\\[\\text{Var}[a \\cdot X + b \\cdot Y] = a^2 \\cdot \\text{Var}[X] + b^2 \\cdot \\text{Var}[Y] + 2 \\cdot a \\cdot b \\cdot \\text{Cov}[X, Y]\\]",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#example-explanation-of-integration-using-the-exponential-distribution-discussed",
    "href": "AG_w00_HamAppTheory.html#example-explanation-of-integration-using-the-exponential-distribution-discussed",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "8 EXAMPLE: EXPLANATION OF INTEGRATION USING THE EXPONENTIAL DISTRIBUTION (DISCUSSED)",
    "text": "8 EXAMPLE: EXPLANATION OF INTEGRATION USING THE EXPONENTIAL DISTRIBUTION (DISCUSSED)\n\nIntegration is key mathematical operation in statistics which is used to find the expectation, variance and higher order moments of continues random variables.\nBackground information on the exponential distribution\n\nExample: the waiting times \\(x\\) between two independent random events (earthquakes, customers lining up in-front of a cashier etc.) may be exponential distributed.\nThe exponential distribution only has the one parameter \\(\\lambda\\), with \\(E[X] = 1/\\lambda\\) being the average waiting time.\nYou can look at some exponential distributions using dexp() function.\nThe exponential distribution is related to the Poisson distribution:\n\nIt provides a stochastic model for the number of independent random events \\(y\\) within a fixed time-interval.\nThe expected number of random events within a fixed time-interval is \\(E[Y] = \\lambda\\).\nIf the expected number of events is large, then the average waiting time between the events will be small.\n\n\n\nThus, we have an inverse relationship between both expectations for the exponential and the Poisson distribution.\n\nThe density function of the exponential distribution is\n\n\\[f(x|\\lambda) = \\begin{cases} \\lambda \\cdot \\exp(-\\lambda \\cdot x) & \\text{for } x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nIts cumulative distribution function is\n\n\\[F(x|\\lambda) = \\int_0^x \\lambda \\cdot \\exp(-\\lambda \\cdot x) \\cdot dx = \\begin{cases} 1 - \\exp(-\\lambda \\cdot x) & \\text{for } x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nProof:\n\nLet \\(v = -\\lambda \\cdot x\\). Then \\(\\frac{dv}{dx} = -\\lambda \\Rightarrow dx = -\\frac{1}{\\lambda} \\cdot dv\\)\nThen\n\n\n\\[F(x|\\lambda) = \\int_0^x \\lambda \\cdot \\exp(-\\lambda \\cdot x) \\cdot dx = \\lambda \\cdot \\int_0^x \\exp(-\\lambda \\cdot x) \\cdot dx\\]\nand after a change in variable, i.e., integration by substitution, it becomes\n\\[F(x|\\lambda) = \\lambda \\cdot \\int_0^x -\\frac{1}{\\lambda} \\cdot \\exp(v) \\cdot dv = -\\int_0^x \\exp(v) \\cdot dv = -\\exp(v)|_0^x = 1 - \\exp(-\\lambda \\cdot x)\\]\n\nIts moments are known analytical:\n\nThe expectation is \\[E[X] = \\int_0^{\\infty} x \\cdot \\lambda \\cdot \\exp(-\\lambda \\cdot x) \\cdot dx = \\frac{1}{\\lambda}\\] and\nThe variance is \\[\\text{Var}[X] = \\int_0^{\\infty} \\left(x - \\underbrace{1/\\lambda}_{=E[X]}\\right)^2 \\cdot \\lambda \\cdot \\exp(-\\lambda \\cdot x) \\cdot dx = \\frac{1}{\\lambda^2}\\]\n\nThe parameter \\(\\lambda\\) can be estimated from sample observations by \\(\\hat{\\lambda} = 1/\\overline{x}\\).\nEvaluation of the moments by numerical integration (see script `RIEMANNSUM.R):\n\nThe Riemann sum approximates a continuous integral by \\[\\int_a^b f(x) \\, dx \\approx \\sum_{i=1}^{n} f(t_i) \\cdot dx_i\\] by discrete evaluations with \\(a = x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_{n-1} &lt; x_n = b\\), with the bin width \\(dx_i = x_i - x_{i-1}\\) and \\(t_i \\in [x_{i-1}, x_i]\\), which usually is set to the halfway point \\(t_i = \\frac{x_{i-1} + x_i}{2}\\)\nThe parameters \\(dx_i\\) and \\(n\\) determine the resolution and therefore the accuracy of the Riemann sum integral approximation.\nAdvance integration algorithms make the differences \\(dx_i = x_i - x_{i-1}\\) adaptive relative to the variability of \\(f(x)\\):\n\nIf the underlying function \\(f(x)\\) varies heavily, then the differences \\(dx_i\\) should be small (much finer).\nOn the other hand, if the underlying function is fairly smooth the differences \\(dx_i\\) could larger.\n\n\n\nThe underlying idea is similar to an adaptive kernel density estimator.\n\nEvaluation of the exponential density, expectation, and variance for \\(\\lambda = 1\\) in the range \\(x \\in [0, 10]\\):\n\n\n\n\nPage 17\n\n\n\nNotes:\n\nThe integral \\(\\int_{-\\infty}^{\\infty} f(x) \\cdot dx = 1\\) over any density functions \\(f(x)\\) always is one.\nTheoretically all integrals in the example should be equal to one, because \\(\\int_{-\\infty}^{\\infty} f(x) \\cdot dx = 1\\), \\(E(X) = 1/\\lambda\\) and \\(Var(X) = 1/\\lambda^2\\) for an exponential distribution with \\(\\lambda = 1\\).\nEven if we increase the number of bins, these integrals will not approach 1 because we are truncating the infinitive integration range by the upper value \\(b = 10 &lt; \\infty\\).\n\n\n\n\n\nPage 18\n\n\n\n\n8.1 COVARIANCE (SKIPPED)\n\nThe covariance is a basic measure of the linear relationship between pairs of random variables.\nThe covariance is the numerator of the correlation coefficient. That is \\(\\rho = \\frac{\\text{Cov}[X,Y]}{\\sqrt{\\text{Var}[X] \\cdot \\text{Var}[Y]}}\\)\nThe covariance of a variable with itself is called the variance:\n\n\\[\\text{Cov}[X, X] = \\text{Var}[X]\\]\n\n\\[\\begin{aligned}\n\\text{Cov}[X, Y] &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x - E[X])(y - E[Y]) \\cdot f(x, y) \\, dx \\, dy \\\\\n&= E[(X - E[X])(Y - E[Y])] \\\\\n&= E[XY] - E[X] \\cdot E[Y]\n\\end{aligned}\\]\nAn unbiased estimator for the population covariance is\n\n\\[s_{XY} = \\frac{\\sum_{i=1}^{n} \\left[ (x_i - \\overline{X}) \\cdot (y_i - \\overline{Y}) \\right]}{n-1}\\]\n\nSome rules:\n\n\\[\\text{Cov}[a, Y] = 0\\]\n\\[\\text{Cov}[b \\cdot X, Y] = b \\cdot \\text{Cov}[X, Y]\\]\n\\[\\text{Cov}[X + W, Y] = \\text{Cov}[X, Y] + \\text{Cov}[W, Y]\\]\n\nThe covariance is unaffected by the addition of a constant to either random variable:\n\n\\[\n\\mathrm{Cov}[a + X, Y]\n= \\underbrace{\\mathrm{Cov}[a, Y]}_{=0}\n+ \\mathrm{Cov}[X, Y]\n= \\mathrm{Cov}[X, Y]\n\\]\n\nThe covariance between sums of variables reduces to sums of covariances between their components\n\n\\[\\begin{aligned}\n\\text{Cov}[X + W, Y + Z] &= \\text{Cov}[X + W, Y] + \\text{Cov}[X + W, Z] \\\\\n&= \\text{Cov}[X, Y] + \\text{Cov}[W, Y] + \\text{Cov}[X, Z] + \\text{Cov}[W, Z]\n\\end{aligned}\\]\n\\[\\text{Cov}[X, Y - X] = \\text{Cov}[X, Y] - \\text{Cov}[X, X] = \\text{Cov}[X, Y] - \\text{Var}[X]\\]\n\nThe Ordinary Least Squares slope estimator in terms of covariances becomes\n\nThe slope regression coefficient for a regression of \\(Y\\) onto \\(X\\) becomes\n\n\n\\[\\beta_{1,Y|X} = \\frac{\\text{Cov}[X, Y]}{\\text{Var}[X]}\\]\n\nVice versa, for a regression of \\(X\\) onto \\(Y\\) one gets\n\n\\[\\beta_{1,X|Y} = \\frac{\\text{Cov}[X, Y]}{\\text{Var}[Y]}\\]\n\nThe regression intercept for a regression of \\(Y\\) onto \\(X\\) becomes\n\n\\[\\beta_{0,Y|X} = E[Y] - \\beta_{1,Y|X} \\cdot E[X]\\]\nbecause the expectations \\(E[Y]\\) and \\(E[X]\\) lie on the regression line.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#normal-distribution-and-its-relatives-discussed",
    "href": "AG_w00_HamAppTheory.html#normal-distribution-and-its-relatives-discussed",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "9 NORMAL DISTRIBUTION AND ITS RELATIVES (DISCUSSED)",
    "text": "9 NORMAL DISTRIBUTION AND ITS RELATIVES (DISCUSSED)\n\nDefinition: Let \\(z\\) and the sets \\(z_1, z_2, \\ldots, z_n\\) with \\(n\\) elements and \\(\\tilde{z}_1, \\tilde{z}_2, \\ldots, \\tilde{z}_m\\) with \\(m\\) elements be standard normal distributed random variables \\(x \\sim \\mathcal{N}(0,1)\\), which are all mutually independent.\nThe \\(\\chi^2\\)-distribution: The random variables\n\n\\[s_n^2 = \\sum_{i=1}^{n} z_i^2 \\quad \\text{and} \\quad \\tilde{s}_m^2 = \\sum_{i=1}^{m} \\tilde{z}_i^2\\]\nof the sums of squared independent standard normal distributed variables are \\(\\chi^2\\)-distributed\n\\[s_n^2 \\sim \\chi_{df=n}^2 \\quad \\text{and} \\quad \\tilde{s}_m^2 \\sim \\chi_{df=m}^2\\]\nwith \\(n\\) and \\(m\\) degrees of freedom, respectively.\nThe expected value of a \\(\\chi^2\\)-distributed variable is equal to its degrees of freedom.\n\nThe \\(t\\)-distribution: Let \\(t_n = \\frac{z}{\\sqrt{s_n^2/n}}\\) and \\(\\tilde{t}_m = \\frac{z}{\\sqrt{\\tilde{s}_m^2/m}}\\) with \\(z\\) being independent standard normal distributed. Then \\(t_n\\) and \\(\\tilde{t}_m\\) are \\(t\\)-distributed with \\(n\\) and \\(m\\) degrees of freedom, respectively.\nThe \\(F\\)-distribution: Let \\(F_n^m = \\frac{s_n^2/n}{\\tilde{s}_m^2/m}\\). Then \\(F_n^m\\) is \\(F\\)-distributed with \\(n\\) and \\(m\\) degrees of freedom.\nSee R-script SimulateFtCh.r.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#bivariate-normal-distribution-skipped",
    "href": "AG_w00_HamAppTheory.html#bivariate-normal-distribution-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "10 BIVARIATE NORMAL DISTRIBUTION (SKIPPED)",
    "text": "10 BIVARIATE NORMAL DISTRIBUTION (SKIPPED)\n\nThe bivariate normal distribution between two continuous random variables \\(X_1 \\in [-\\infty, \\infty]\\) and \\(X_2 \\in [-\\infty, \\infty]\\) is characterized by five parameters:\n\\(\\mu_{X_1}\\) and \\(\\mu_{X_2}\\) for the central tendency on each axis \\(X_1\\) and \\(X_2\\).\n\\(\\sigma_{X_1}\\) and \\(\\sigma_{X_2}\\) for the spread along each axis \\(X_1\\) and \\(X_2\\).\n\\(\\sigma_{X_1,X_2}\\) for the covariance between both \\(X_1\\) and \\(X_2\\).\n3-D examples of a bivariate normal distribution are:\nThe conditional density is given by \\(f(x_1|x_2) = \\frac{f(x_1, x_2)}{f(x_2)}\\) is shown by the \\(x_1\\) and \\(x_2\\) gridlines.\n\n\n\n\nPage 22\n\n\n\nThe conditional distribution is again normal distributed with the expectation\n\n\\[\\mu_{X_1|X_2=x_2} = \\mu_{X_1} + \\frac{\\sigma_{X_1,X_2}}{\\sigma_{X_2}^2} \\cdot (x_2 - \\mu_{X_2})\\]\nand the variance\n\\[\\sigma_{X_1|X_2=x_2}^2 = (1 - \\rho_{X_1,X_2}^2) \\cdot \\sigma_{X_1}^2\\]\n\nContour plot example with equal density isolines and marginal densities (see BivariateNormalDistrib.r):\nThe marginal distributions are again normal distributed with the parameters \\(\\mu_{X_1}\\) and \\(\\sigma_{X_1}^2\\):\n\ne.g. \\(\\mathcal{N}(\\mu_{X_1}, \\sigma_{X_1}^2) = f(X_1 = x_1) = \\int_{-\\infty}^{\\infty} f(x_1, x_2) \\, dx_2\\).\nThese marginal distributions are schematically shown by the blue curves at the margins\n\nThe total probability under the density is\n\n\\[\\int_{-\\infty}^{\\infty} \\underbrace{\\left( \\int_{-\\infty}^{\\infty} f(x_1, x_2) \\cdot dx_2 \\right)}_{=f(x_1)} \\cdot dx_1 = 1\\]\n\n\n\nPage 23",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w00_HamAppTheory.html#bias-and-mean-square-error-skipped",
    "href": "AG_w00_HamAppTheory.html#bias-and-mean-square-error-skipped",
    "title": "Week00: Basic Math Review - Hamilton’s Applied Theory",
    "section": "11 BIAS AND MEAN SQUARE ERROR (SKIPPED)",
    "text": "11 BIAS AND MEAN SQUARE ERROR (SKIPPED)\n\nThe theoretical sampling distribution of a statistic \\(\\hat{\\theta}\\) is evaluated over all possible random samples of a given size \\(n\\).\nA statistic is unbiased if \\(E[\\hat{\\theta}] = \\theta\\), that is, \\(E[\\hat{\\theta}] - \\theta = 0\\). It is biased if \\(E[\\hat{\\theta}] \\neq \\theta\\), that is, the estimator’s \\(\\hat{\\theta}\\) expected value differs from the true population parameter \\(\\theta\\).\nThe variance \\(\\text{Var}[\\hat{\\theta}] = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]\\) expresses the precision of a sample statistics. The square root of this variance is called standard error of a sample statistics.\nThe* mean square error is defined by\n\n\\[\\begin{aligned}\nMSE &= E[(\\hat{\\theta} - \\theta)^2] \\\\\n&= E[(\\hat{\\theta} - E[\\hat{\\theta}] + E[\\hat{\\theta}] - \\theta)^2] \\\\\n&= E[(\\hat{\\theta} - E[\\hat{\\theta}])^2 + 2 \\cdot (\\hat{\\theta} - E[\\hat{\\theta}]) \\cdot (E[\\hat{\\theta}] - \\theta) + (E[\\hat{\\theta}] - \\theta)^2] \\\\\n&= \\text{Var}[\\hat{\\theta}] + bias^2\n\\end{aligned}\\]\nwith the term \\(E[2 \\cdot (\\hat{\\theta} - E[\\hat{\\theta}]) \\cdot (E[\\hat{\\theta}] - \\theta)] = 0\\) is equal to zero because\n\\[\\begin{aligned}\nE[(\\hat{\\theta} - E[\\hat{\\theta}]) \\cdot (E[\\hat{\\theta}] - \\theta)] &= E[\\hat{\\theta} \\cdot E[\\hat{\\theta}] - \\hat{\\theta} \\cdot \\theta - E[\\hat{\\theta}] \\cdot E[\\hat{\\theta}] + E[\\hat{\\theta}] \\cdot \\theta] \\\\\n&= E[\\hat{\\theta}] \\cdot E[\\hat{\\theta}] - E[\\hat{\\theta}] \\cdot \\theta - E[\\hat{\\theta}] \\cdot E[\\hat{\\theta}] + E[\\hat{\\theta}] \\cdot \\theta \\\\\n&= 0\n\\end{aligned}\\]\nOnly \\(\\hat{\\theta}\\) is a random variable, whereas \\(E[\\hat{\\theta}]\\) and \\(\\theta\\) are constants.\nTherefore, \\(E[E[\\hat{\\theta}]] = E[\\hat{\\theta}]\\) and \\(E[\\theta] = \\theta\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk00-Basic Math Review"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html",
    "href": "AG_w01_TheoryReverseBoxCox.html",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "",
    "text": "The objective of this note is to discuss properties of predictions \\(\\hat{y}_i\\) in its original measurement units when the intermediate predicted values \\(\\hat{y}_i^{(\\lambda)} = E(y_i^{(\\lambda)} | \\mathbf{x}_i)\\) are based on a regression model in which \\(y_i\\) was transformed by a Box-Cox transformation \\(y_i^{(\\lambda)} \\leftarrow g(y_i;\\lambda)\\). The underlying problem is that the distributional properties of \\(y_i^{(\\lambda)}\\) in transformed system are different from those in the untransformed system. Therefore, a direct remapping leads to predictions of conditional medians \\(median(y_i | \\mathbf{x}_i) \\leftarrow g^{-1}(\\hat{y}_i^{(\\lambda)})\\) in the original system rather than to a prediction of the conditional expectations \\(E(y_i | \\mathbf{x}_i)\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#objective",
    "href": "AG_w01_TheoryReverseBoxCox.html#objective",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "",
    "text": "The objective of this note is to discuss properties of predictions \\(\\hat{y}_i\\) in its original measurement units when the intermediate predicted values \\(\\hat{y}_i^{(\\lambda)} = E(y_i^{(\\lambda)} | \\mathbf{x}_i)\\) are based on a regression model in which \\(y_i\\) was transformed by a Box-Cox transformation \\(y_i^{(\\lambda)} \\leftarrow g(y_i;\\lambda)\\). The underlying problem is that the distributional properties of \\(y_i^{(\\lambda)}\\) in transformed system are different from those in the untransformed system. Therefore, a direct remapping leads to predictions of conditional medians \\(median(y_i | \\mathbf{x}_i) \\leftarrow g^{-1}(\\hat{y}_i^{(\\lambda)})\\) in the original system rather than to a prediction of the conditional expectations \\(E(y_i | \\mathbf{x}_i)\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#motivating-example",
    "href": "AG_w01_TheoryReverseBoxCox.html#motivating-example",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "2 Motivating Example",
    "text": "2 Motivating Example\nThe R-script BoxCoxBivariateRegression.r provides an application of the boxcox() function2 from the MASS library which uses a grid search algorithm of the profile log-likelihood function (see Aitkin et al., 2009) to obtain an estimate for the best \\(\\hat{\\lambda}\\) (see Figure 1) such that the regression residuals \\(e_i^{(\\lambda)} = y_i^{(\\lambda)} - \\hat{y}_i^{(\\lambda)}\\) in the transformed system are approximately normal distributed. Therefore, the associated function call to evaluated the distribution of the residuals must explicitly account for the underlying regression model: findMaxLambda(lm(y~bcPower(x,lambdaX), data=myData)). Note that the independent variable has also been transformed so that its variation around its mean is approximately symmetric. The function call to identify its best transformation parameter \\(\\hat{\\lambda}\\) is findMaxLambda(lm(x~1,data=myData)).\nFigure 2 displays the non-linear relationship between both variables in the original measurement system before the application of the Box-Cox transformation. Clearly, in the original system both variables are positively skewed and the residual variances increase as the independent variable increases. In the transformed system (see Figure 3) the relationship between both variables becomes almost linear as can be seen by the straight lowess smoother line, both variables are symmetrically distributed and the residual variation is now homoscedastic. Finally, in Figure 4 the prediction \\(\\hat{y}_i^{(\\lambda)}\\) was mapped back into the original units. The conditional median (green line) is lower than the conditional expectation (red line) because for any value of the independent variable, the conditional distribution of the dependent variable is positively skewed. For the bulk of the data points in the lower left-hand quadrant, lowess smoother in Figure 2 traces the conditional median in Figure 4.\n\n\n\nFigure 1: Concentrated log-likelihood function of the Box-Cox transformation for the dependent variable income to achieve symmetrically distributed regression residuals.\n\n\n\n\n\nFigure 2: Linear regression line and lowess-smoother in original units with positively skewed dependent and independent variables.\n\n\n\n\n\nFigure 3: Relationship between both variables in the transformed system. The green line denotes the predicted values \\(\\hat{y}^{(\\lambda)}\\).\n\n\n\n\n\nFigure 4: Conditional median and expectation prediction lines after the reverse Box-Cox transformation back into the original units.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#the-box-cox-transformation",
    "href": "AG_w01_TheoryReverseBoxCox.html#the-box-cox-transformation",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "3 The Box-Cox Transformation",
    "text": "3 The Box-Cox Transformation\nIt is required that all observations of \\(y_i\\) are strictly positive, that is, \\(y_i &gt; 0\\) \\(\\forall i\\). The Box-Cox transformation is defined as\n\\[\ny_i^{(\\lambda)} = g(y_i;\\lambda) =\n\\begin{cases}\n\\frac{(y_i^{\\lambda} - 1)}{\\lambda} & \\lambda \\neq 0 \\\\\n\\ln y_i & \\lambda = 0\n\\end{cases}\n.\n\\]\nNote that in the limit \\(\\lambda \\to 0\\) the Box-Cox transformation becomes \\(\\lim_{\\lambda \\to 0} \\frac{y^{\\lambda} - 1}{\\lambda} = \\ln(y)\\) because, following the limit calculus rule of l’Hôpital3, \\(\\lim_{\\lambda \\to 0} \\frac{y^{\\lambda} - 1}{\\lambda} = \\lim_{\\lambda \\to 0} \\frac{\\partial(y^{\\lambda} - 1)/\\partial \\lambda}{\\partial \\lambda / \\partial \\lambda} = \\lim_{\\lambda \\to 0} \\frac{\\ln(y) \\cdot y^{\\lambda}}{1} = \\ln(y)\\).\nThe inverse transformation reversing the transformed values \\(y_i^{(\\lambda)}\\) back into their original units \\(y_i\\) is given by\n\\[\ny_i = g^{-1}(y_i^{(\\lambda)}) =\n\\begin{cases}\n(\\lambda \\cdot y_i^{(\\lambda)} + 1)^{1/\\lambda} & \\lambda \\neq 0 \\\\\n\\exp(y_i^{(0)}) & \\lambda = 0\n\\end{cases}\n.\n\\]\nThe objective of the Box-Cox transformation is to convert the distribution of the regression residuals \\(e_i^{(\\lambda)} = y_i^{(\\lambda)} - \\hat{y}_i^{(\\lambda)}\\)\n\nto a symmetric and homoscedastic distribution and preferably even\nto a normal distribution with \\(e_i^{(\\lambda)} \\sim N(0, \\sigma_{(\\lambda)}^2)\\).\n\nThis allows the robust estimation of the regression parameters \\(\\{\\hat{\\beta}_0^{(\\lambda)}, \\hat{\\beta}_1^{(\\lambda)}, \\ldots, \\hat{\\beta}_{K-1}^{(\\lambda)}\\}\\) in the transformed system. The predicted values of the endogenous variable in the transformed system are \\(\\hat{y}_i^{(\\lambda)}\\). These are conditional expectations \\(\\hat{y}_i^{(\\lambda)} = E[y_i^{(\\lambda)} | x_{i1}, \\ldots, x_{i,K-1}] = \\hat{\\beta}_0^{(\\lambda)} + \\hat{\\beta}_1^{(\\lambda)} \\cdot x_{i1} + \\cdots + \\hat{\\beta}_{K-1}^{(\\lambda)} \\cdot x_{i,K-1}\\) given the set of exogenous observations \\(\\{1, x_{i1}, x_{i2}, \\ldots, x_{i,K-1}\\}\\), which may or may not be transformed themselves.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#review-properties-of-median-and-expectation",
    "href": "AG_w01_TheoryReverseBoxCox.html#review-properties-of-median-and-expectation",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "4 Review: Properties of Median and Expectation",
    "text": "4 Review: Properties of Median and Expectation\nHighly skewed distributions or distributions with outliers in one tail have a levering effect on the arithmetic mean making it potentially an invalid estimate for the central tendency of a distribution. In contrast, the median (and also the trimmed mean) is more robust because extreme observations or long tails do not “pull” the median away from the central tendency of the underlying distribution.\n\nRecall that the mean minimizes the sum of squared deviations \\(\\min_{\\nu} \\sum_{i=1}^{n} (y_i - \\nu)^2\\), that is, this expression becomes minimal for \\(\\nu = \\bar{y}\\). Consequently, squaring the deviation of extreme observations exaggerates their large divergences even further. In order to mitigate these quadratic impacts and still minimize the sum of squared deviations, the mean needs to move towards the extreme observations rather than reflecting the central tendency of the underlying distribution.\nIn contrast, the median minimizes the sum of absolute differences \\(\\min_{\\nu} \\sum_{i=1}^{n} |y_i - \\nu|\\), that is, this expression become minimal for \\(\\nu = y_{median}\\). Therefore, the impact of extreme deviations remains comparable to that of typical deviations rather than becoming exaggerated as it is the case for the arithmetic mean.\n\nFor highly skewed distributions the median has a smaller mean-square-error and is therefore a more precise measure of centrality. Only if the data are approximately normal distributed then the mean is a more efficient measure of central tendency than the median.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#calibration-of-a-regression-model-in-a-transformed-system",
    "href": "AG_w01_TheoryReverseBoxCox.html#calibration-of-a-regression-model-in-a-transformed-system",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "5 Calibration of a Regression Model in a Transformed System",
    "text": "5 Calibration of a Regression Model in a Transformed System\nAny time one or more of the conditions below affect the dependent or independent variables, a data transformation toward symmetry or preferably normality may be advisable:\n\nAn untransformed regression system may be influenced by extreme observations and/or heavy tails of any of its variables. Their impact enters the model calibration with a quadratic weight and thus gives extreme observations a large leverage.\nThe residuals may show a pattern of heteroscedasticity, which may be induced by asymmetric distributions of the underlying variables in the regression model.\nThe relationships among the untransformed variables may be non-linear and thus ordinary least squares will not perform well to capture the relationships among the endogenous variable and the set of exogenous variables.\nThe distribution of the regression residuals, which were obtained by a linear regression model, exhibits a high degree of skewness.\n\nIn any of these cases modeling a linear regression system in the transformed domain with all variables and regression residuals being symmetrically distributed alleviates above problems and will lead to more robust estimates.\nIn order to further the interpretation of the model it is advisable to reverse the transformation into the original data units after the model has been calibrated in the transformed domain. The independent variables and predicted values of the dependent variable are mapped back into their natural scales. The key question becomes how do we map the predicted values, which were obtained in the transformed regression system, back into the untransformed domain? There are two alternative approaches:\n\nIf we want to maintain the robust qualities of the transformed system then we need to express the original relationship in terms of conditional medians.\nOn the other hand, if we want to account for the original skewness and outliers then the conditional expectations should be used.\n\nThe subsequent sections formally develop both reverse transformations. First it is shown that for any non-linear transformation \\(g^{-1}(\\cdot)\\) of a symmetrized variable \\(y_i^{(\\lambda)}\\) back into its original scale \\(y_i\\) the expectations differ \\(E[g^{-1}(y_i^{(\\lambda)})] \\neq g^{-1}(E[y_i^{(\\lambda)}])\\), where \\(g^{-1}(E[y_i^{(\\lambda)}])\\) is in fact the expected median of \\(y_i\\). Then the special case with \\(\\lambda = 0\\) will be discussed. This leads to the log-normal distribution for which the exact expectation \\(E[g^{-1}(y_i^{(\\lambda)})]\\) can be given in analytical terms. Finally, for the general case of a power normal distribution with \\(\\lambda \\neq 0\\) the expectation \\(E[g^{-1}(y_i^{(\\lambda)})]\\) is approximated using a Taylor series expansion.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#properties-of-transformed-density-functions",
    "href": "AG_w01_TheoryReverseBoxCox.html#properties-of-transformed-density-functions",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "6 Properties of Transformed Density Functions",
    "text": "6 Properties of Transformed Density Functions\nRecall that a density function \\(f(x)\\) of a continuous random variable \\(X\\) cannot be interpreted as probability per se, only the integral \\(\\int_{x-h}^{x+h} f(x) \\cdot dx\\) of the density function measures the probability \\(Pr(x - h \\leq X \\leq x + h)\\) that \\(X\\) is within an interval \\([x-h, x+h]\\).\nThe figure below, taken from Kennedy (1998, p. 36), demonstrates for a non-linear transformation \\(Y = g(X)\\) how the density function \\(f(x)\\) of the original random variable \\(X\\) changes to the density function \\(f^*(y)\\) of the transformed random variable \\(Y\\) (note a minor misprint: the axis labels \\(pdf(\\hat{\\beta})\\) and \\(pdf(g(\\hat{\\beta}))\\) should be switched).\n\n\n\nFigure 2.8 Why the expected value of a nonlinear function is not the nonlinear function of the expected value\n\n\nAssume that the probability of observing \\(x\\) in a very small neighborhood \\(dx\\) around \\(x\\) is \\(f(x) \\cdot dx\\). The transformation changes this small neighborhood to \\(|dy|\\), which is the absolute value of the range of \\(y\\) corresponding to \\(dx\\). The absolute value is used in case the transformation is a decreasing function rather than an increasing function. We obtain therefore the equality:\n\\[\n\\begin{aligned}\nf(x) \\cdot dx &= f^*(y) \\cdot |dy| \\text{ or } f^*(g(x)) \\cdot |dg(x)| \\\\\n\\Rightarrow \\quad f^*(y) &= f(x) \\cdot \\frac{dx}{|dy|}\n\\end{aligned}\n\\tag{1}\n\\]\nThe term \\(dx/|dy|\\) is the Jacobian, which ensures that the probabilities \\(\\int_{g(x_1)}^{g(x_2)} f^*(y) \\cdot dy = \\int_{x_1}^{x_2} f(x) \\cdot dx\\) in the transformed and untransformed system are equal and that \\(\\int_{-\\infty}^{\\infty} f^*(y) \\cdot dy = 1\\). In order to express the density \\(f^*(y)\\) just in terms of the argument \\(Y\\) we need to make use of the inverse transformation \\(x = g^{-1}(y)\\). This allows rewriting the density function (1) as\n\\[\nf^*(y) = f(g^{-1}(y)) \\cdot \\frac{d(g^{-1}(y))}{|dy|},\n\\tag{2}\n\\]\nwhich is expressed solely in terms of the transformed argument \\(y\\). In order to evaluate the Jacobian, the first derivative of the in inverse function \\(d(g^{-1}(y))/|dy|\\) with respect to \\(y\\) needs to be evaluated for each \\(y\\) within the support of \\(f^*(y)\\).\nOne can derive this rule easily by using the cumulative distribution function and the chain rule of differential calculus and noting that for the distribution function the equality \\(F(x) = F(g(x))|_{=y}\\) will hold for a positively increasing function \\(g(x)\\):\n\\[\n\\begin{aligned}\n\\frac{dF(x)}{dx} &= \\frac{dF(g(x))}{dx} \\\\\n\\Rightarrow f(x) &= \\frac{dF(y)}{dy} \\cdot \\frac{dg(x)}{dx} = f(y) \\cdot \\frac{dy}{dx} \\\\\n\\Rightarrow f(y) &= f(x) \\cdot \\frac{dx}{dy}\n\\end{aligned}\n\\]\nFor negatively decreasing functions \\(g(x)\\) the absolute value \\(|dy|\\) must be used.\nThe implications for the quantiles and expectation are:\n\nQuantiles of Y: The quantiles of a distribution function just shift with the transformation so that the equality \\(Pr(g(X) \\leq g(x)) = Pr(X \\leq x)\\) continues to hold. In particular, for the median \\(Pr(X \\leq X_{median}) = \\frac{1}{2}\\) we maintain the relationship \\(Pr(g(X) \\leq g(x_{median})) = Pr(X \\leq x_{median})\\).\nExpectation of Y: In contrast, after the transformation \\(Y = g(X)\\) has been performed the true expectation \\(\\mu_Y = E(Y) = \\int_{-\\infty}^{\\infty} y \\cdot f(g^{-1}(y)) \\cdot d(g^{-1}(y))/|dy| \\cdot dy\\) will not be identical to the transformed expectation of \\(X\\) unless the transformation \\(g(\\cdot)\\) is a linear function.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#special-case-the-log-normal-distribution-with-λ0",
    "href": "AG_w01_TheoryReverseBoxCox.html#special-case-the-log-normal-distribution-with-λ0",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "7 Special Case: The log-normal Distribution with λ=0",
    "text": "7 Special Case: The log-normal Distribution with λ=0\nThe density function: For \\(\\lambda = 0\\) the transformed variable \\(y_i^{(0)} = \\ln(x_i)\\) with follows a normal distribution \\(y_i^{(0)} \\sim N(\\mu_Y, \\sigma_Y^2)\\). Then the untransformed \\(x_i = \\exp(y_i^{(0)})\\) variable will follow a log-normal distribution\n\\[\nf_{ln}(x_i; \\mu_Y, \\sigma_Y) = \\frac{1}{x_i \\cdot \\sigma_Y \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot \\exp\\left(-\\frac{(\\ln(x_i) - \\mu_Y)^2}{2 \\cdot \\sigma_Y^2}\\right) \\quad \\text{for all } x_i &gt; 0.\n\\]\nThe median of log-normal distribution is \\(x_{median} = \\exp(y_{median}) = \\exp(\\mu_Y)\\) because the normal distribution of \\(Y^{(0)}\\) is symmetric. The expectation and variance of log-normal distribution, respectively, are \\(E(X) = \\exp(\\mu_Y + \\frac{1}{2} \\cdot \\sigma_Y^2)\\) and \\(Var(X) = (\\exp(\\sigma_Y^2) - 1) \\cdot \\exp(2 \\cdot \\mu_Y + \\sigma_Y^2)\\).\nExpected value in the regression model: For a regression model in the log-transformed system, the predicted conditional expectation \\(\\hat{y}_i^{(0)}\\) for each observation depends on the exogenous variables \\(\\hat{y}_i^{(0)} = E[y_i^{(0)} | x_{i1}, \\ldots, x_{i,K-1}] = \\hat{\\beta}_0^{(0)} + \\hat{\\beta}_1^{(0)} \\cdot x_{i1} + \\cdots + \\hat{\\beta}_{K-1}^{(0)} \\cdot x_{i,K-1}\\). After reversing the Box-Cox transformation we get the exact predictions \\(\\hat{y}_i = \\exp(\\hat{y}_i^{(0)} + \\frac{1}{2} \\cdot \\sigma_{e^{(0)}}^2)\\) where \\(\\sigma_{e^{(0)}}^2\\) is the variance of the regression residuals \\(e^{(0)} = \\hat{y}_i^{(0)} - y_i^{(0)}\\) in the transformed system.\nVariance heterogeneity in the regression model: The variance \\(Var(\\hat{y}_i) = (\\exp(\\sigma_{e^{(0)}}^2) - 1) \\cdot \\exp(2 \\cdot \\hat{y}_i^{(0)} + \\sigma_{e^{(0)}}^2)\\) also depends on the predicted values \\(\\hat{y}_i^{(0)}\\) and, therefore, is no longer constant. Consequently, the variances of the predictions \\(\\hat{y}_i\\) become heteroscedastic.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#the-power-normal-distribution-for-arbitrary-transformation-values-λ",
    "href": "AG_w01_TheoryReverseBoxCox.html#the-power-normal-distribution-for-arbitrary-transformation-values-λ",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "8 The Power Normal Distribution for Arbitrary Transformation Values λ",
    "text": "8 The Power Normal Distribution for Arbitrary Transformation Values λ\nFor any value for \\(\\lambda \\neq 0\\) with \\(y_i^{(\\lambda)} \\sim N(\\mu_{(\\lambda)}, \\sigma_{(\\lambda)})\\) we get the power normal density function of the inversely untransformed variable \\(y_i\\) by making use of equation (2), that is,\n\\[\nf(y_i; \\lambda, \\mu_i, \\sigma) \\sim \\frac{1}{K} \\cdot \\frac{1}{\\sigma_{(\\lambda)} \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot |\\lambda| \\cdot (y_i - 1)^{\\lambda - 1} \\cdot \\exp\\left(-\\frac{1}{2 \\cdot \\sigma_{(\\lambda)}^2} \\cdot \\left(\\underbrace{(\\lambda \\cdot y_i^{(\\lambda)} + 1)^{1/\\lambda}}_{=\\tilde{y}_i} - \\mu_{(\\lambda)}\\right)^2\\right).\n\\]\nThe adjustment factor \\(1/K\\) is required because the power normal distribution depends on a truncated normal distribution (see Freeman and Modarres, 2006).\nFor a transformed Box-Cox variable \\(Y^{(\\lambda)}\\) the median \\(Y_{median}\\) in the original units is simply given by \\(Y_{median} = (\\lambda \\cdot Y_{median}^{(\\lambda)} + 1)^{1/\\lambda}\\). However, if we are interested in the expectation and the variance of an inversely transformed Box-Cox variable \\(Y^{(\\lambda)}\\) both statistics must be approximated through a Taylor-series expansion \\(g^{-1}(x) = g^{-1}(a) + \\frac{\\partial g^{-1}(a)}{\\partial a} \\cdot (x - a) + \\frac{1}{2!} \\cdot \\frac{\\partial^2 g^{-1}(a)}{\\partial^2 a} \\cdot (x - a)^2 + \\frac{1}{3!} \\cdot \\frac{\\partial^3 g^{-1}(a)}{\\partial^3 a} \\cdot (x - a)^3 + R\\) with \\(R\\) being the remainder approximation error. For the inverse Box-Cox transformation, the Taylor-series approximation is developed for \\(g^{-1}(y_i^{(\\lambda)}) = (\\lambda \\cdot (y_i^{(\\lambda)}) + 1)^{1/\\lambda}\\) around \\(y_i^{(\\lambda)} = \\mu_{(\\lambda)}\\) up to the second degree term:\n\\[\n\\begin{aligned}\n(\\lambda \\cdot Y^{(\\lambda)} + 1)^{1/\\lambda} &\\approx (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda} + (Y^{(\\lambda)} - \\mu_{(\\lambda)}) \\cdot \\frac{\\partial(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}}{\\partial \\mu_{(\\lambda)}} + \\frac{1}{2} \\cdot (Y^{(\\lambda)} - \\mu_{(\\lambda)})^2 \\cdot \\frac{\\partial^2(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}}{\\partial^2 \\mu_{(\\lambda)}} \\\\\n&\\approx (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda} + (Y^{(\\lambda)} - \\mu_{(\\lambda)}) \\cdot (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{\\frac{1}{\\lambda} - 1} + \\frac{1}{2} \\cdot (Y^{(\\lambda)} - \\mu_{(\\lambda)})^2 \\cdot (1 - \\lambda) \\cdot (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{\\frac{1}{\\lambda} - 2}\n\\end{aligned}\n\\]\nThe moments of \\((\\lambda \\cdot Y^{(\\lambda)} + 1)^{1/\\lambda}\\) are difficult to calculate, but the moments on the right side of the Taylor-series approximation can be evaluated. Taking the expectation on both sides of the approximation and noting that the expectation of a constant is equal to the constant \\(E[(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}] = (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}\\) as well as that the expectation operator is distributive over a summation, that is, \\(E(Y^{(\\lambda)} - \\mu_{(\\lambda)}) = \\underbrace{E(Y^{(\\lambda)})}_{=\\mu_{(\\lambda)}} - \\mu_{(\\lambda)} = 0\\) we yield:\n\\[\n\\begin{aligned}\nE(Y) &\\approx \\underbrace{(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}}_{\\text{median term}} + \\underbrace{\\frac{1}{2} \\cdot \\sigma_{(\\lambda)}^2 \\cdot (1 - \\lambda) \\cdot (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{\\frac{1}{\\lambda} - 2}}_{\\text{expectation adjustment term}} \\\\\n&\\approx \\underbrace{(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{1/\\lambda}}_{\\text{median term}} \\cdot \\underbrace{\\left(1 + \\frac{1}{2} \\cdot \\sigma_{(\\lambda)}^2 \\cdot \\frac{(1 - \\lambda)}{(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^2}\\right)}_{\\text{expectation adjustment factor}}.\n\\end{aligned}\n\\]\nThe expectation adjustment factor highlights the difference between the median and the expectation. As expected, it is neutral when no transformation is applied, i.e., \\(\\lambda = 1\\).\nFor the variance we focus just on the 1 degree term of the Taylor-series approximating because the evaluation of the variance of the second degree term becomes rather elaborated (see Tiwari and Elston, 1999). The variance of the zero degree term is zero because it is not a random variable. Since the variance of \\(Var(a \\cdot X) = a^2 \\cdot Var(X) = a^2 \\cdot \\sigma^2\\) we obtain:\n\\[\n\\begin{aligned}\nVar(Y) &\\approx \\sigma_{(\\lambda)}^2 \\cdot \\left[(\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{\\frac{1}{\\lambda} - 1}\\right]^2 \\\\\n&\\approx \\sigma_{(\\lambda)}^2 \\cdot (\\lambda \\cdot \\mu_{(\\lambda)} + 1)^{\\frac{2}{\\lambda} - 2}\n\\end{aligned}\n\\]\nSubstituting the predicted values \\(\\hat{y}_i^{(\\lambda)}\\) in the transformed system for \\(\\mu_{(\\lambda)}\\) and the estimated residual variance \\(\\sum_{i=1}^{n} (y_i^{(\\lambda)} - \\hat{y}_i^{(\\lambda)})/(n - K)\\) in the transformed system for \\(\\sigma_{(\\lambda)}^2\\), respectively, gives the approximations for \\(E(\\hat{y}_i)\\) and \\(Var(\\hat{y}_i)\\) in the original system.\nFreeman and Modarres (2006) provide exact expectations and variances of the power normal density function for specific values of \\(\\lambda\\) through the evaluation of Chebyshev–Hermite polynomial expressions:\n\n\n\n\n\n\n\n\n\\(\\lambda\\)\n\\(E(Y)\\)\n\\(Var(Y)\\)\n\n\n\n\n0\n\\(\\exp(\\mu + \\frac{1}{2} \\cdot \\sigma^2)\\)\n\\((\\exp(\\sigma^2) - 1) \\cdot \\exp(2 \\cdot \\mu + \\sigma^2)\\)\n\n\n1/4\n\\((\\frac{1}{4} \\cdot \\mu + 1)^4 + \\frac{3}{8} \\cdot \\sigma^2 \\cdot (\\frac{1}{4} \\cdot \\mu + 1)^2 + \\frac{3}{256} \\cdot \\sigma^4\\)\n\\(\\frac{3}{2048} \\cdot \\sigma^8 + \\frac{3}{32} \\cdot \\sigma^6 \\cdot (\\frac{1}{4} \\cdot \\mu + 1)^2 + \\frac{21}{32} \\cdot \\sigma^4 \\cdot (\\frac{1}{4} \\cdot \\mu + 1)^4 + \\sigma^2 \\cdot (\\frac{1}{4} \\cdot \\mu + 1)^6\\)\n\n\n1/3\n\\((\\frac{1}{3} \\cdot \\mu + 1)^3 + \\frac{1}{3} \\cdot \\sigma^2 \\cdot (\\frac{1}{3} \\cdot \\mu + 1)\\)\n\\(\\frac{5}{243} \\cdot \\sigma^6 + \\frac{4}{9} \\cdot \\sigma^4 \\cdot (\\frac{1}{3} \\cdot \\mu + 1)^2 + \\sigma^2 \\cdot (\\frac{1}{3} \\cdot \\mu + 1)^4\\)\n\n\n1/2\n\\((\\frac{1}{2} \\cdot \\mu + 1)^2 + \\frac{1}{4} \\cdot \\sigma^2\\)\n\\(\\frac{1}{8} \\cdot \\sigma^4 + \\sigma^2 \\cdot (\\frac{1}{2} \\cdot \\mu + 1)^2\\)\n\n\n1\n\\(\\mu + 1\\)\n\\(\\sigma^2\\)\n\n\n\nThe parameters \\(\\mu\\) and \\(\\sigma\\) refer to the transformed system. Their formula needs to be evaluated individually for each value of \\(\\lambda\\). The second degree Taylor series approximations for the expectation are identically for \\(\\lambda \\in \\{0, \\frac{1}{3}, \\frac{1}{2}, 1\\}\\) and for \\(\\lambda = \\frac{1}{4}\\) they just differ by the summand \\(\\frac{3}{256} \\cdot \\sigma^4\\). However, the first degree Taylor approximation for the variance differs substantially from Freeman and Modarres’s exact moments except for \\(\\lambda \\in \\{0, 1\\}\\).",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "AG_w01_TheoryReverseBoxCox.html#literature-overview",
    "href": "AG_w01_TheoryReverseBoxCox.html#literature-overview",
    "title": "Week01: A Note on Switching between Transformed and Untransformed Regression Systems",
    "section": "9 Literature Overview",
    "text": "9 Literature Overview\nThe discussion above draws on the statistical derivations in\nAitkin, Francis, Hinde and Darnell (2009). Statistical Modelling in R. Oxford University Press, pp 123-126 and Kennedy, P. (1998). A Guide to Econometrics, 4th edition, MIT Press.\nContinue reading Aitken et al. beyond that section for an interesting application with a twist and a distinction between the Box-Cox transformation and link functions in the context of generalized linear models.\nYou may also want to look at these original articles by Box and coauthors:\nBox, G. E. P. and Cox, D. R. (1964). An analysis of transformations (with discussion). Journal of the Royal Statistical Society B, 26, 211–252.\nBox, G.E.P. and Tidwell, P.W. (1962). Transformations of the independent variable. Technometrics, 4, 541–50\nAn extension of the Box-Cox transformation for not strictly positive \\(y_i\\) is the Yeo-Johnson family of transformations. See also the function yjPower() in the car library. For a discussion see\nYeo and Johnson (2000) A new family of power transformations to improve normality or symmetry, Biometrika, 87:954-959\nA recent paper that develops the exact expectation and variance of the reverse Box-Cox transformation \\(g^{-1}(\\cdot)\\) by Chebyshev–Hermite polynomials from the perspective of a truncated normal distribution can be found at:\nFreeman, J. and Modarres R. (2006). Inverse Box–Cox: The power-normal distribution. Statistics & Probability Letters, 76, 764–772\nThe following paper develops the variance of a function of random variables in terms of a second degree multivariate Taylor-Series expansion and compares it to the Delta method which only uses a first degree Taylor expansion:\nTiwari, H. K. and Elston, R. C. (1999). The Approximate Variance of a Function of Random Variables. Biometrical Journal, 41, 351-357\n\n1 These notes are only for interested students. They are not test relevant.\n2 Alternatively, the function powerTransform(lm.mod) in the car library finds that λ-value, which brings the regression residuals the closest to the normal distribution.\n3 The l’Hôpital (sometimes spelled l’Hôspital) rule states that \\(\\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{\\partial f(x)/\\partial x}{\\partial g(x)/\\partial x}\\) with \\(\\partial g(c)/\\partial c \\neq 0\\). The limit of the later term is sometimes easier to evaluate.",
    "crumbs": [
      "Home",
      "(一) GISC7310 Advanced GIS Data Analysis",
      "Wk01-Switching between Transformed and Untransformed Regression Systems"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPPS6326_material_quarto2",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "",
    "text": "As with any statistical procedure machine learning algorithms train an estimator or prediction function on sample data.\nOur aim is to apply the predictor function to new (or future) dataset, that is, we hope to generalize the estimated predictor function to the whole population from which the training sample has been drawn.\nTherefore, the sample must be representative of the underlying population, that is, the target variable \\(Y\\) as well as the relevant set of \\(p\\) predictive features \\(\\{X_1, X_2, \\ldots, X_p\\}\\), which model the variation of the target variable \\(Y\\).\nAside from having a meaningful signal in the data, the target variable \\(Y\\) is also influenced by an observation specific unique random error. The predictive features cannot capture this random error.\n\nProblem: Overfitting the model will start capturing the unique random error, which cannot be generalized to new data.\n\nAny lack of representativeness will lead to biased estimated predictor function. The article BigDataAndStatistics.pdf gives several examples of this problem even if we are dealing with “big data”. Ultimately, we would like to learn what process mechanism has generated the observed data.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#basic-problem-of-machine-learning-estimated-prediction-function-is-sample-based",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#basic-problem-of-machine-learning-estimated-prediction-function-is-sample-based",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "",
    "text": "As with any statistical procedure machine learning algorithms train an estimator or prediction function on sample data.\nOur aim is to apply the predictor function to new (or future) dataset, that is, we hope to generalize the estimated predictor function to the whole population from which the training sample has been drawn.\nTherefore, the sample must be representative of the underlying population, that is, the target variable \\(Y\\) as well as the relevant set of \\(p\\) predictive features \\(\\{X_1, X_2, \\ldots, X_p\\}\\), which model the variation of the target variable \\(Y\\).\nAside from having a meaningful signal in the data, the target variable \\(Y\\) is also influenced by an observation specific unique random error. The predictive features cannot capture this random error.\n\nProblem: Overfitting the model will start capturing the unique random error, which cannot be generalized to new data.\n\nAny lack of representativeness will lead to biased estimated predictor function. The article BigDataAndStatistics.pdf gives several examples of this problem even if we are dealing with “big data”. Ultimately, we would like to learn what process mechanism has generated the observed data.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#central-limit-theorem-and-sampling-uncertainty",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#central-limit-theorem-and-sampling-uncertainty",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "2 Central Limit Theorem and Sampling Uncertainty",
    "text": "2 Central Limit Theorem and Sampling Uncertainty\n\nAnother uncertainty emerges: by chance even random samples may be outliers with respect to the underlying population.\n\nTherefore, the estimated predictor function will have a standard error measuring its inherent uncertainty.\nThe central limit theorem allows us to demonstrate this problem.\n\nDef. Central Limit Theorem: Let \\(X_1, X_2, \\ldots, X_n\\) be a random independent sample of size \\(n\\) drawn from an arbitrarily distributed population with expectation \\(\\mu\\) and standard deviation \\(\\sigma\\). Then for large enough sample sizes \\(n\\), the sampling distribution of the mean \\(\\bar{X}\\) is asymptotically (i.e., as \\(n \\to \\infty\\)) normal distributed with \\(\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)\\).\nThere are two parts to this theorem:\n\nIrrespectively of the sample size the expected value of the mean \\(\\bar{X}\\) is \\(E(\\bar{X}) = \\mu\\), i.e., it is equal to that of the underlying population expectation \\(\\mu\\), and its variance is \\(Var(\\bar{X}) = \\sigma^2/n\\).\nNote, \\(n\\) in the denominator. Therefore, as the sample size \\(n\\) increases the standard error (or variance) \\(s_{\\bar{X}} = \\sqrt{\\sigma^2/n} = \\frac{\\sigma}{\\sqrt{n}}\\) of the mean estimator will shrink. Thus, we are gaining precision, and it is less likely that we work with an extreme sample.\n\n\n\n\n\nFigure 2.24 - Three samples of five observations drawn randomly from U-shaped distribution\n\n\n\nAsymptotically the sample mean estimator will follow a normal distribution irrespective of the underlying distribution of the population.\n\nProof of independent sample objects:\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} Var(X_i) = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}\n\\]\n\nExample: Central limit theorem with the R-script CENTRALLIMIT.R:\n\n\n\n\nCentral Limit Theorem Example\n\n\n\nIn summary: different underlying population distributions will lead to a normal distribution of the sample statistic \\(\\bar{X}\\) (and equivalently the sum \\(\\sum_{i=1}^{n} X_i\\)), which is unbiased and the standard error for the mean shrinks with increasing sample size following the rule \\(\\sigma_{\\bar{X}} = \\frac{\\sigma_{population}}{\\sqrt{n}}\\).\nThe central limit theorem becomes important later when we use ensemble learners by pooling and averaging high variance models together.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#fundamental-setting-of-supervised-ml",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#fundamental-setting-of-supervised-ml",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "3 Fundamental Setting of Supervised ML",
    "text": "3 Fundamental Setting of Supervised ML\nSome of the figures in this presentation are taken from Chapter 2 of “An Introduction to Statistical Learning, with applications in R (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani”\n\n\\(Y_i\\): target, dependent, or response variable for the \\(i^{th}\\) sample observation of a sample of size \\(n\\).\n\\(\\mathbf{X}_i = (X_{i1}, X_{i2}, \\ldots, X_{ip})^T\\): vector features, independent or predictor variables for the \\(i\\) sample observation with \\(p\\) being number of single features describing object \\(i\\).\n\n\n\n\nFundamental setting showing f and error\n\n\n\nThe objective of machine learning is to identify the true underlying population function \\(f(\\cdot)\\) connecting the target variable with the features: \\(y_i = f(\\mathbf{x}_i) + \\varepsilon_i\\) where \\(\\varepsilon_i\\) is the irreducible error.\nThe irreducible error is due to:\n\nmodel misspecification (e.g., missing relevant \\(X_j\\)s) or\nunmeasurable variation in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) or\ninherent random noise allowing in \\(Y_i\\) for each population member, thus even the true prediction function \\(f(\\cdot)\\) will deviate to some degree from \\(Y_i\\).\n\nTo calibrate the prediction function we aim at minimizing the reducible error with a properly estimated function \\(\\hat{f}(\\mathbf{x})\\), which is estimated using sample data:\n\n\\[\nE(y - \\hat{y})^2 = E[f(\\mathbf{x}) + \\varepsilon - \\hat{f}(\\mathbf{x})]^2\n\\] \\[\n= \\underbrace{E[f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})]^2}_{reducible} + \\underbrace{Var(\\varepsilon)}_{irreducible}\n\\]\n\nMachine learning focuses on estimating the predictor function based on a training sample. The estimated function may not be optimally calibrated for the sample data.\nFuture observations of the target variable (the underlying population) are predicted using predictor function.\nMachine learning usually approaches the function \\(\\hat{f}(\\mathbf{x})\\) from a black box perspective, i.e., not being directly interested in its internal structure and estimated parameters.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#parametric-vs-non-parametric-methods",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#parametric-vs-non-parametric-methods",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "4 Parametric vs Non-Parametric Methods",
    "text": "4 Parametric vs Non-Parametric Methods\n\nIn contrast, inferential statistics (confirmatory statistics) focus on\n\nWhich explanatory variables are associated with the response?\nWhat is the relationship between the explanatory variables and the response?\nWhich functional form adequately summarizes this relationship. \\(\\Rightarrow\\) Perhaps beyond a simple linear relationship.\n\nParametric methods:\n\nAssumptions about the functional form \\(f(\\cdot)\\), e.g., linear model \\(f(\\mathbf{x}_i) = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\beta_2 \\cdot x_{i2} + \\cdots + \\beta_p \\cdot x_{ip}\\).\nEstimate the parameters \\(\\{\\beta_0, \\beta_1, \\ldots, \\beta_p\\}\\) of the functional form with training dataset\nAssume a distribution of the unmeasurable variation \\(\\varepsilon_i\\).\nThis allows us to calculate confidence intervals for the predictions \\(\\hat{y}\\) and the estimate parameters assuming the model is specified correctly.\nModels are supposed to be parsimonious.\n\nNon-parametric methods:\n\nAim is to trace the conditional expectation \\(f(x_i) = E(y_i|X = x_i)\\).\nNo explicit assumption about the functional form is taken.\nError distribution is ignored.\nLack of an exact functional form \\(f(x_i)\\) provides a more flexibility fitting to the sample data.\nA higher number of parameters defining \\(f(x_i)\\) is needed.\nThere is a strong risk of overfitting the model by capturing the observation specific irreducible error.\n\nThere is a trade-off between prediction accuracy and model simplicity. Abstraction (model simplicity) fosters interpretability and generalizability.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#curse-of-dimensionality",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#curse-of-dimensionality",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "5 Curse of Dimensionality",
    "text": "5 Curse of Dimensionality\n\nFrequently, information from neighboring data points need to be borrowed:\n\n\\[\n\\hat{f}(x_i) = Ave(y_j | X = x_j; x_j \\in \\mathcal{N}(x_i))\n\\]\nwhere \\(x_j \\in \\mathcal{N}(x_i)\\) signifies the data values in the neighborhood of the data point \\(i\\).\nThis can lead to smoothing of the estimated function \\(\\hat{f}(x_i)\\).\n\nAs the number of variables, i.e., the dimensions \\(p\\), increases the distribution of data points becomes more and more sparse in the \\((p + 1)\\)-dimensional space.\n\nThus there are less data points in the neighborhood \\(\\mathcal{N}(x_i)\\).\n\nThis forces an increase of the neighborhood to have sufficient data values in its range of the prediction location \\(x_i\\) to estimate \\(\\hat{f}(x_i)\\).\n\n\n\n\nCurse of Dimensionality\n\n\n\nHowever, the further we move away from \\(x_i\\) the more varied the functional form \\(\\hat{f}(x_j)\\) will become. Thus, averaging will accumulate more deviation from the true value \\(f(x_i)\\) at \\(x_i\\).\nMethods to reduce the dimensionality in the \\(p\\) features to obtain more precise estimates (parsimonious) are called in machine learning regularization.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#impact-of-the-irreducible-error-variance",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#impact-of-the-irreducible-error-variance",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "6 Impact of the Irreducible Error Variance",
    "text": "6 Impact of the Irreducible Error Variance\n\nIn supervised learning both the outcome \\(y_i\\) and the features \\(\\mathbf{x}_i\\) of object \\(i\\) are observable.\nThe outcome and the features are connected by a true function \\(f(\\cdot)\\) plus an unknown random error \\(\\varepsilon_i\\):\n\n\\[\ny_i = f(\\mathbf{x}_i) + \\varepsilon_i\n\\]\n\nThe uncertainty of estimating \\(\\hat{f}(\\cdot)\\) for true but unknown \\(f(\\cdot)\\) increases as the irreducible error increases.\n\nThe left panel shows the distribution of data points from a data generating process (DGP) based on a cubic linear model\n\\[\ny = \\beta_0 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + \\beta_3 \\cdot x^3 + \\varepsilon\n\\]\nwith increasing variation of the irreducible error distribution \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nThe right panel compares the true function \\(f(\\cdot)\\) (blue line) against the estimated function \\(\\hat{f}(\\cdot)\\) (red line).\n\n\n\nImpact of Irreducible Error Variance",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#supervised-learning",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#supervised-learning",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "7 Supervised Learning",
    "text": "7 Supervised Learning\n\nIn supervised learning a training sample with \\((y_i, \\mathbf{x}_i)\\) is used to obtain an “accurate” estimate \\(\\hat{y} = \\hat{f}(\\cdot)\\) of \\(f(\\cdot)\\).\nAt test sample \\((y_0, \\mathbf{x}_0)\\) is usually withheld to evaluate the predictive accuracy of the estimator \\(\\hat{f}(\\cdot)\\).\nSince many different estimators \\(\\hat{f}(\\cdot)\\) of varying degrees of flexibility can be specified there is usually a trade-off between prediction accuracy and model generality (see R-script RegPolyThinJunk.r).\nOverfitting leads to estimators \\(\\hat{f}(\\cdot)\\) being well adapted to a particular (given) sample and most likely modeling its specific random error \\(\\varepsilon\\), but they do not generalize to another sample which will have a different random error \\(\\varepsilon\\).\n\n\\(\\Rightarrow\\) There is no free lunch in statistics: not one estimation model dominates all others over all possible data sets.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#evaluating-the-model-fit",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#evaluating-the-model-fit",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "8 Evaluating the Model Fit",
    "text": "8 Evaluating the Model Fit\n\nIn a regression setting the Mean-Square-Error of the training or testing data set can be used\n\n\\[\nMSE_{Train} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - \\hat{f}(\\mathbf{x}_i))^2 \\text{ or } MSE_{Test} = \\frac{1}{m} \\cdot \\sum_{j=1}^{m} (y_{0,j} - \\hat{f}(\\mathbf{x}_{0,j}))^2\n\\]\nwhere the subscript 0 in \\(y_{0,j}, \\mathbf{x}_{0,j}\\) denotes the test data \\(j \\in \\{1, 2, \\ldots, m\\}\\)\n\nOther goodness of fit measures, such as the “confusion” matrix and ROC (receiver operating characteristics) are also used for classification problems.\nDecomposition of expected squared prediction error for the testing dataset becomes\n\n\\[\nE(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\varepsilon)\n\\]\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\n\\(E(y_0 - \\hat{f}(x_0))^2\\)\nExpected \\(MSE\\) over all possible test samples \\((y_0, x_0)\\) with\\(E(y_0 - \\hat{f}(x_0))^2 = E(f(x_0) + \\varepsilon - \\hat{f}(x_0))^2\\)mean square error\n\n\n\\(Var(\\hat{f}(x_0))\\)\nVariation of the estimated function \\(\\hat{f}(\\cdot)\\) based on the use of different training samples to calibrate the function.\\(Var(\\hat{f}(x_0)) = E[\\hat{f}(x_0) - E(\\hat{f}(x_0))]^2\\)Simpler models are more stable with less variability in \\(\\hat{f}(\\cdot)\\). Simpler models do not run the risk of modeling the sample specific random variability \\(\\varepsilon\\) but may not capture the true function \\(f(\\cdot)\\) well.\n\n\n\\([Bias(\\hat{f}(x_0))]^2\\)\nThis variation error is introduced by mis-specifying the true functional form \\(f(\\cdot)\\) of the data generating process (DGP). The bias is the difference \\(f(x_0) - \\hat{f}(x_0)\\).\\(Bias^2(\\hat{f}(x_0)) = [\\hat{f}(x_0) - f(x_0)]^2\\)Under-fitting using a simple model relative to more complex DGP is a certain cause for a large bias.A properly selected model \\(\\hat{f}(\\cdot)\\) will have no bias.\n\n\n\\(Var(\\varepsilon)\\)\nIrreducible error variation. Overfitted models try to capture this sample specific random variation. This is the smallest value that the expected MSE can achieve for a perfectly fitting estimated function \\(\\hat{f}(\\cdot)\\).\n\n\n\n\nGeneral rule: As the model becomes more flexible (\\(\\uparrow\\)) the bias will decrease (\\(\\downarrow\\)) but the variance will increase (\\(\\uparrow\\)).\nA detailled derivation of the decomposition of the MSE into the predictor variance and the predictor bias can be found at: Bias–variance tradeoff - Wikipedia.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#bias-variance-tradeoff-examples",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#bias-variance-tradeoff-examples",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "9 Bias-Variance Tradeoff Examples",
    "text": "9 Bias-Variance Tradeoff Examples\n\nExample: Medium number of parameters in DGP and high error variance:\n\n\n\n\nFigure 2.9 - Medium parameters, high error variance\n\n\n\nBlack line is the true underlying data generating process (DGP) with a random error.\nThe horizontal dashed line is the MSE solely associated with the variance of irreducible random error \\(Var(\\varepsilon)\\), that is, only overfitted models have a MSE below the variation of the irreducible error.\nTrade-off between Variance and Bias for the independent prediction dataset \\(y_0\\) and \\(\\mathbf{x}_0\\).\nExample: Low number of paramters in DGP and high error variance of DGP:\n\n\n\n\nFigure 2.10 - Low parameters, high error variance\n\n\n\nExample: High number of parameters in DGP and low error variance\n\n\n\n\nFigure 2.11 - High parameters, low error variance",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#classification-bayesian-k-nearest-neighbor-class-assignment",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#classification-bayesian-k-nearest-neighbor-class-assignment",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "10 Classification: Bayesian k-Nearest Neighbor Class Assignment",
    "text": "10 Classification: Bayesian k-Nearest Neighbor Class Assignment\n\nIn the classification setting the outcome variable is discrete, that is,\n\n\\[\ny_i = k \\text{ if object } i \\text{ belongs to the } k^{th} \\text{ class}\n\\]\n\nThe error rate, which we aim to minimize, becomes \\(\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\\), where the indicator function\n\n\\[\nI(\\cdot) = \\begin{cases}\n1 & \\text{if } y_i \\neq \\hat{y}_i \\\\\n0 & \\text{if } y_i = \\hat{y}_i\n\\end{cases}\n\\]\n\nA Bayesian classifier assigns an observation \\(i\\) to the class \\(k\\) if its conditional probability \\(Pr(Y = k | X = \\mathbf{x}_0)\\) is larger than that for any other class.\n\nFor just two classes \\(K = 2\\) the probability must be \\(Pr(Y = k | X = \\mathbf{x}_0) &gt; 0.5\\).\n\nThe Bayesian class membership probabilities in k-nearest neighborhood (kNN) is calculated as:\n\nDetermine those \\(k\\) training points which are the closest to the prediction point \\(\\mathbf{x}_0\\).\nCloseness is measured in terms of the Euclidian distance between two objects \\(i\\) and \\(j\\) with their feature vectors \\(\\mathbf{x}_i = (x_{i1}, x_{i2}, \\ldots, x_{iP})^T\\) and \\(\\mathbf{x}_j = (x_{j1}, x_{j2}, \\ldots, x_{jP})^T\\) by\n\\[\nd_{ij} = \\sqrt{\\sum_{p=1}^{P} (x_{ip} - x_{jp})^2}\n\\]\n\n\n\n\n\nFigure 2.13 - Simulated data set with Bayes decision boundary\n\n\n\nImportant: Distances change with the scale (i.e., range) of the underlying variables. This is a problem encountered in all distance-based machine learning methods.\n\n\n\n\nFigure 2.14 - KNN approach with K=3\n\n\n\nSubsequently estimate the Bayesian group probabilities by\n\\[\nPr(Y = k | X = \\mathbf{x}_0) = \\frac{1}{|\\mathcal{N}_0|} \\sum_{i \\in \\mathcal{N}_0} I(y_i = k)\n\\]\nwhere \\(\\mathcal{N}_0\\) is the neighborhood around the prediction point \\(\\mathbf{x}_0\\) with \\(|\\mathcal{N}_0|\\) the number of training points \\(K\\) around the prediction location.\nAssign the observation to class \\(k\\) with the highest probability.\nThe user decides the hyper-parameter \\(|\\mathcal{N}_0|\\) to the desired number of nearest neighbors\n\nThis hyper-parameter \\(|\\mathcal{N}_0|\\) determines flexibility of kNN algorithm:\n\n\n\nFigure 2.16 - Comparison of KNN decision boundaries K=1 and K=100\n\n\n\nThe hyperparameter \\(|\\mathcal{N}_0|\\) also influences the test and training error rates:\n\n\n\n\nFigure 2.17 - KNN training and test error rates\n\n\n\nThe function e1071::gknn() performs kNN model calibrations.\nk-nearest neighbors can also be used to handle metric dependent variables \\(y\\). In that case, the predicted value \\(y_0\\) is the average value of the observed values \\(y_i\\) in its neighborhood \\(\\mathcal{N}_0\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#advantages-and-disadvantages-of-knn",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#advantages-and-disadvantages-of-knn",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "11 Advantages and Disadvantages of kNN",
    "text": "11 Advantages and Disadvantages of kNN\n\nIn case of an overall class tie, the predicted class is chosen at random.\nAdvantages of the kNN model:\n\nSimple and effective estimation, which gives by default the probability of the predicted group membership.\nMakes no assumptions about the underlying distribution of the features.\nVirtually no training time required.\n\nDisadvantages of the kNN model:\n\nAs the number of features increases, the decision space becomes more sparsely populated (curse of dimensionality) which increases the overall uncertainty.\nHow are ties (two classes have equal maximum probabilities) broken?\nFeatures that are irrelevant for the classification task are not eliminated (Machine Learning concept of regularization) thus there is the potential of overfitting the data.\nThe performance of the algorithm depends on how comparable the features are scaled to evaluate their distances.\nFor each test data point \\(\\mathbf{x}_0\\) the distances need to be calculated to identify the k nearest neighborhood \\(\\mathcal{N}_0\\).\nEuclidian distances for nominal scaled features and missing feature information require additional processing.\nThe algorithm’s prediction performance depends on the the hyper-parameter \\(k\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w02_KeyConceptsofStatisticalLearning.html#fundamental-setting-of-unsupervised-ml",
    "href": "ML_w02_KeyConceptsofStatisticalLearning.html#fundamental-setting-of-unsupervised-ml",
    "title": "Week02: Key Concepts of Statistical Learning",
    "section": "12 Fundamental Setting of Unsupervised ML",
    "text": "12 Fundamental Setting of Unsupervised ML\n\nIn unsupervised learning, only the object features \\(\\mathbf{x}_i\\) but not the outcome \\(y_i\\) are known to the investigator.\nA function \\(f(\\cdot)\\) is sought that allows us to guess what \\(y\\) could be based on the inherent structure within the observed feature matrix \\(\\mathbf{X}\\).\nAn example of unsupervised learning is to group feature adjacent objects together into clusters. With respect to their features \\(\\mathbf{x}_i\\) the clusters are supposed to be internally as homogenous as possible, but the identified clusters should be as heterogeneous (distinct) from each other.\nFuzziness in the cluster delimitations determines the success of the classification rule.\nExample: Identifying a multivariate mixture distribution\n\nThe distribution of points with multiple centers and ellipsoidal standard distances can be modeled by a mixture of multi-normal distributions.\nFor example in the 2-dimensional setting the mixture distribution the joint density becomes:\n\n\n\n\n\nFigure 2.8 - Clustering data set with three groups\n\n\n\\[\nf\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix} \\sim \\sum_{q=1}^{Q} \\pi_q \\cdot \\mathcal{N}\\left(\\begin{bmatrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{bmatrix}, \\begin{bmatrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{bmatrix}\\right)\n\\]\nwith \\(\\pi_q &gt; 0\\) and \\(\\sum_{q=1}^{Q} \\pi_q = 1\\)\n\nUnknowns are:\n\n[a] the number of clusters \\(Q\\),\n[b] the proportion of points per cluster \\(\\pi_q\\),\n[c] the \\(Q\\) cluster centers \\(\\begin{bmatrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{bmatrix}\\) and\n[d] their \\(Q\\) ellipsoidal covariance matrices \\(\\begin{bmatrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{bmatrix}\\).\n\nThe probability \\(Pr(q | \\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix})\\) of observation \\(\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}\\) belonging to cluster \\(q\\) is evaluated using the Bayesian theorem\n\n\\[\nPr\\left(q \\middle| \\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}\\right) \\sim \\frac{f\\left(\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix} \\middle| q\\right) \\cdot Pr(q)}{\\sum_{q=1}^{Q} f\\left(\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix} \\middle| q\\right) \\cdot Pr(q)}\n\\]\nwith the likelihood of \\(\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}\\) being in the \\(q^{th}\\) class\n\\[\nf\\left(\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix} \\middle| q\\right) \\sim \\mathcal{N}\\left(\\begin{bmatrix} \\mu_{x,q} \\\\ \\mu_{y,q} \\end{bmatrix}, \\begin{bmatrix} \\sigma_{x,q}^2 & \\sigma_{xy,q} \\\\ \\sigma_{xy,q} & \\sigma_{y,q}^2 \\end{bmatrix}\\right)\n\\]\nand the prior probability \\(Pr(q)\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk02-Key Concepts of Statistical Learning"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html",
    "href": "ML_w03_ClassificationRegression.html",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "",
    "text": "Regression is a parametric method:\n\nParametric methods are rooted in specific assumptions.\nTheir modeling outcomes can be generalized to an unknown population if their assumptions are satisfied. Thus, the validity of the underlying assumptions needs to be verified.\nImplicitly the scale of the target and features are accounted. Thus, standardization to make the variables comparable not neccessary.\n\nHowever, it may improve numerical stability during the model calibration.\n\nAside from making predictions, parametric methods also allow us to make statements about the underlying data generating process through an interpretation of the estimated model parameters.\nDue to the small number of parameters, parametric methods are rather inflexible.\nMost parametric methods are based on explicit distributional assumptions. This allows to calculate confidence intervals directly.\n\nNon-parametric methods (e.g., \\(k\\) nearest neighbors):\n\nThey are more data driven than relying on assumptions, i.e., about an underlying distribution.\nConfidence intervals can only be obtained by using bootstrapping or \\(k\\)-fold cross-validation.\nThey are more flexible to adjust to an underlying pattern in the sample data.\nThe sole objective of non-parametric methods in AI/ML is to perform predictions.\nThe scale of the features needed to be handled explicitly in distance-based methods.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#regression-logistic-regression-and-briefly-discriminant-analysis",
    "href": "ML_w03_ClassificationRegression.html#regression-logistic-regression-and-briefly-discriminant-analysis",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "",
    "text": "Regression is a parametric method:\n\nParametric methods are rooted in specific assumptions.\nTheir modeling outcomes can be generalized to an unknown population if their assumptions are satisfied. Thus, the validity of the underlying assumptions needs to be verified.\nImplicitly the scale of the target and features are accounted. Thus, standardization to make the variables comparable not neccessary.\n\nHowever, it may improve numerical stability during the model calibration.\n\nAside from making predictions, parametric methods also allow us to make statements about the underlying data generating process through an interpretation of the estimated model parameters.\nDue to the small number of parameters, parametric methods are rather inflexible.\nMost parametric methods are based on explicit distributional assumptions. This allows to calculate confidence intervals directly.\n\nNon-parametric methods (e.g., \\(k\\) nearest neighbors):\n\nThey are more data driven than relying on assumptions, i.e., about an underlying distribution.\nConfidence intervals can only be obtained by using bootstrapping or \\(k\\)-fold cross-validation.\nThey are more flexible to adjust to an underlying pattern in the sample data.\nThe sole objective of non-parametric methods in AI/ML is to perform predictions.\nThe scale of the features needed to be handled explicitly in distance-based methods.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#parametric-linear-regression",
    "href": "ML_w03_ClassificationRegression.html#parametric-linear-regression",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "2 Parametric Linear Regression",
    "text": "2 Parametric Linear Regression\n\nThe parameters in multiple linear regression model \\(y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\cdots + \\beta_K \\cdot x_{iK} + \\varepsilon_i\\) are the regression coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_K\\).\nThe predicted value is \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_{i1} + \\cdots + \\hat{\\beta}_K \\cdot x_{iK}\\) where \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) are the estimated regression coefficients.\nThese parameters are estimated by a method called ordinary least squares, which aims at finding that set of the parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) which minimize the residual sum of squares RSS, i.e.,\n\\[\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n\n\n\n\nFigure 3.2 - Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given by (3.4).\n\n\n\nFor two independent variables \\(X_1\\) and \\(X_2\\) the model has the graphical representation:\n\n\n\n\nFigure 3.4 - In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The plane is chosen to minimize the sum of the squared vertical distances between each observation (shown in red) and the plane.\n\n\n\nThe estimate parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) internally account for the scale of the features \\(X_1, \\dots, X_K\\).\nAssumptions about the model structure:\n\n[A1] The features \\(X\\) are free of random effects.\n[A2] The error term has an expected value of zero, i.e., \\(E[\\varepsilon_i] = 0\\)\n[A3] All relevant features are in the model.\n[A4] The underlying data generating process is linear in the features.\n[A5] The variance of the error term is constant, i.e., \\(Var[\\varepsilon_i] = constant \\ \\forall \\ i\\)\n[A6] The error terms are independent among each other, i.e., \\(Cov[\\varepsilon_i, \\varepsilon_j] = \\begin{cases} 0 & i \\neq j \\\\ \\sigma^2 & i = j \\end{cases}\\).\n[A7] Additionally, the error terms are preferred to be normally distributed \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\ \\forall \\ i\\).\n\nWhen these assumptions are satisfied, the estimated regression parameter \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) are unbiased with the smallest standard errors. Thus, the estimate model can be generalized to yet not available feature values.\nLinear regression is a special case of the generalized linear model (GLM).\n\n\n\n2.1 Addressing questions about the model\n\nDo all features or just a selected subset help to explain the target? (\\(\\rightarrow\\) \\(t\\)-test and stepwise regression, Ridge or Lasso regression)\nHow well does the model fit the data? (\\(\\rightarrow\\) \\(R_{adj}^2\\), \\(AIC\\), or \\(BIC\\))\nHow do we handle uncertainty in the prediction? (\\(\\rightarrow\\) prediction confidence intervals)\n\n\nThe global F-test allows to evaluate whether the model overall has some explanatory power, i.e.,\n\\[H_0: \\beta_1 = \\cdots = \\beta_K = 0 \\text{ against at least one } \\beta_k \\neq 0\\]\n\\[F = \\frac{(TSS - RSS)/K}{RSS/(n - K - 1)}\\]\nThe partial F-test allows us to evaluate whether a sub-set of features is statistically irrelevant.\nEach feature can be tested whether it is relevant in explaining a proportion of the variation in the target by the statistical test by the t-test:\n\\[H_0: \\beta_k = 0 \\text{ against } H_1: \\beta_k \\neq 0\\]\nIf the associate error probability of rejection the null hypothesis \\(H_0\\) – even though it is true – becomes reasonable small, then we reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_1\\).\n\n\n\n\nTable 3.4 - For the Advertising data, least squares coefficient estimates of the multiple linear regression of number of units sold on radio, TV, and newspaper advertising budgets.\n\n\n\nRegression is a statistical model involving an error distribution. The error distribution is associated with the irreducible error of the model. Confidence intervals around the regression plane or an individual point prediction allows us to assess the predictive quality of the model.\nStepwise, Ridge and Lasso feature selection will be discussed in Gareth James Chapter 6 “Linear Model Selection and Regularization”.\nThe multiple feature perspective of regression models allows control for confounding.\nExample: Students tend to have higher debit balances than non-students, so their marginal default rate is higher than for non-students. But for each level of balance, students default less than non-students.\n\n\n\n\nFigure - Default Rate by Credit Card Balance and Student Status",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#flexibility-of-the-regression-model",
    "href": "ML_w03_ClassificationRegression.html#flexibility-of-the-regression-model",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "3 Flexibility of the regression model",
    "text": "3 Flexibility of the regression model\n\nCategorical features (see also Boehmke p 61)\n\nCategorical variables in R are called factors.\nBesides metric features regression can also handle factors (categorical features). Distance-based methods cannot handle factors well even after dummy or one-hot encoding.\nEach factor level is encoded as a dummy variable.\n\n\n\n\n\nFigure 3.9 - Eight observations containing a categorical feature X and the difference in how one-hot and dummy encoding transforms this feature.\n\n\n\nDue to the redundancy of the set of factor levels one factor level needs to be dropped in regression analysis explicitly from the model. It can be calculated implicitly.\nR’s function in the regression family automatically encode factors as dummy variables.\nNon-linear functions in the features:\n\nAllows expressing non-linear relationships between the target and the features in a linear setting.\nEach feature can be transformed, e.g., Box-Cox or Yeo-Johnson.\nEach feature can be expressed as a polynomial function (see R’s function poly( )), i.e.,\n\n\\[\\beta_{k_1} \\cdot X_k + \\beta_{k_2} \\cdot X_k^2 + \\beta_{k_3} \\cdot X_k^3 + \\cdots\\]\n\nPolynomial functions bear the risk of overfitting the data and multicollinearity among the power terms.\n\n\n\n\n\nFigure 3.8 - The Auto data set. For a number of cars, mpg and horsepower are shown. The linear regression fit is shown in orange. The linear regression fit for a model that includes horsepower^2 is shown as a blue curve. The linear regression fit for a model that includes all polynomials of horsepower up to fifth-degree is shown in green.\n\n\n\nInteraction effects\n\nFeatures many influence a target in non-linear unison rather than separately. One feature may enhance or diminish the effects of another variable.\nThis interplay among features is modelled by interaction effects, e.g.,\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\beta_3 \\cdot X_1 \\cdot X_2 + \\varepsilon\\] \\[= \\beta_0 + (\\beta_1 + \\beta_3 \\cdot X_2) \\cdot X_1 + \\beta_2 \\cdot X_2 + \\varepsilon\\]\n\n\n\n\nTable 3.9 - For the Advertising data, least squares coefficient estimates associated with the regression of sales onto TV and radio, with an interaction term, as in (3.33).\n\n\n\n\n3.1 Caveats of Regression\n\nAs soon as the target variable is non-linearly transformed, the regression model becomes non-linear. It still can be evaluated by conditional effects plots.\nOutlying observations must be identified and handled with care because they exhibit a strong influence on the estimated parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\).\nHighly correlated features are redundant. This redundancy increases the uncertainty (standard error) in the estimated parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\).\nAutocorrelation and heteroscedasticity leave the estimated parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) unbiased but usually inflate their standard errors.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#non-parametric-k-nearest-neighbors",
    "href": "ML_w03_ClassificationRegression.html#non-parametric-k-nearest-neighbors",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "4 Non-parametric \\(k\\)-nearest Neighbors",
    "text": "4 Non-parametric \\(k\\)-nearest Neighbors\n\n\\(k\\)-nearest neighbors’ estimation of a metric or class feature is a non-parametric method and needs to hold all sample observation in memory to perform the prediction.\nIt is only driven by the hyper-parameter \\(k\\) which cannot directly be estimated from the data.\nTo calculate the among-objects-distances, the scale of metric features needs to be set by the analyst perhaps by making the feature scales comparable.\nFactor variables should be avoided, because the definition of object distances in terms of categorical features is ambiguous.\nIrrespectively of whether the target \\(Y\\) is metric or categorical, the underlying predicted value \\(\\hat{Y}_0\\) at location \\(X_{01}, \\dots, X_{0K}\\) is\n\\[\\hat{Y}_0 = \\frac{1}{k} \\cdot \\sum_{X_{i1}, \\dots, X_{iK} \\in \\mathcal{N}_k} Y_i\\]\n\n\n\n\nFigure 3.16 - Plots of f(X) using KNN regression on a two-dimensional data set with 64 observations (orange dots). Left: K = 1 results in a rough step function fit. Right: K = 9 produces a much smoother fit.\n\n\n\nFor \\(k = 1\\) the KNN fits the sample observations perfectly (most flexible fit). The bias is low, but the sample-to-sample variance is high.\nSee R-script kNNRegression.r.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#parametric-logistic-regression",
    "href": "ML_w03_ClassificationRegression.html#parametric-logistic-regression",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "5 Parametric Logistic Regression",
    "text": "5 Parametric Logistic Regression\n\nLogistic regression is a parametric supervised classification procedure for binary outcomes.\nThe target variable is a factor (categorical variable) describing the mutually exclusive and exhaustive class membership of each observation.\nThe objective is to predict the class membership probabilities for each observation. Overall possible classes these probabilities must sum to one, because the classes are exhaustive and mutually exclusive.\nThe number of training observations in each class should preferably be balanced as done in retrospective case-control studies.\n\n\n\n\nFigure - Coefficient Variance vs Control/Case Ratio\n\n\n\nFor a binary (just two categories) target variable the target variable becomes\n\\[Y_i = \\begin{cases} 1 & event \\ happening \\\\ 0 & event \\ not \\ happing \\end{cases}\\]\nand the predicted value given at a given set features becomes\n\\[\\hat{p}_i = \\Pr(Y_i = 1|x_{i1}, \\dots, x_{iK}) = \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_{i1} + \\cdots + \\hat{\\beta}_K \\cdot x_{iK})}{1 + \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_{i1} + \\cdots + \\hat{\\beta}_K \\cdot x_{iK})}\\]\nPer standard assumption \\(\\Pr(Y_i = 1|x_{i1}, \\dots, x_{iK})\\) follows a binary (binomial) distribution with an associated likelihood function.\nThe numerical optimization of the associated log-likelihood function finds the estimated parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\).\nThe probabilities are inherently non-linear with respect to the features \\(X_1, \\dots, X_K\\)\n\n\n\n\nFigure 4.2 - Classification using the Default data. Left: Estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default (No or Yes). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1.\n\n\nbut after the transformation \\(\\log\\left(\\frac{\\hat{p}_i}{1-\\hat{p}_i}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_{i1} + \\cdots + \\hat{\\beta}_p \\cdot x_{ip}\\) the logits \\(\\log\\left(\\frac{\\hat{p}_i}{1-\\hat{p}_i}\\right)\\) becomes are linear function in \\(X_1, \\dots, X_k\\) because \\(\\log\\left(\\frac{\\hat{p}_i}{1-\\hat{p}_i}\\right) \\in [-\\infty, \\infty]\\).\n\nThe estimated parameters \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_K\\) again capture the varying scales of \\(X_1, \\dots, X_k\\). Thus, scaling of the features is not required.\nFeatures that are based on factors, interaction effects and polynomial specifications can be easily accommodated.\nSelection of relevant features can again be achieved with stepwise regression.\nLogistic regression is a special case of the generalized linear model (GLM).\nWarning: For well separated classes logistic regression can become numerically unstable (problem of high discrimination).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#multinomial-logistic-regression",
    "href": "ML_w03_ClassificationRegression.html#multinomial-logistic-regression",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "6 Multinomial Logistic Regression",
    "text": "6 Multinomial Logistic Regression\n\nLogistic regression can be extended to model more than two classes \\(K &gt; 2\\).\nThe estimated probabilities again satisfy the constraint: \\(\\sum_{k=1}^{K} \\Pr(Y_i = k|X = x) = 1\\).\nThe model uses log-odds ratios to estimate a set of \\(K - 1\\) regression coefficients with the last class used by convention as reference:\n\\[\\log\\left(\\frac{\\Pr(Y_i = k|X = \\mathbf{x}_i)}{\\Pr(Y_i = K|X = \\mathbf{x}_i)}\\right) = \\beta_{0k} + \\beta_{1k} \\cdot x_{i1} + \\cdots + \\beta_{pk} \\cdot x_{ip}\\]\nThe probability for an individual class \\(k\\) becomes\n\\[\\Pr(Y_i = k|X = \\mathbf{x}_i) = \\frac{\\exp(\\beta_{0k} + \\beta_{1k} \\cdot x_{i1} + \\cdots + \\beta_{pk} \\cdot x_{ip})}{1 + \\sum_{k=1}^{K-1} \\exp(\\beta_{0k} + \\beta_{1k} \\cdot x_{i1} + \\cdots + \\beta_{pk} \\cdot x_{ip})}\\]\nand for class \\(K\\)\n\\[\\Pr(Y_i = k|X = \\mathbf{x}_i) = \\frac{1}{1 + \\sum_{k=1}^{K-1} \\exp(\\beta_{0k} + \\beta_{1k} \\cdot x_{i1} + \\cdots + \\beta_{pk} \\cdot x_{ip})}\\]",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#discriminant-analysis-versus-logistic-regression-skipped",
    "href": "ML_w03_ClassificationRegression.html#discriminant-analysis-versus-logistic-regression-skipped",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "7 Discriminant Analysis versus Logistic Regression (skipped)",
    "text": "7 Discriminant Analysis versus Logistic Regression (skipped)\n\nLogistic regression directly estimates conditional \\(\\Pr(Y_i = k|X_i)\\) while discriminant analysis estimates the multivariate class likelihood \\(f(X_i|Y_i = k)\\) and then applies the Bayesian theorem to obtain the posterior probability \\(\\Pr(Y_i = k|X_i)\\).\nFor small sample sizes and approximately normal distributed \\(N(X_i|Y_i = k)\\) classes discriminant analysis is superior.\nDue to the distributional assumption, discriminant analysis performs best for metrically scaled variables. However, it is relatively robust when a few factor variables are involved.\nIt is easier to handle more than \\(K &gt; 2\\) classes in discriminant analysis.\nBoth methods do not require that the features are normalized or standardized.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#use-of-bayesian-classification",
    "href": "ML_w03_ClassificationRegression.html#use-of-bayesian-classification",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "8 Use of Bayesian Classification",
    "text": "8 Use of Bayesian Classification\n\nLet \\(\\pi_k \\ with \\ k \\in \\{1, 2, \\dots, K\\}\\) be the prior probability of class \\(k\\), i.e., an observation \\(Y_i\\) coming from class \\(k\\) without to any additional information.\nCombined with the likelihood \\(f(X_i|Y_i = k)\\) we get the Bayesian posteriori probability\n\\[\\Pr(Y_i = k|X_i) = \\frac{\\pi_k \\cdot f(X_i|Y_i = k)}{\\underbrace{\\sum_{l=1}^{K} \\pi_l \\cdot f(X_i|Y_i = l)}_{=f(X_i)}}\\]\nAn object \\(i\\) is assigned to class \\(k\\) for which \\(\\max_k \\Pr(Y_i = k|X_i)\\) is the largest.\nTo obtain the posteriori probabilities, the prior probabilities and the likelihood need to be estimated from the data.\nThe exact functional form of the likelihood needs to be fully specified. In discriminant analysis it is usually the multivariate normal distribution with either a common or class-specific covariance matrix.\n\n\n\n8.1 Example with \\(K = 2\\) and \\(p = 1\\)\n\nFor \\(K = 2\\) and \\(p = 1\\) the two normal class distributions have parameters\n\n\\(N(x_i|\\mu_1, \\sigma_1^2)\\) with a mixing proportion \\(\\pi_1\\) and\n\\(N(x_i|\\mu_2, \\sigma_2^2)\\) with a mixing proportion \\(\\pi_2\\).\n\nFor \\(\\sigma_1^2 = \\sigma_2^2\\) the Bayesian decision function separating both classes becomes\n\\[\\delta_k(x_i) = x_i \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\cdot \\sigma^2} + \\log \\pi_k\\]\nand \\(x_i\\) is assigned to class \\(k\\) for which \\(\\delta_k(x_i) &gt; \\delta_l(x_i)\\) with \\(k \\neq l\\).\nTheoretical and empirical mixture distributions:\n\n\n\n\nFigure 4.4 - Left: Two one-dimensional normal density functions are shown. The dashed vertical line represents the Bayes decision boundary. Right: 20 observations were drawn from each of the two classes, and are shown as histograms. The Bayes decision boundary is again shown as a dashed vertical line. The solid vertical line represents the LDA decision boundary estimated from the training data.\n\n\n\nThe estimated parameters are:\n\n\\(\\hat{\\pi}_k = \\frac{n_k}{\\sum_{l=1}^{K} n_l}\\)\n\\(\\hat{\\mu}_k = \\frac{1}{n_k} \\cdot \\sum_{i \\in \\{y_i = k\\}} x_i\\)\n\\(\\hat{\\sigma}^2 = \\frac{1}{n-K} \\cdot \\sum_{k=1}^{K} \\sum_{i \\in \\{y_i = k\\}} (x_i - \\hat{\\mu}_k)^2\\)",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#linear-discriminant-functions-for-k-geq-2-p-geq-2-and-sigma_k-sigma_l-forall-k-l",
    "href": "ML_w03_ClassificationRegression.html#linear-discriminant-functions-for-k-geq-2-p-geq-2-and-sigma_k-sigma_l-forall-k-l",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "9 Linear Discriminant functions for \\(K \\geq 2\\), \\(p \\geq 2\\) and \\(\\Sigma_k = \\Sigma_l \\ \\forall k, l\\)",
    "text": "9 Linear Discriminant functions for \\(K \\geq 2\\), \\(p \\geq 2\\) and \\(\\Sigma_k = \\Sigma_l \\ \\forall k, l\\)\n\nFor two and more classes and more than one feature but identical group covariance matrices the Bayesian classification functions are:\n\\[\\delta_k(\\mathbf{x}_i) = \\mathbf{x}_i^T \\cdot \\mathbf{\\Sigma}^{-1} \\cdot \\boldsymbol{\\mu}_k - \\frac{1}{2} \\cdot \\boldsymbol{\\mu}_k^T \\cdot \\mathbf{\\Sigma}^{-1} \\cdot \\boldsymbol{\\mu}_k + \\log \\pi_k\\]\nThis is a linear function in \\(\\mathbf{x}_i\\)\nAgain, an object \\(i\\) is assigned to the group for which \\(\\delta_k(\\mathbf{x}_i)\\) is the largest.\nExample with \\(K = 3\\) and \\(p = 2\\)\n\n\n\n\nFigure 4.6 - An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95% of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_ClassificationRegression.html#comparison-of-the-different-classification-methods",
    "href": "ML_w03_ClassificationRegression.html#comparison-of-the-different-classification-methods",
    "title": "Week03: Regression, Logistic Regression and Discriminant Analysis",
    "section": "10 Comparison of the different classification methods",
    "text": "10 Comparison of the different classification methods\n\nStructurally linear discriminant analysis and logistic regression are similar because in both cases the decision rules are expressed as linear functions in \\(X_i\\).\nHowever, discriminant analysis makes the stronger assumption of applying multivariate normal distributions. If the Gaussian assumption is not satisfied, then logistic regression may outperform linear discriminant analysis.\nDiscriminant analysis relies on substantially more estimated parameters ([a] the group probabilities, [b] vectors of group means and [c] group covariance matrices).\nDiscriminant analysis belongs to the class of generative learners because it focuses on the joint distribution \\(\\Pr(X, Y)\\) whereas logistic regression is a discriminative learner because it focuses on the conditional distribution \\(\\Pr(Y|X)\\).",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Classification Regression"
    ]
  },
  {
    "objectID": "ML_w03_RadioOperatorCurve.html",
    "href": "ML_w03_RadioOperatorCurve.html",
    "title": "Week03: Evaluating the Performance of Binary Classification Models",
    "section": "",
    "text": "Start experimenting with the R-script ROCTutorial.r.\nImportant R functions are:\n\nDisplay a cross-tabulation of observed (rows) against predicted (column) class memberships: gmodels::CrossTable( )\nProvide detailed statistics: caret::confusionMatrix( ). See online help help(confusionMatrix).\nReceiver operating characteristic (ROC) curve: pROC::roc( ) or library(ROCR)\n\nFor the sake of terminology let us call one outcome as “positive”, which is usually an outcome of interest leading to action.\nThink as a medical doctor for whom “positive”, means a test indicates the presence of a disease.\nUsually rare classes (loan default, insurance fraud, disease outcome in a screening test, spam text messages etc.) are labelled “positive”.\nPositives are usually coded as factor level 1 whereas the negatives are set to 0.\nThis convention makes sense under specific test scenarios but can be arbitrary if both classes are “value-free”.\nFor rare positives an intuitive negative prediction will lead to a small error rate equal to the frequency of the rare positives. This provides motivation for conditional error rates.\n\nFor just two classes we get “confusion matrix”, which has the observed true classes in its columns and the predicted class in its rows:\n\n\n\n\n\n\n\n\n\nTrue Negativ\nTrue Positive\n\n\n\n\nPredicted Negative\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nPredicted Positive\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\nThis allows us to calculate several key statistics:\n\n\n\n\n\n\n\n\n\nName\nDefinition\nSynonyms\n\n\n\n\nTrue pos. rate\n\\(TP/(FN + TP)\\)\nsensitivity, \\(1 -\\) Type \\(II\\) error, power, recall (column perspective) \\(y\\)-axis of ROC\n\n\nTrue neg. rate\n\\(TN/(TN + FP)\\)\nspecificity (column perspective)\n\n\nFalse pos. rate\n\\(FP/(TN + FP)\\)\nType \\(I\\) error, \\(1 -\\) specificity (column perspective) \\(x\\)-axis of ROC\n\n\nPos. pred. value\n\\(TP/(FP + TP)\\)\nPrecision, \\(1 -\\) false discovery proportion (row perspective)\n\n\nTotal Accuracy\n\\(\\frac{TP + TN}{TN + FP + FN + TP}\\)\n\n\n\nTotal Error Rate\n\\(\\frac{FP + FN}{TN + FP + FN + TP}\\)\n\\(1 -\\) accuracy\n\n\n\n\nIn the medical sciences\n\nSensitivity: probability of predicting disease given that the true state is positive.\nSpecificity: probability of predicting non-disease given that the true state is negative.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Radio Operator Curve"
    ]
  },
  {
    "objectID": "ML_w03_RadioOperatorCurve.html#evaluating-the-predictive-performance-for-two-classes",
    "href": "ML_w03_RadioOperatorCurve.html#evaluating-the-predictive-performance-for-two-classes",
    "title": "Week03: Evaluating the Performance of Binary Classification Models",
    "section": "",
    "text": "Start experimenting with the R-script ROCTutorial.r.\nImportant R functions are:\n\nDisplay a cross-tabulation of observed (rows) against predicted (column) class memberships: gmodels::CrossTable( )\nProvide detailed statistics: caret::confusionMatrix( ). See online help help(confusionMatrix).\nReceiver operating characteristic (ROC) curve: pROC::roc( ) or library(ROCR)\n\nFor the sake of terminology let us call one outcome as “positive”, which is usually an outcome of interest leading to action.\nThink as a medical doctor for whom “positive”, means a test indicates the presence of a disease.\nUsually rare classes (loan default, insurance fraud, disease outcome in a screening test, spam text messages etc.) are labelled “positive”.\nPositives are usually coded as factor level 1 whereas the negatives are set to 0.\nThis convention makes sense under specific test scenarios but can be arbitrary if both classes are “value-free”.\nFor rare positives an intuitive negative prediction will lead to a small error rate equal to the frequency of the rare positives. This provides motivation for conditional error rates.\n\nFor just two classes we get “confusion matrix”, which has the observed true classes in its columns and the predicted class in its rows:\n\n\n\n\n\n\n\n\n\nTrue Negativ\nTrue Positive\n\n\n\n\nPredicted Negative\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nPredicted Positive\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\nThis allows us to calculate several key statistics:\n\n\n\n\n\n\n\n\n\nName\nDefinition\nSynonyms\n\n\n\n\nTrue pos. rate\n\\(TP/(FN + TP)\\)\nsensitivity, \\(1 -\\) Type \\(II\\) error, power, recall (column perspective) \\(y\\)-axis of ROC\n\n\nTrue neg. rate\n\\(TN/(TN + FP)\\)\nspecificity (column perspective)\n\n\nFalse pos. rate\n\\(FP/(TN + FP)\\)\nType \\(I\\) error, \\(1 -\\) specificity (column perspective) \\(x\\)-axis of ROC\n\n\nPos. pred. value\n\\(TP/(FP + TP)\\)\nPrecision, \\(1 -\\) false discovery proportion (row perspective)\n\n\nTotal Accuracy\n\\(\\frac{TP + TN}{TN + FP + FN + TP}\\)\n\n\n\nTotal Error Rate\n\\(\\frac{FP + FN}{TN + FP + FN + TP}\\)\n\\(1 -\\) accuracy\n\n\n\n\nIn the medical sciences\n\nSensitivity: probability of predicting disease given that the true state is positive.\nSpecificity: probability of predicting non-disease given that the true state is negative.",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Radio Operator Curve"
    ]
  },
  {
    "objectID": "ML_w03_RadioOperatorCurve.html#receiver-operating-characteristics-curve",
    "href": "ML_w03_RadioOperatorCurve.html#receiver-operating-characteristics-curve",
    "title": "Week03: Evaluating the Performance of Binary Classification Models",
    "section": "2 Receiver Operating Characteristics Curve",
    "text": "2 Receiver Operating Characteristics Curve\n\nROC curves are used to evaluate the model performance for binary classification scenarios.\nError rates are affected by a threshold probability \\(\\delta \\in [0,1]\\) of assigning an object \\(y_i\\) either to the positive \\(\\hat{y}_i = 1\\) of negative \\(\\hat{y}_i = 0\\) class in relation to the observed features \\(\\mathbf{x}_i\\):\n\n\\[\\hat{y}_i^{\\delta} = 0 \\text{ if } \\widehat{\\Pr(y_i = 1|\\mathbf{x}_i)} \\leq \\delta\\]\n\\[\\hat{y}_i^{\\delta} = 1 \\text{ if } \\widehat{\\Pr(y_i = 1|\\mathbf{x}_i)} &gt; \\delta\\]\n\nTherefore, the true positive rate TPR and false positive rate FPS respectively become\n\n\\[\\text{TPR}(\\mathbf{y}, \\hat{\\mathbf{y}}^{\\delta}) = \\Pr(\\hat{y}_i^{\\delta} = 1|y_{obs} = 1) \\text{ and}\\]\n\\[\\text{FPR}(\\mathbf{y}, \\hat{\\mathbf{y}}^{\\delta}) = \\Pr(\\hat{y}_i^{\\delta} = 1|y_{obs} = 0)\\]\n\nDepending on the threshold \\(\\delta\\) the total error rate, false positive rate (a.k.a. \\(1 -\\) specificity) and the true positive rate (a.k.a. sensitivity) change:\n\nFor \\(\\delta = 0\\) the table of predicted against observed cases becomes\n\n\n\n\n\n\\(y_{obs} = 0\\)\n\\(y_{obs} = 1\\)\n\n\n\n\n\n\\(\\hat{y}^{\\delta=0} = 0\\)\n0\n0\n0\n\n\n\\(\\hat{y}^{\\delta=0} = 1\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n\\)\n\n\n\n\\(n_1\\)\n\\(n_2\\)\n\\(n\\)\n\n\n\nThus \\(sensitivity\\ (TPR) = n_2/n_2 = 1\\) and \\(specificity\\ (TNR) = 0/n_1 = 0\\)\n\nFor \\(\\delta = 1\\) the table of predicted against observed cases becomes\n\n\n\n\n\n\\(y_{obs} = 0\\)\n\\(y_{obs} = 1\\)\n\n\n\n\n\n\\(\\hat{y}^{\\delta=0} = 0\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n\\)\n\n\n\\(\\hat{y}^{\\delta=0} = 1\\)\n0\n0\n0\n\n\n\n\\(n_1\\)\n\\(n_2\\)\n\\(n\\)\n\n\n\nThus \\(sensitivity\\ (TPR) = 0/n_2 = 0\\) and \\(specificity\\ (TNR) = n_1/n_1 = 1\\)",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Radio Operator Curve"
    ]
  },
  {
    "objectID": "ML_w03_RadioOperatorCurve.html#example-credit-data",
    "href": "ML_w03_RadioOperatorCurve.html#example-credit-data",
    "title": "Week03: Evaluating the Performance of Binary Classification Models",
    "section": "3 Example: Credit Data:",
    "text": "3 Example: Credit Data:\n\n\n\nTABLE 4.4. A confusion matrix compares the LDA predictions to the true default statuses for the 10,000 training observations in the Default data set. Elements on the diagonal of the matrix represent individuals whose default statuses were correctly predicted, while off-diagonal elements represent individuals that were misclassified. LDA made incorrect predictions for 23 individuals who did not default and for 252 individuals who did default.\n\n\n\nThe training error rate is low with \\((252 + 23)/10,000 \\times 100 = 2.75\\%\\).\nHowever, using a naïve but useless classifier in which no one defaults would only lead to a just slightly larger error rate of \\(333/10,000 \\times 100 = 3.33\\%\\).\nTo catch more true positive defaulter, i.e., increase the sensitivity, the cut of value \\(\\delta\\) must become smaller than \\(\\delta = 0.5\\) at the risk of denying more trustworthy borrowers.\n\n\n\n\nFIGURE 4.7. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers.\n\n\n\nThe ROC (radio operating characteristic) plots the false positive rate (\\(1 -\\) specificity) on the \\(x\\)-axis against the true positive rate (sensitivity) on the \\(y\\)-axis.\n\n\n\n\nFIGURE 4.8. A ROC curve for the LDA classifier on the Default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.\n\n\n\nFor an uninformative predictor the equality \\(sensitivity = 1 - specificity\\) holds at any threshold \\(\\delta \\in [0,1]\\). Thus the diagonal signifies an uninformative classifier.\nFor a well-discriminating classifier, the ROC curve takes a rectangular shape:\nThe upper left corner denotes that predictor for which all positives are properly classified, and no negative is falsely classified.\nThe area underneath the ROC curve (AUC) measures the discriminating power of a classifier:\n\n0.9 to 1.0: outstanding\n0.8 to 0.9: good\n0.7 to 0.8: fair\n0.6 to 0.7: poor\n0.5 to 0.6: no discrimination\n\nTwo competing classifiers may have an identical AUC but different shapes of the ROC curve.\n\n\n\n\nFigure 10.5: ROC curves may have different performance despite having the same AUC",
    "crumbs": [
      "Home",
      "(三) EPPS6326 Machine Learning",
      "Wk03-Radio Operator Curve"
    ]
  }
]